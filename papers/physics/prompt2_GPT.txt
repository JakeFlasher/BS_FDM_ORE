# MARKET REGIME IDENTIFICATION AND ADAPTIVE STRATEGY SELECTION:  
## Bridging Trending and Mean-Reverting Market Structures

Prepared for: \[Supervisor Name]  
Date: February 19, 2026  
Classification: Internal â€” Research Division

---

## Abstract

This report addresses a central practical asymmetry in systematic trading: the same price process can alternately reward momentum-like behavior that â€œchases the rise and cuts the fallâ€ (è¿½æ¶¨æ€è·Œ) and punish it severely, while at other times rewarding mean-reversion behavior that â€œsells highs and buys lowsâ€ (é«˜æŠ›ä½å¸) and punishing momentum. The core unsolved problem is not the existence of these strategyâ€“regime pairings, but the reliable and timely identification of the current market regime from noisy, multi-scale, nonstationary data. The report formalizes trending, mean-reverting, and random-walk regimes in the language of stochastic processes, autocorrelation structure, and scaling laws; surveys econometric, technical, probabilistic, machine-learning, and fractal/chaos-based regime detection methods with explicit formulations and failure modes; analyzes the regime-transition interval as the dominant drawdown contributor; and proposes a composite multi-signal scoring framework that fuses heterogeneous indicators into a single regime-confidence measure with explicit uncertainty handling and transition-aware de-risking. The report concludes with a mapping from inferred regimes to strategy families and regime-conditional risk management, and it enumerates limitations, open questions, and research recommendations, explicitly flagging areas where the empirical literature remains contested or where the authorâ€™s knowledge may be incomplete.

---

## 1. Theoretical Foundations of Market Regimes

A â€œmarket regimeâ€ is usefully defined not as a narrative label but as a set of statistical regularities governing the joint distribution of observables such as returns, realized volatility, and (when available) order-flow and liquidity measures over a time interval that is long enough for estimation yet short enough that the parameters can be treated as approximately stable. In this report, the primary regime axis of interest is the sign and persistence of serial dependence in returns across the investment horizon at which the strategy is deployed, because the expected edge of momentum versus mean-reversion is structurally tied to that serial dependence. A secondary axis, which becomes critical in transitions, is conditional heteroskedasticity and the stability of volatility and liquidity, because both momentum and mean-reversion can fail when volatility regimes shift faster than position management can adapt.

Let \(\{P_t\}_{t\in \mathbb{Z}}\) denote the price process sampled at a fixed interval and define log-returns \(r_t = \ln P_t - \ln P_{t-1}\). The autocorrelation function (ACF) of returns is defined for lag \(k\ge 1\) by
$$
\rho(k) \equiv \frac{\mathrm{Cov}(r_t, r_{t-k})}{\mathrm{Var}(r_t)}.
$$
A stylized â€œtrending regimeâ€ at the horizon of interest is characterized by persistence, meaning that \(\rho(k)\) is non-negligibly positive for a range of small lags \(k\) relevant to the holding period, possibly together with slow decay of dependence across scales. In such a regime, conditional on transaction costs and risk constraints, momentum strategies that align with recent price movement can exhibit positive expectancy because the sign of recent returns contains predictive information about near-future returns. A stylized â€œmean-reverting regimeâ€ is characterized by anti-persistence, meaning that \(\rho(k)\) is non-negligibly negative for a range of relevant lags, so that deviations tend to reverse and contrarian strategies can earn by betting on reversion toward a local equilibrium. A â€œrandom walk regime,â€ in the narrow sense relevant to short-horizon forecasting, corresponds to \(\rho(k)\approx 0\) for the relevant lags and to the approximate absence of predictable structure in returns beyond what is explained by time-varying volatility; in that case, most directional alpha strategies are dominated by costs and risk premiums, and only risk harvesting, carry, or microstructure-based effects may remain.

The mathematical basis for the momentum versus mean-reversion mapping is most transparent under simple parametric approximations. Consider an \( \mathrm{AR}(1) \) return model \(r_t = \phi r_{t-1} + \varepsilon_t\) with \(|\phi|<1\) and \(\varepsilon_t\) a martingale difference sequence. Then \(\rho(1)=\phi\), so \(\phi>0\) implies persistence and favors momentum-like predictors, while \(\phi<0\) implies anti-persistence and favors mean-reversion predictors. Real markets are not well described by a single-lag autoregression; nevertheless, the sign structure of short-lag serial correlation is a first-order diagnostic of whether â€œrecent directionâ€ is informative or misleading at a given sampling and holding horizon.

Because regime structure is frequently multi-scale, the Hurst exponent offers a complementary, scale-sensitive description of persistence. Under scaling models such as fractional Brownian motion, the Hurst exponent \(H\in (0,1)\) governs how the variance of increments scales with the time step. For log-price increments over horizon \(\tau\), a canonical scaling relationship is
$$
\mathrm{Var}\!\left[\ln P_{t+\tau} - \ln P_t\right] \propto \tau^{2H}.
$$
When \(H>0.5\), increments are persistent and the process exhibits long-range dependence consistent with trending behavior; when \(H<0.5\), increments are anti-persistent and consistent with mean-reverting behavior; and when \(H=0.5\), the increments scale as in standard Brownian motion, aligning with the classical random-walk benchmark. This scaling perspective is valuable because it highlights an operationally crucial point: a series can be â€œtrendingâ€ at one horizon and â€œmean-revertingâ€ at another, not merely because of estimation error but because the underlying market ecology can produce horizon-dependent structure. In practical terms, a strategy must match its holding period to the regime assessed at that horizon, rather than treating â€œthe marketâ€ as globally trending or reverting.

Fractional Brownian motion (fBm) provides a unifying theoretical framework for these ideas by generalizing Brownian motion to allow correlated increments while maintaining Gaussianity and self-similarity. The fBm process \(B_H(t)\) is a zero-mean Gaussian process with covariance function
$$
\mathbb{E}[B_H(t)B_H(s)] = \frac{1}{2}\left(t^{2H} + s^{2H} - |t-s|^{2H}\right),
$$
which implies increment correlations when \(H\neq 0.5\). While financial returns are not Gaussian and exhibit heavy tails, volatility clustering, jumps, and leverage effects, fBm remains conceptually useful as a stylized model linking scaling exponents to serial dependence. The practical implication is not that markets follow fBm, but that estimating \(H\) can be interpreted as estimating a coarse persistence-versus-anti-persistence tendency, subject to serious caveats about finite samples, nonstationarity, and microstructure noise.

The Fractal Market Hypothesis (FMH) emphasizes that market stability and predictability are emergent properties of heterogeneous participants operating at different investment horizons, and it argues that scaling and long-memory properties are not anomalies but structural features of markets (Peters, 1994, *Fractal Market Analysis: Applying Chaos Theory to Investment and Economics*, Wiley). This perspective contrasts with the strongest forms of the Efficient Market Hypothesis that treat returns as essentially unpredictable and consistent with a random walk (Fama, 1970, â€œEfficient Capital Markets: A Review of Theory and Empirical Work,â€ *Journal of Finance*). The tension is not merely philosophical. If markets alternate between regimes where persistence dominates and regimes where anti-persistence dominates, then a single stationary random-walk null is an insufficient description, and adaptive models that allow latent regime switching become conceptually appropriate.

Within econometrics, the seminal formalization of regime switching in macro-financial time series is Hamiltonâ€™s Markov regime-switching framework, which models parameters as functions of an unobserved discrete state following a Markov chain (Hamilton, 1989, â€œA New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle,â€ *Econometrica*). While Hamiltonâ€™s original application was the business cycle, the underlying methodology provides a disciplined way to treat â€œregimesâ€ as latent states inferred probabilistically from observable data, thereby directly addressing the regime identification problem rather than assuming a fixed regime.

---

## 2. Comprehensive Survey of Regime Detection Methods

Regime detection methods can be organized by the theoretical object they attempt to estimate. Some estimate serial dependence directly through autocorrelation or variance scaling; some infer regimes indirectly through trend-strength and volatility proxies; some specify a latent-state probabilistic model and infer state posteriors; some detect structural breaks or change points; and some attempt to capture nonlinear dynamics, clustering structure, or fractal properties. A central practical trade-off runs throughout: methods that are fast and robust often provide blunt signals with high false-alarm rates, while methods that are statistically principled often require longer samples and introduce latency that is costly precisely when regimes shift.

To anchor the model-based methods, the following schematic illustrates the mapping between latent regime states and observable market variables in a Hidden Markov Model (HMM), emphasizing that regime identification is an inference problem under uncertainty rather than a deterministic classification.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Hidden Markov Regime Model                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Latent state process:  S_t âˆˆ {Trend, Range, RandomWalk, ...}        â”‚
â”‚  Markov dynamics:       P(S_t = j | S_{t-1} = i) = p_ij              â”‚
â”‚                                                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚            â”‚   S_{t-1}     â”‚        â”‚      S_t      â”‚               â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                    â”‚  transition p_ij       â”‚                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                     â”‚
â”‚  Observation (emission) model:                                      â”‚
â”‚     x_t = (r_t, |r_t|, RV_t, Î”spread_t, V_t, OFI_t, ...)             â”‚
â”‚     p(x_t | S_t = j) = ğ’Ÿ_j(Î¸_j)                                     â”‚
â”‚                                                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚            â”‚      S_t      â”‚                                       â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                    â”‚ emission p(x_t | S_t)                           â”‚
â”‚                    â†“                                                 â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚            â”‚      x_t      â”‚  (observed market features)            â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The remainder of this section surveys methods within five categories. For each method, the discussion covers formulation, data requirements, computational cost, look-back sensitivity, real-time applicability, failure modes, and false-signal characteristics. Because these properties depend strongly on horizon and asset class, the analysis is framed as conditional statements rather than universal claims.

### 2.A. Statistical and Econometric Methods

#### 2.A.1. Hurst exponent estimation via R/S analysis, DFA, and variance scaling

Hurst exponent estimation attempts to measure the scaling behavior of a process and infer persistence. The variance scaling method follows directly from the relationship
$$
\mathrm{Var}\!\left[\ln P_{t+\tau} - \ln P_t\right] \propto \tau^{2H},
$$
which implies that regressing \(\ln \mathrm{Var}[\Delta \ln P(\tau)]\) on \(\ln \tau\) yields a slope estimate \(2H\). In practical implementations, one constructs increments across multiple aggregation horizons \(\tau\), estimates sample variances for each \(\tau\), and fits a line over a chosen scaling range. The data requirement depends on the breadth of scales used; robust estimation across multiple \(\tau\) typically demands hundreds to thousands of observations, and the estimate is highly sensitive to nonstationarity, volatility clustering, and the presence of jumps. At daily frequency, a moving-window â€œMoving Hurstâ€ indicator often uses windows on the order of \(N\in[250,1000]\) observations, with the shorter end reacting faster but producing noisy and often spurious regime flips. At intraday frequencies, microstructure noise can bias \(H\) estimates upward or downward depending on sampling and the estimator, making careful pre-processing (such as subsampling or noise-robust estimators) essential.

Rescaled range (R/S) analysis estimates \(H\) by scaling of the rescaled cumulative deviation range. If \(X_t\) is a return series, one forms the cumulative deviation \(Y_k = \sum_{t=1}^k (X_t - \bar{X})\), computes the range \(R(N)=\max_{1\le k\le N} Y_k - \min_{1\le k\le N} Y_k\), and rescales by the standard deviation \(S(N)\). Under long-memory scaling, one expects \(\mathbb{E}[R(N)/S(N)] \propto N^H\). While historically influential (Hurst, 1951, â€œLong-Term Storage Capacity of Reservoirs,â€ *Transactions of the American Society of Civil Engineers*), R/S is known to be sensitive to short-term dependence and nonstationary variance, and it can report spurious long memory in finite samples.

Detrended Fluctuation Analysis (DFA) was developed to estimate scaling exponents in the presence of trends by detrending within windows before computing fluctuation functions. In DFA, one integrates the demeaned series, partitions into segments of length \(s\), detrends each segment (often with a polynomial fit), computes the root-mean-square fluctuation \(F(s)\), and estimates \(H\) from \(F(s)\propto s^H\). DFA is widely used because it is more robust to certain types of nonstationary mean behavior than R/S, but it remains sensitive to volatility clustering and structural breaks, and its results depend on the range of scales \(s\) used (Peng et al., 1994, â€œMosaic Organization of DNA Nucleotides,â€ *Physical Review E*).

In real time, moving-window Hurst estimates can be computed efficiently, but their interpretability must be constrained to a specific horizon. A common failure mode is multi-scale inconsistency: \(H\) may exceed \(0.5\) over medium horizons while falling below \(0.5\) over short horizons, which is not contradictory but implies that â€œtrend versus mean reversionâ€ is horizon-specific. False-signal characteristics are dominated by estimator variance and by regime transitions: a rapid shift in volatility structure can cause \(H\) to cross \(0.5\) transiently even if serial dependence in returns is unchanged.

A conceptual example illustrates the horizon dependency. Suppose daily returns are weakly anti-persistent due to microstructure and liquidity effects, while weekly returns exhibit persistence due to macro flows. Estimating \(H\) on daily increments over \(\tau\in\{1,2,5\}\) might yield \(H<0.5\), suggesting mean reversion at short horizons, while estimating over \(\tau\in\{5,10,20\}\) might yield \(H>0.5\), suggesting trend at longer horizons. If a strategy uses a 3â€“5 day holding period but uses a 60-day moving Hurst computed over longer scales, it may systematically mismatch regime and horizon, thereby generating apparently â€œunreliableâ€ regime signals that are in fact signals about a different horizon.

#### 2.A.2. Variance Ratio test (Lo and MacKinlay, 1988)

The Variance Ratio (VR) test evaluates the random-walk null by comparing the variance of \(q\)-period aggregated returns to \(q\) times the variance of one-period returns. Let \(r_t\) be one-period returns and define \(q\)-period returns \(r_t^{(q)}=\sum_{i=0}^{q-1} r_{t-i}\). The variance ratio is
$$
\mathrm{VR}(q) = \frac{\mathrm{Var}(r_t^{(q)})}{q\cdot \mathrm{Var}(r_t)}.
$$
Under an i.i.d. random walk in returns, \(\mathrm{Var}(r_t^{(q)}) = q\cdot \mathrm{Var}(r_t)\) and hence \(\mathrm{VR}(q)=1\). If returns are positively serially correlated, the variance of aggregated returns exceeds the i.i.d. benchmark and \(\mathrm{VR}(q)>1\), which is consistent with trending behavior at horizon \(q\). If returns are negatively serially correlated, \(\mathrm{VR}(q)<1\), consistent with mean reversion. The formal test statistics include heteroskedasticity-robust versions, which is essential in financial data due to volatility clustering (Lo and MacKinlay, 1988, â€œStock Market Prices Do Not Follow Random Walks: Evidence from a Simple Specification Test,â€ *Review of Financial Studies*).

Data requirements are moderate because VR can be estimated from returns over a chosen window; however, statistical power depends on window size and the strength of dependence. In practice, reliable inference often requires hundreds of observations per regime evaluation at daily frequency, especially when dependence is weak. Computational cost is low, enabling rolling VR estimation over multiple \(q\) values to capture multi-horizon structure. Look-back sensitivity is meaningful: short windows produce noisy VR estimates with frequent sign changes around 1, while long windows produce stable estimates but introduce latency and contaminate the estimate with past regimes. Real-time applicability is good for signal generation, but poor for formal hypothesis testing if multiple testing across \(q\) and rolling windows is not controlled.

A worked conceptual example clarifies interpretation. Suppose a rolling window produces \(\mathrm{VR}(5)=1.12\) and \(\mathrm{VR}(20)=1.03\). The first value suggests meaningful persistence at the one-week horizon, while the second suggests only slight persistence at the one-month horizon. A trend-following strategy with a 5-day holding horizon may be aligned with \(\mathrm{VR}(5)\), while a long-horizon momentum strategy would require stronger evidence at \(\mathrm{VR}(20)\). A common failure mode occurs in volatility regime shifts: if volatility rises rapidly, the aggregated-return variance can increase disproportionately, potentially biasing naive VR interpretation if heteroskedasticity is not robustly handled, which is precisely why robust VR variants are used.

#### 2.A.3. Augmented Dickeyâ€“Fuller (ADF) test for unit roots and stationarity

The ADF test evaluates whether a univariate time series contains a unit root, typically in the context of testing whether a level series is nonstationary. For prices, the null of a unit root in levels is often unsurprising; however, for spreads, residuals of cointegration relationships, or certain transformed variables, ADF can help distinguish mean-reverting behavior from random-walk behavior. In one common specification, for series \(y_t\), the test estimates
$$
\Delta y_t = \alpha + \beta t + \gamma y_{t-1} + \sum_{i=1}^{p} \phi_i \Delta y_{t-i} + \varepsilon_t,
$$
and tests \(H_0:\gamma=0\) (unit root) against \(H_1:\gamma<0\) (stationarity). The augmentation terms \(\Delta y_{t-i}\) aim to whiten residual autocorrelation. The classical reference is Dickey and Fuller (1979, â€œDistribution of the Estimators for Autoregressive Time Series with a Unit Root,â€ *Journal of the American Statistical Association*), with the â€œaugmentedâ€ variant appearing in subsequent econometric practice.

For regime identification between trending and mean-reverting regimes in returns, ADF is often misapplied. Trending regimes in finance do not necessarily imply nonstationary drift in levels in a way that ADF can detect reliably, and mean reversion in returns is different from stationarity in levels. A more appropriate application is to test mean reversion in a constructed spread or deviation, such as \(z_t = \ln P_t - \ln \hat{P}_t\) where \(\hat{P}_t\) is a moving equilibrium proxy, or to test cointegration residuals in pair trading. Data requirements are nontrivial because ADF has low power against near-unit-root alternatives and because structural breaks can bias results toward false non-rejection of stationarity. Computational cost is low, but look-back sensitivity is high because lag length \(p\) selection and deterministic terms \((\alpha,\beta t)\) alter inference. Real-time applicability is limited by the fact that formal inference requires stable samples, and rolling ADF tests can generate frequent and misleading regime flips. Failure modes are dominated by structural breaks, time-varying volatility, and model mis-specification of deterministic components.

#### 2.A.4. Autocorrelation analysis and Ljungâ€“Box tests for serial dependence

Direct autocorrelation analysis estimates \(\rho(k)\) and assesses whether a set of autocorrelations differs from zero. The Ljungâ€“Box test aggregates autocorrelation estimates up to lag \(m\) into a statistic of the form
$$
Q = n(n+2)\sum_{k=1}^{m} \frac{\hat{\rho}(k)^2}{n-k},
$$
which under the null of no serial correlation is approximately \(\chi^2_m\) for large \(n\) (Ljung and Box, 1978, â€œOn a Measure of Lack of Fit in Time Series Models,â€ *Biometrika*). For regime identification, one may compute rolling autocorrelations and interpret the sign pattern, or compute rolling Ljungâ€“Box \(p\)-values as a â€œdependence strengthâ€ proxy.

Data requirements depend on the lags considered; reliable estimation of \(\rho(k)\) for \(k\) up to \(m\) typically requires \(n\) substantially larger than \(m\), and rolling estimates can be unstable for small windows. Computational cost is low. Look-back sensitivity is moderate to high: short windows inflate estimator variance, while long windows smear regimes. Real-time applicability is good as a descriptive indicator but weak as a definitive classifier, because many assets have near-zero autocorrelation in returns at daily frequency, while exhibiting dependence in absolute returns or volatility rather than signed returns. Failure modes include the conflation of serial dependence in volatility with serial dependence in returns, and the mechanical generation of negative autocorrelation from bidâ€“ask bounce at very high frequencies, which can falsely indicate mean reversion when the effect is microstructure rather than economic reversion.

### 2.B. Technical-Indicator-Based Methods

Technical indicators operationalize regime proxies that are often more robust to noise than direct serial correlation estimates but less theoretically specific. They are best interpreted as filters that gate strategy families rather than as â€œtruthâ€ about the data-generating process.

#### 2.B.1. ADX (Average Directional Index) and Wilderâ€™s trend-strength framework

The Average Directional Index (ADX) is designed to measure trend strength independent of direction by combining directional movement measures and smoothing them. In Wilderâ€™s formulation, one computes directional movement \(\mathrm{DM}^+\) and \(\mathrm{DM}^-\), true range \(\mathrm{TR}\), and then constructs smoothed directional indicators \(\mathrm{DI}^+\) and \(\mathrm{DI}^-\) as ratios of smoothed \(\mathrm{DM}\) to smoothed \(\mathrm{TR}\). The ADX is then a smoothed version of the directional index \(\mathrm{DX} = 100\cdot |\mathrm{DI}^+ - \mathrm{DI}^-|/(\mathrm{DI}^+ + \mathrm{DI}^-)\). The classical reference is Wilder (1978, *New Concepts in Technical Trading Systems*, Trend Research).

Data requirements are modest because ADX can be computed from OHLC data over a window such as 14 periods, but regime classification thresholds such as ADX \(>25\) for trending and ADX \(<20\) for ranging are heuristic and asset-dependent. Computational cost is negligible. Look-back sensitivity is significant: the smoothing parameter introduces lag, meaning that ADX will often identify trends after they are underway and will often remain elevated as trends end, which is particularly dangerous in transitions. Real-time applicability is high as a gating filter, but false-signal characteristics include high false trending signals in high-volatility mean-reverting markets and delayed detection of early-stage trends. A critical failure mode is that ADX can rise in strong oscillatory markets where range expansions increase directional movement measures without producing sustained directional drift; in such cases ADX indicates â€œtrend strengthâ€ while the optimal strategy may still be mean reversion at the scale of interest.

#### 2.B.2. Bollinger Bandwidth as a volatility regime proxy

Bollinger Bands define an upper and lower band around a moving average \(m_t\) using a multiple \(k\) of the rolling standard deviation \(s_t\) of price or returns over a window \(n\). The bandwidth is often defined as
$$
\mathrm{BBW}_t = \frac{\mathrm{Upper}_t - \mathrm{Lower}_t}{m_t} = \frac{2k s_t}{m_t},
$$
or in alternative forms depending on implementation (Bollinger, 2001, *Bollinger on Bollinger Bands*, McGrawâ€“Hill). Bandwidth acts as a proxy for volatility and for the â€œexpansion versus contractionâ€ phase of price movement. In regime detection, rising BBW can indicate a transition from low-volatility range to high-volatility breakout, while falling BBW can indicate trend exhaustion and compression.

Data requirements are modest. Computational cost is negligible. Look-back sensitivity is high because the moving window controls both responsiveness and stability. Real-time applicability is good, but BBW alone does not distinguish between trending and mean-reverting volatility expansions. False signals are common when volatility rises within a range-bound market, producing breakouts that reverse, or when volatility compresses during a trendâ€™s mid-phase, producing false â€œrangeâ€ classifications. BBW is most useful when combined with a directionality measure such as slope, breakout confirmation, or HMM regime probabilities.

#### 2.B.3. Moving average slope, crossover density, and whipsaw frequency

Moving average slope measures the rate of change of a smoothed price, such as \(\beta_t\) from a rolling regression of \(\ln P_t\) on time, or simply \(\Delta \mathrm{MA}_t\). Crossover density counts the frequency with which short and long moving averages cross, which tends to be high in choppy, range-bound markets and low in smooth trends. Whipsaw frequency, defined as the rate at which a trend-following signal reverses within a short interval, serves as a direct proxy for â€œtrend tradability.â€

These measures require only price data and modest history. Computational cost is low. Look-back sensitivity is high because smoothing windows determine both slope magnitude and crossover rates. Real-time applicability is good for operational gating. Failure modes include the dependence on volatility: high volatility increases crossover events even in trending markets, and low volatility can suppress crossovers even in weak trends, potentially generating false regime classifications. False-signal characteristics are often asymmetric: these indicators may be more reliable at confirming non-trending conditions (high whipsaw, high crossover density) than at confirming sustained trends, particularly in the presence of fat tails and gap risk.

#### 2.B.4. ATR (Average True Range) regime classification

The Average True Range (ATR) measures absolute price movement, commonly computed as a moving average of true range \(\mathrm{TR}_t = \max\{H_t - L_t, |H_t - C_{t-1}|, |L_t - C_{t-1}|\}\). ATR is primarily a volatility proxy rather than a trend proxy. Regime classification using ATR typically distinguishes low-volatility from high-volatility regimes, or uses ATR-normalized trend metrics such as slope divided by ATR.

Data requirements and computational cost are low. Look-back sensitivity is moderate. Real-time applicability is high. Failure modes arise when volatility is high but directionality is absent, causing misclassification of â€œtrend opportunityâ€ if ATR is used improperly. ATR is most valuable as a risk-scaling input to position sizing and stop placement rather than as a standalone trend-versus-range discriminator.

### 2.C. Model-Based and Probabilistic Methods

Model-based methods specify an explicit generative structure and infer regimes as latent states. Their advantage is probabilistic output and principled handling of uncertainty; their disadvantage is sensitivity to mis-specification and the requirement for sufficient data for stable estimation.

#### 2.C.1. Markov Regime-Switching models (Hamilton, 1989) and EM estimation

A two-state Markov regime-switching model for returns can be written as
$$
r_t = \mu_{S_t} + \sigma_{S_t}\varepsilon_t,\quad \varepsilon_t\sim \mathcal{N}(0,1),
$$
where \(S_t\in\{1,2\}\) is an unobserved regime following a Markov chain with transition matrix
$$
\mathbf{P}=
\begin{bmatrix}
p_{11} & p_{12}\\
p_{21} & p_{22}
\end{bmatrix},
\quad p_{ij} = \mathbb{P}(S_t=j\mid S_{t-1}=i),
$$
and where \((\mu_1,\sigma_1)\) and \((\mu_2,\sigma_2)\) differ. In many financial applications, one state corresponds to higher volatility and possibly negative drift, while another corresponds to lower volatility and possibly positive drift; for trend-versus-range classification, one can instead specify states with different autocorrelation structures, such as regime-dependent AR parameters:
$$
r_t = \phi_{S_t} r_{t-1} + \sigma_{S_t}\varepsilon_t.
$$
Estimation typically proceeds by maximum likelihood using the Hamilton filter, often framed as an EM-like procedure in which state posteriors are computed given parameters and then parameters are updated (Hamilton, 1989, *Econometrica*). For a state-space perspective and practical estimation details, a standard reference is Kim and Nelson (1999, *State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications*, MIT Press).

Data requirements are substantial if one expects stable estimation of transition probabilities and state-dependent parameters, particularly when regimes are not strongly separated. A common institutional heuristic is that daily models require at least one to several years of data, with the caveat that more data increases regime contamination if the process is nonstationary. Computational cost is moderate for two-state models and increases with state count and model complexity. Look-back sensitivity is implicit: although the filter uses all past data for inference, in practice one may re-estimate parameters on a rolling window, introducing a tunable trade-off between responsiveness and stability. Real-time applicability is good if one accepts probabilistic regime posteriors rather than hard classifications, but a major failure mode is mis-specification: if the true dynamics involve heavy tails, jumps, or time-varying transition probabilities, Gaussian two-state models can produce overconfident posteriors and unstable regime assignments. False-signal characteristics typically involve delayed recognition of regime shifts when states overlap, and spurious switching when volatility changes mimic regime changes.

#### 2.C.2. Hidden Markov Models (HMM): Baumâ€“Welch training and Viterbi decoding

HMMs generalize the regime-switching idea by allowing flexible emission distributions for observed feature vectors \(x_t\) that can include returns, volatility proxies, volume, and microstructure variables. The model specifies a Markov chain \(S_t\) and emissions \(x_t\) distributed according to state-dependent densities \(p(x_t\mid S_t=j)\). If emissions are modeled as multivariate Gaussians, the emission parameters are \((\mu_j,\Sigma_j)\) for each state, but mixtures, t-distributions, or nonparametric emissions can be used to accommodate heavy tails.

Training is typically performed via the Baumâ€“Welch algorithm, which is an EM procedure for HMMs, and decoding the most likely state sequence uses the Viterbi algorithm (Rabiner, 1989, â€œA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,â€ *Proceedings of the IEEE*). In market applications, the key benefit is that one can include multiple observables that jointly characterize regimes, such as combining return directionality with volatility level and liquidity proxies, thereby reducing reliance on any single noisy indicator.

Data requirements depend on the dimensionality of \(x_t\) and the number of states. With Gaussian emissions, stable covariance estimation requires significantly more observations than feature dimension, and regularization becomes necessary in high-dimensional settings. Computational cost is moderate and scalable for small state counts. Look-back sensitivity emerges through retraining frequency and window selection; frequent retraining reacts to new regimes but increases overfitting risk. Real-time applicability is strong because the filter provides \(p(S_t\mid x_{1:t})\), a naturally probabilistic regime-confidence measure. Failure modes include label switching, sensitivity to initialization, and the tendency of HMMs to segment by volatility rather than by â€œtrend tradabilityâ€ unless features are chosen carefully. False signals can arise when a new volatility regime causes emissions to shift without corresponding changes in serial dependence, leading to regime reclassification that is irrelevant to momentum versus mean-reversion performance.

#### 2.C.3. Threshold Autoregressive (TAR) and Smooth Transition Autoregressive (STAR) models

TAR models allow dynamics to change when an observed variable crosses a threshold. A simple TAR for returns might be
$$
r_t =
\begin{cases}
\phi_1 r_{t-1} + \varepsilon_t, & z_{t-d}\le c,\\
\phi_2 r_{t-1} + \varepsilon_t, & z_{t-d}> c,
\end{cases}
$$
where \(z_{t-d}\) is a threshold variable such as lagged return, volatility, or a spread, \(c\) is a threshold, and \(d\) is a delay. STAR models replace the hard threshold with a smooth transition function \(G(\cdot)\), yielding
$$
r_t = \phi_1 r_{t-1} + \phi_2 r_{t-1} G(z_{t-d};\gamma,c) + \varepsilon_t,
$$
where \(G\) is often logistic or exponential and \(\gamma\) controls smoothness. A standard reference for threshold models is Tong (1983, *Threshold Models in Non-linear Time Series Analysis*, Springer), and for smooth transition models TerÃ¤svirta (1994, â€œSpecification, Estimation, and Evaluation of Smooth Transition Autoregressive Models,â€ *Journal of the American Statistical Association*).

Data requirements are substantial because nonlinear models require enough observations in each regime region to estimate parameters. Computational cost is moderate to high due to nonlinear optimization. Look-back sensitivity is high because thresholds and transition smoothness may drift. Real-time applicability is limited by parameter instability and by the risk that threshold dynamics are artifacts of the estimation window. Failure modes include overfitting and spurious threshold detection in noisy data. False signals can be frequent unless the threshold variable has strong structural meaning, such as a volatility threshold that changes market-maker behavior.

### 2.D. Machine Learning and Computational Methods

Computational methods often focus on detecting change points or learning discriminative mappings from features to regime labels. Their utility depends critically on label definition and on controlling leakage and overfitting.

#### 2.D.1. Online Bayesian Change-Point Detection (Adams and MacKay, 2007)

Online Bayesian Change-Point Detection (BOCPD) models the time since the last change point as a latent â€œrun lengthâ€ and updates posterior beliefs sequentially. In one formulation, the hazard function \(H(\ell)\) gives the probability of a change point after run length \(\ell\). Given predictive likelihoods of the data under a model with parameters that reset at change points, one computes \(p(\ell_t\mid x_{1:t})\) recursively (Adams and MacKay, 2007, â€œBayesian Online Changepoint Detection,â€ arXiv preprint; note that publication venues vary across versions, and my knowledge of the most definitive archival publication is incomplete). In market contexts, BOCPD can detect shifts in mean, variance, or other parameters of a chosen likelihood model, and thereby identify transition periods.

Data requirements are flexible because the method is online, but model choice matters. Computational cost ranges from moderate to high depending on truncation of run length and complexity of the predictive model. Look-back sensitivity is implicit through the hazard function and through priors. Real-time applicability is strong for detecting change points, but change points do not uniquely imply â€œtrend versus rangeâ€; they indicate â€œsomething changed.â€ Failure modes include frequent false alarms in heavy-tailed data and sensitivity to hazard calibration. False signals are common when volatility clustering produces apparent parameter shifts without regime changes relevant to strategy performance.

#### 2.D.2. CUSUM control charts adapted for financial series

CUSUM accumulates deviations from a target mean to detect shifts. For observations \(x_t\) and target \(\mu_0\), one may define
$$
C_t = \max\{0, C_{t-1} + (x_t - \mu_0 - k)\},
$$
with reference value \(k\), and signal a change when \(C_t\) exceeds a threshold \(h\). The classical reference is Page (1954, â€œContinuous Inspection Schemes,â€ *Biometrika*). In finance, CUSUM is used to detect shifts in return mean, volatility proxies, or spreads. Its value in regime detection is primarily as an early warning of transitions rather than as a direct classifier.

Data requirements are modest and online. Computational cost is negligible. Look-back sensitivity is controlled by thresholds and reference values rather than explicit windows. Real-time applicability is high. Failure modes include sensitivity to heavy tails and volatility clustering, which can trigger false alarms. False-signal characteristics can be mitigated by applying CUSUM to robust statistics, such as winsorized returns or volatility estimates, but such modifications must be tested.

#### 2.D.3. Clustering-based regime detection using K-means and Gaussian Mixture Models (GMM)

Clustering methods treat regime detection as unsupervised segmentation of feature vectors. Let \(f_t\) be a feature vector containing quantities such as return, realized volatility, volume change, ADX, and bandwidth. K-means partitions the feature space into \(K\) clusters minimizing within-cluster squared distances (MacQueen, 1967, â€œSome Methods for Classification and Analysis of Multivariate Observations,â€ *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability*). GMM assumes data are generated from a mixture of Gaussians with mixing weights \(\pi_k\) and parameters \((\mu_k,\Sigma_k)\), typically estimated via EM (Dempster, Laird, and Rubin, 1977, â€œMaximum Likelihood from Incomplete Data via the EM Algorithm,â€ *Journal of the Royal Statistical Society: Series B*).

Data requirements increase with feature dimension and cluster count. Computational cost is moderate. Look-back sensitivity arises from the choice of training window and feature normalization. Real-time applicability is moderate: one can assign clusters online, but the cluster definitions may drift, requiring periodic retraining. Failure modes include instability of cluster labels over time, clustering by volatility rather than by tradability, and sensitivity to feature scaling. False signals can be frequent near cluster boundaries, and transitions can appear as rapid alternation between clusters, which is informative as â€œuncertaintyâ€ but not necessarily as a stable regime label.

#### 2.D.4. XGBoost and Random Forest classifiers for regime prediction with engineered features

Supervised learning approaches map features \(f_t\) to a regime label \(y_t\) using models such as Random Forests (Breiman, 2001, â€œRandom Forests,â€ *Machine Learning*) or gradient-boosted decision trees such as XGBoost (Chen and Guestrin, 2016, â€œXGBoost: A Scalable Tree Boosting System,â€ *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*). The central difficulty is label definition. If labels are derived from future returns or future strategy performance, there is an inherent risk of look-ahead bias unless labels are constructed carefully with purging and embargo methods and evaluated in a nested walk-forward framework (Lopez de Prado, 2018, *Advances in Financial Machine Learning*, Wiley).

Data requirements can be large because featureâ€“label relationships are unstable and because overfitting risk is high. Computational cost ranges from moderate to high. Look-back sensitivity is complex: the model can implicitly learn windowed patterns, but performance depends on the training windowâ€™s representativeness. Real-time applicability can be high if the model is stable, but in practice frequent retraining is often needed, which can introduce operational complexity and instability. Failure modes include overfitting to transient microstructure patterns, label leakage, and degradation under regime shifts. False signals can appear as confident predictions that fail catastrophically when the market enters an unseen regime, which is precisely why supervised classifiers must output calibrated probabilities and must be coupled with uncertainty-based sizing reduction.

### 2.E. Fractal and Chaos-Based Methods

Fractal and chaos-based methods aim to characterize complexity, scaling, and deterministic instability. Their conceptual appeal is high, but empirical reliability in noisy financial data is often contested, and the authorâ€™s knowledge of the strongest consensus empirical results in this area is incomplete.

#### 2.E.1. Fractal dimension analysis

Fractal dimension attempts to quantify the roughness of a time series path. For example, for the graph of a time series, a higher fractal dimension indicates greater roughness and, heuristically, more â€œrange-likeâ€ behavior, while a lower dimension indicates smoother trajectories consistent with trending. Methods include box-counting, Higuchiâ€™s method (Higuchi, 1988, â€œApproach to an Irregular Time Series on the Basis of the Fractal Theory,â€ *Physica D*), and Katzâ€™s method (Katz, 1988, â€œFractals and the Analysis of Waveforms,â€ *Computers in Biology and Medicine*).

Data requirements are moderate to high depending on estimator. Computational cost is moderate. Look-back sensitivity is high because estimates vary with scale range. Real-time applicability is limited because estimates are noisy in short windows and can be biased by nonstationary volatility. Failure modes include confusing volatility increases with changes in fractal structure. False-signal characteristics are significant, particularly when sampling frequency changes or when market microstructure effects dominate at high frequency.

#### 2.E.2. Lyapunov exponent estimation for detecting deterministic chaos

The largest Lyapunov exponent \(\lambda_{\max}\) measures sensitivity to initial conditions; \(\lambda_{\max}>0\) indicates exponential divergence consistent with chaos. Estimation from observed time series typically requires phase-space reconstruction via delay embedding and then tracking divergence of nearby trajectories (Wolf et al., 1985, â€œDetermining Lyapunov Exponents from a Time Series,â€ *Physica D*). In finance, the signal-to-noise ratio and nonstationarity complicate interpretation. Positive estimated exponents can reflect stochastic volatility and noise rather than deterministic chaos.

Data requirements are high and methods are fragile in noisy data. Computational cost is high. Look-back sensitivity is high. Real-time applicability is low for production regime gating, and the method is better treated as exploratory analysis. Failure modes and false-signal characteristics are severe; it is easy to obtain apparently significant estimates that are artifacts of noise, sampling, or nonstationarity. This is an area where the empirical consensus is not robust, and my knowledge of the most decisive evidence for or against practical trading utility is incomplete.

#### 2.E.3. Detrended Fluctuation Analysis (DFA) revisited as a fractal scaling tool

Although DFA was discussed as a Hurst estimator, it is also used more broadly as a fractal scaling diagnostic. Its strengths and weaknesses in financial contexts are similar: it can characterize scaling behavior in the presence of certain trends, but it is sensitive to regime shifts and volatility clustering. As a regime tool, DFA-based \(H\) estimates can complement more direct dependence measures, especially when combined in an ensemble rather than used alone.

### 2.F. Comparative summary matrix across methods

Because regime identification is a multi-constraint engineering problem rather than a single statistical test, a comparative summary is useful for design decisions. The following matrix provides a qualitative operational comparison across methods. The entries are necessarily approximate and depend on implementation, asset, frequency, and market microstructure. In the matrix, â€œMin \(N\)â€ denotes a typical minimum number of observations for a reasonably stable rolling estimate at daily frequency in liquid markets, â€œGranâ€ denotes typical granularity suitability, â€œCompâ€ denotes computational cost, â€œLatâ€ denotes signal latency, â€œRTâ€ denotes real-time suitability, â€œLBSâ€ denotes look-back sensitivity, and â€œFSRâ€ denotes false-signal rate risk.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ Method                        â”‚ Min N  â”‚ Gran â”‚ Comp â”‚ Lat  â”‚ RT â”‚ LBS â”‚ FSR â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Hurst (R/S)                   â”‚ 500+   â”‚ D/ID â”‚ Med  â”‚ Med  â”‚ Y  â”‚ Highâ”‚ Highâ”‚
â”‚ Hurst (DFA)                   â”‚ 500+   â”‚ D/ID â”‚ Med  â”‚ Med  â”‚ Y  â”‚ Highâ”‚ Med â”‚
â”‚ Variance Ratio (VR)           â”‚ 250+   â”‚ D/ID â”‚ Low  â”‚ Med  â”‚ Y  â”‚ Med â”‚ Med â”‚
â”‚ ADF (unit root / station.)    â”‚ 500+   â”‚ D    â”‚ Low  â”‚ High â”‚ Y* â”‚ Highâ”‚ Med â”‚
â”‚ ACF + Ljungâ€“Box               â”‚ 250+   â”‚ D/ID â”‚ Low  â”‚ Low  â”‚ Y  â”‚ Med â”‚ Med â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ ADX (Wilder)                  â”‚ 50+    â”‚ D/ID â”‚ Low  â”‚ High â”‚ Y  â”‚ Med â”‚ Med â”‚
â”‚ Bollinger Bandwidth (BBW)     â”‚ 50+    â”‚ D/ID â”‚ Low  â”‚ Med  â”‚ Y  â”‚ Highâ”‚ Med â”‚
â”‚ MA slope / crossover density  â”‚ 50+    â”‚ D/ID â”‚ Low  â”‚ Med  â”‚ Y  â”‚ Highâ”‚ Med â”‚
â”‚ ATR regimes                   â”‚ 50+    â”‚ D/ID â”‚ Low  â”‚ Med  â”‚ Y  â”‚ Med â”‚ Med â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Markov Regime Switching (MS)  â”‚ 500+   â”‚ D    â”‚ Med  â”‚ Med  â”‚ Y  â”‚ Med â”‚ Med â”‚
â”‚ HMM (multifeature)            â”‚ 1000+  â”‚ D/ID â”‚ Med  â”‚ Med  â”‚ Y  â”‚ Med â”‚ Med â”‚
â”‚ TAR / STAR                    â”‚ 1000+  â”‚ D    â”‚ High â”‚ High â”‚ Y* â”‚ Highâ”‚ Highâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Online Bayesian CPD (BOCPD)   â”‚ 100+   â”‚ D/ID â”‚ Med  â”‚ Low  â”‚ Y  â”‚ Low â”‚ Med â”‚
â”‚ CUSUM (mean/var shift)        â”‚ 100+   â”‚ D/ID â”‚ Low  â”‚ Low  â”‚ Y  â”‚ Low â”‚ Med â”‚
â”‚ Clustering (K-means/GMM)      â”‚ 1000+  â”‚ D/ID â”‚ Med  â”‚ Med  â”‚ Y* â”‚ Highâ”‚ Highâ”‚
â”‚ RF / XGBoost classifier       â”‚ 5000+  â”‚ D/ID â”‚ High â”‚ Med  â”‚ Y* â”‚ Highâ”‚ Highâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Fractal dimension             â”‚ 1000+  â”‚ D/ID â”‚ Med  â”‚ High â”‚ Y* â”‚ Highâ”‚ Highâ”‚
â”‚ Lyapunov exponent             â”‚ 5000+  â”‚ ID/T â”‚ High â”‚ High â”‚ N  â”‚ Highâ”‚ Highâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

A sober reading of this comparison is that no single method simultaneously achieves low data requirements, low latency, low false signals, and high interpretability. Consequently, regime identification for production trading is better treated as an ensemble inference problem in which multiple imperfect indicators are fused, and uncertainty itself is made a first-class output that controls aggressiveness and strategy switching.

---

## 3. The Regime Transition Problem (Critical Section)

The most dangerous scenario for adaptive trading systems is not a stable trending regime or a stable range-bound regime, but the transition interval between them. In that interval, momentum and mean-reversion can simultaneously fail because the assumptions underlying both are violated. Momentum fails when persistence decays and trend signals whipsaw; mean reversion fails when the mean itself shifts, when ranges expand into breakouts, or when volatility regimes change in ways that invalidate stationarity assumptions for reversion. In institutional terms, the transition interval is often the dominant contributor to maximum drawdown, because it couples two adverse effects: increased signal error and increased realized volatility.

Formally, one can interpret the transition as a period in which the conditional distribution \(p(r_{t+1}\mid \mathcal{F}_t)\) changes in ways that are not captured by the model trained on previous data, where \(\mathcal{F}_t\) is the information set. If a trend-following strategy relies on \(\mathbb{E}[r_{t+1}\mid r_{t}]>0\) when \(r_t>0\), the transition is precisely when this conditional expectation collapses toward zero or reverses sign. If a mean-reversion strategy relies on stable mean and stable conditional variance, the transition is when the mean drifts and the conditional variance shifts, causing â€œreversion tradesâ€ to be run over by a new directional move. In regime-switching models, the transition corresponds to a time when posterior state probabilities are diffuse and the one-step-ahead regime forecast \(p(S_{t+1}\mid \mathcal{F}_t)\) changes rapidly, which is exactly when hard switching is most hazardous.

Early warning signals are therefore best conceptualized not as â€œtrend has endedâ€ declarations but as indicators that regime confidence is falling and that the system should degrade gracefully. A practical early-warning set often includes declining ADX from elevated levels, indicating weakening trend strength; simultaneously increasing Bollinger Bandwidth, indicating rising volatility and potential instability; a moving Hurst estimate crossing toward \(0.5\), indicating loss of persistence at the relevant scale; HMM or Markov-switching posterior probabilities flipping or becoming ambiguous; and volume or liquidity anomalies, such as abnormal volume with rising spreads, which can signal forced flows or thinning liquidity that often accompanies regime shifts. The central point is that no single indicator is reliable at transitions; rather, the co-occurrence pattern and the direction of change matter.

The following ASCII timeline illustrates a hypothetical but operationally representative transition from trending to range-bound. The diagram overlays a stylized price path with indicative readings of ADX, Bollinger bandwidth, moving Hurst, and HMM trend posterior. The numerical values are schematic rather than empirical, intended to convey the qualitative co-movement patterns that often precede strategy failure if unaddressed.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Hypothetical Regime Transition Timeline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Time â†’      t0        t1        t2        t3        t4        t5        t6          â”‚
â”‚                                                                                     â”‚
â”‚ Price        /Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯\__/Â¯\__   __/Â¯\__  _/Â¯\_  _/Â¯\_  _/Â¯\_                      â”‚
â”‚            _/           \     \__/     \__/     \_/     \_/     \_                  â”‚
â”‚            Trend regime            Transition (instability)          Range regime   â”‚
â”‚                                                                                     â”‚
â”‚ ADX         35        38        32        26        19        16        14          â”‚
â”‚ BBW         0.06      0.05      0.07      0.10      0.12      0.09      0.08        â”‚
â”‚ H (roll)    0.62      0.60      0.56      0.51      0.48      0.46      0.47        â”‚
â”‚ p_trend     0.88      0.91      0.74      0.52      0.31      0.22      0.25        â”‚
â”‚                                                                                     â”‚
â”‚ Strategy    Trend-follow (full)  Trend-follow (scaled down)  Mean-revert (scaled)  â”‚
â”‚ switch                                â†“ uncertainty â†‘                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This timeline encodes a regime-management principle: during the transition, the correct â€œstrategyâ€ is often not a different alpha model but a different risk posture. Specifically, when regime confidence is low, the system should reduce exposure, widen diversification, increase hedging, or shift to lower-turnover strategies less sensitive to directionality assumptions. This is consistent with the notion that expected edge is small precisely when model error is large.

Ensemble and adaptive approaches that degrade gracefully during transitions can be framed as decision-making under model uncertainty. A practical approach is to reduce gross exposure as a function of regime posterior entropy. If \(p_t\) denotes the posterior probability of the trending regime, then entropy \(H_t^{(\mathrm{ent})} = -p_t\ln p_t - (1-p_t)\ln(1-p_t)\) is maximized near \(p_t=0.5\), which corresponds to maximal uncertainty. One can define an exposure multiplier \(m_t\) such that \(m_t\) decreases as entropy increases, for example \(m_t = 1 - H_t^{(\mathrm{ent})}/\ln 2\), yielding \(m_t\in[0,1]\). While this functional form is not unique, the principle is robust: uncertainty should reduce leverage.

A second transition-aware mechanism is regime-conditional Kelly sizing. Let a strategyâ€™s expected excess return in regime \(s\) be \(\mu_s\) and variance be \(\sigma_s^2\). In a simplified Kelly framework with normal returns, the Kelly fraction is \(f_s^\star = \mu_s/\sigma_s^2\) (Kelly, 1956, â€œA New Interpretation of Information Rate,â€ *Bell System Technical Journal*). If regime is uncertain, one can compute an effective Kelly fraction under the posterior \(p(S_t=s\mid \mathcal{F}_t)\) as
$$
f_t^\star \approx \frac{\sum_s p_t(s)\mu_s}{\sum_s p_t(s)\sigma_s^2 + \mathrm{Var}_s(\mu_s)},
$$
where the additional term \(\mathrm{Var}_s(\mu_s)\) reflects regime uncertainty in mean estimates. The qualitative implication is that regime uncertainty reduces optimal leverage even if expected return is unchanged, which is precisely the desired de-risking behavior at transitions.

---

## 4. Proposed Composite Regime-Identification Framework

A production-grade regime identification system should be designed as a composite inference engine that fuses heterogeneous signals capturing different aspects of regime structure, and that outputs both a classification and a confidence measure that controls strategy selection and sizing. The design goal is not perfect classification, which is unattainable in nonstationary markets, but rather improved decision quality relative to any single indicator and reduced drawdown during transitions via explicit uncertainty-aware controls.

### 4.1. Signal set and normalization

A minimal composite for the trend-versus-range axis can fuse at least four complementary methods, chosen to reduce correlated failure modes. One workable set consists of a rolling Hurst estimate \( \hat{H}_t \) computed by DFA over a scale range matched to the strategy horizon; an ADX value \( \mathrm{ADX}_t \) capturing trend strength; an HMM posterior probability \(p_t(\mathrm{trend})\) inferred from a multivariate feature vector including returns and volatility proxies; and a variance ratio value \(\mathrm{VR}_t(q)\) for one or more \(q\) values aligned to the holding period.

Because these raw indicators live on different scales, one should normalize them into comparable scores. For example, define standardized scores
$$
z^{(H)}_t = \frac{\hat{H}_t - 0.5}{\sigma_H},\qquad
z^{(\mathrm{ADX})}_t = \frac{\mathrm{ADX}_t - \mathrm{ADX}_0}{\sigma_{\mathrm{ADX}}},\qquad
z^{(\mathrm{VR})}_t = \frac{\mathrm{VR}_t(q) - 1}{\sigma_{\mathrm{VR}}},
$$
where \(\sigma_H\), \(\sigma_{\mathrm{ADX}}\), and \(\sigma_{\mathrm{VR}}\) are robust scale estimates (for example rolling median absolute deviations mapped to standard deviation equivalents). For the HMM posterior, one can use the log-odds
$$
z^{(\mathrm{HMM})}_t = \ln\left(\frac{p_t(\mathrm{trend})}{1-p_t(\mathrm{trend})}\right),
$$
which naturally spans \((-\infty,\infty)\) and is additive under independent evidence assumptions.

The composite regime score \(C_t\) can then be defined as a weighted sum passed through a squashing function to bound it:
$$
C_t = \tanh\!\left(w_H z^{(H)}_t + w_A z^{(\mathrm{ADX})}_t + w_V z^{(\mathrm{VR})}_t + w_M z^{(\mathrm{HMM})}_t\right),
$$
so that \(C_t\in(-1,1)\), with \(C_t>0\) indicating trending propensity and \(C_t<0\) indicating mean-reverting propensity. The use of \(\tanh\) is not mandatory; it is a convenient way to avoid extreme leverage signals when inputs spike.

A separate uncertainty measure \(U_t\) should be computed to capture disagreement among signals and model instability. One can define \(U_t\) as a function of the dispersion of component scores or the entropy of HMM posteriors. For instance, let \(s^{(i)}_t\) denote component contributions; then
$$
U_t = \mathrm{StdDev}_i\!\left(s^{(i)}_t\right) + \kappa \, H_t^{(\mathrm{ent})},
$$
where \(\kappa\) weights HMM entropy. High \(U_t\) indicates that the system should avoid hard switching and reduce exposure.

### 4.2. Composite decision pipeline and flow diagram

The following diagram summarizes the end-to-end pipeline from raw market data through feature computation, signal fusion, regime classification, strategy selection, and position sizing. The key design element is the explicit branch where uncertainty moderates aggressiveness and mitigates transition drawdowns.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Composite Regime Identification and Control                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Raw data (OHLCV, spreads, order flow proxies, corporate actions adjustments)       â”‚
â”‚                     â”‚                                                              â”‚
â”‚                     â†“                                                              â”‚
â”‚  Preprocessing (cleaning, outlier handling, calendar alignment, microstructure)    â”‚
â”‚                     â”‚                                                              â”‚
â”‚                     â†“                                                              â”‚
â”‚  Feature engine                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Returns r_t, realized vol RV_t, BBW_t, ATR_t, ADX_t, VR_t(q), Hurst H_t,     â”‚  â”‚
â”‚  â”‚  liquidity proxies (spread, depth), volume anomalies, whipsaw metrics         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                                              â”‚
â”‚                     â†“                                                              â”‚
â”‚  Model inference                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ HMM / MS filter               â”‚     â”‚ Rolling econometrics                  â”‚ â”‚
â”‚  â”‚ p_t(trend), entropy           â”‚     â”‚ VR(q), ACF, DFA-Hurst                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                  â”‚                                     â”‚                         â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                  â†“                                               â”‚
â”‚                     Signal normalization and fusion                               â”‚
â”‚                     C_t = tanh(Î£ w_i z_t^(i))                                     â”‚
â”‚                     U_t = disagreement + entropy                                  â”‚
â”‚                                  â”‚                                               â”‚
â”‚                                  â†“                                               â”‚
â”‚                     Regime classification                                         â”‚
â”‚                     if C_t > +Î¸ and U_t low â‡’ Trending regime                     â”‚
â”‚                     if C_t < âˆ’Î¸ and U_t low â‡’ Mean-reverting regime               â”‚
â”‚                     else â‡’ Transition / Uncertain regime                          â”‚
â”‚                                  â”‚                                               â”‚
â”‚                                  â†“                                               â”‚
â”‚                     Strategy selector and risk controller                          â”‚
â”‚                     Strategy family + position sizing + hedging overlays          â”‚
â”‚                                  â”‚                                               â”‚
â”‚                                  â†“                                               â”‚
â”‚                     Execution and monitoring (slippage, fill quality, drift)      â”‚
â”‚                                  â”‚                                               â”‚
â”‚                                  â†“                                               â”‚
â”‚                     Feedback loop (recalibration, walk-forward, diagnostics)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3. Scoring logic in pseudocode

The pseudocode below is intentionally explicit about two outputs, a signed regime score \(C_t\) and an uncertainty score \(U_t\), because production failures often arise when a system produces regime labels without representing uncertainty. The code is schematic and omits engineering details such as transaction cost modeling and data integrity checks.

```text
Inputs at time t:
  OHLCV history up to t, optional microstructure features up to t
  Hyperparameters: windows, q for VR, HMM state count K, thresholds Î¸ and u_max
  Weights: w_H, w_ADX, w_VR, w_HMM

Compute features:
  r_t        = log(P_t) - log(P_{t-1})
  ADX_t      = Wilder_ADX(OHLC, window_A)
  VR_t       = VarianceRatio(returns_window, q)
  H_t        = DFA_Hurst(log_price_window, scales = S)
  HMM_post   = HMM_Filter(x_{1:t})   where x_t = (r_t, |r_t|, RV_t, BBW_t, ...)
  p_trend_t  = HMM_post[trend_state]
  ent_t      = -Î£_j p_j,t * log(p_j,t)

Normalize:
  z_H     = (H_t - 0.5) / robust_scale_H
  z_ADX   = (ADX_t - ADX_0) / robust_scale_ADX
  z_VR    = (VR_t - 1.0) / robust_scale_VR
  z_HMM   = log(p_trend_t / (1 - p_trend_t))

Fuse:
  raw_score = w_H*z_H + w_ADX*z_ADX + w_VR*z_VR + w_HMM*z_HMM
  C_t       = tanh(raw_score)

Uncertainty:
  component_scores = [w_H*z_H, w_ADX*z_ADX, w_VR*z_VR, w_HMM*z_HMM]
  disagreement     = StdDev(component_scores)
  U_t              = disagreement + Îº*ent_t

Classify:
  if U_t > u_max:
      regime = "Transition/Uncertain"
  else if C_t > +Î¸:
      regime = "Trending"
  else if C_t < âˆ’Î¸:
      regime = "MeanReverting"
  else:
      regime = "Transition/Uncertain"

Risk control:
  exposure_multiplier m_t = clamp(1 - U_t/u_max, 0, 1)
  apply regime-conditional sizing and strategy mapping using m_t
```

### 4.4. Calibration, walk-forward optimization, and validation methodology

Parameter calibration must respect the fact that regime identification is inherently backward-looking and that the marketâ€™s regime structure drifts. Consequently, the calibration problem is not to find fixed optimal parameters, but to design a process that maintains acceptable performance under drift. Walk-forward optimization should be used, where parameters are trained on an in-sample window and evaluated on a subsequent out-of-sample window, repeated across time. Critically, this should be done in a nested fashion when the composite system includes hyperparameters such as HMM state count, feature selection, and thresholds \((\theta, u_{\max})\), because selecting these on the full history induces optimistic bias.

Backtesting should integrate realistic transaction cost modeling, including slippage and spread dynamics that vary by volatility regime, because regime-switching strategies often increase turnover during uncertain periods, precisely when costs rise. If intraday or microstructure features are used, the backtest must incorporate timestamp alignment and avoid using information that would not have been available at decision time. For supervised models or any label derived from forward returns, purged and embargoed cross-validation is recommended to mitigate leakage due to overlapping labels and serial dependence (Lopez de Prado, 2018, Wiley). Even for unsupervised regime detection, leakage can occur if model parameters are fitted using future data and then used in the past, so re-estimation must be strictly forward-only.

A practical minimum data requirement for the composite system depends on the most data-hungry component. For daily OHLCV-only implementations, a reasonable institutional baseline is at least 252 trading days for computing stable volatility and indicator statistics and at least 500 to 1000 days for HMM calibration if multiple features are used, with the caveat that too long a window can mix regimes. If the HMM is retrained monthly, a rolling training window on the order of two to five years of daily data often provides a compromise between sample size and stationarity, but this must be empirically validated per asset and market. If intraday features are included, the required history can be measured in traded days rather than calendar years, but the stability of microstructure features across market structure changes must be tested explicitly.

---

## 5. Practical Strategy Mapping

A regime classifier is only as valuable as its downstream decision rules. The mapping from regime classification to strategy selection must be explicit, conservative during uncertainty, and consistent with the horizon and instrument constraints. The high-level mapping aligns with the initial insight: trending regimes favor momentum and trend-following strategies, range-bound regimes favor mean reversion, and transition regimes favor capital preservation and robust risk overlays.

In a trending regime, strategies such as breakouts, time-series momentum, and trend-following with trailing stops are structurally aligned with persistence. Operationally, entries can be based on volatility-adjusted breakouts, moving-average filters, or channel breaks, while exits and risk control are typically handled by trailing stops and volatility-scaled position sizing. The key is to avoid overfitting entry rules; the primary edge is often in aligning with persistence and letting profits run while cutting losses when the trend breaks.

In a range-bound regime, mean-reversion strategies such as RSI extremes, Bollinger band touches, or reversion to a local mean can be structurally aligned with anti-persistence. Entries typically occur on deviations from a central tendency measure, and exits occur on reversion toward that center or on stop-outs when the range breaks. The principal risk is that a range can become a breakout, which is precisely why transition detection and uncertainty-based sizing are essential.

In a transition regime, the appropriate default is not to choose between momentum and mean reversion with high leverage, but to reduce risk, possibly add hedges, and shift to strategies with less sensitivity to directionality, such as volatility targeting, carry with strict risk limits, or market-neutral exposures, depending on mandate. Transition regimes are also where â€œdo lessâ€ is a competitive advantage, because avoiding drawdown improves geometric compounding and preserves risk budget for clearer conditions.

The following decision tree expresses strategy selection logic based on the composite regime score \(C_t\) and uncertainty \(U_t\). The diagram emphasizes that uncertainty overrides directional classification.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Strategy Selection Decision Tree                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Start: compute composite regime score C_t and uncertainty U_t          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Is U_t > u_max ?  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚Yes
                  â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Transition/Uncertain regime               â”‚
     â”‚ Use capital-preservation playbook         â”‚
     â”‚ Reduce gross, widen diversification,      â”‚
     â”‚ prefer hedged or low-turnover exposures   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚No
                  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Is C_t > +Î¸ ?     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚Yes
                  â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Trending regime                            â”‚
     â”‚ Deploy momentum / trend-following          â”‚
     â”‚ Breakouts, trailing stops, vol targeting   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚No
                  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Is C_t < âˆ’Î¸ ?     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚Yes
                  â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Mean-reverting regime                      â”‚
     â”‚ Deploy contrarian / mean-reversion         â”‚
     â”‚ Band touches, RSI extremes, reversion exitsâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚No
                  â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Transition/Low-confidence regime           â”‚
     â”‚ Scale down, tighten risk, avoid hard flips â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

To complement the decision tree, the following decision matrix summarizes regime-to-strategy mapping, including typical position sizing posture and risk overlays. This matrix is not intended as an exhaustive strategy catalog but as an institutional template that enforces consistent behavior under regime uncertainty.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Regime                  â”‚ Primary strategy family       â”‚ Position sizing posture   â”‚ Risk management emphasis â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Trending (C_t â‰« 0, U low)â”‚ Momentum / trend-following   â”‚ Higher, vol-targeted      â”‚ Trailing stops,            â”‚
â”‚                         â”‚ Breakouts, time-series mom    â”‚ with cap on leverage      â”‚ convex loss control        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mean-reverting (C_t â‰ª 0, â”‚ Mean reversion / contrarian  â”‚ Moderate, tighter limits  â”‚ Range-break stops,          â”‚
â”‚ U low)                  â”‚ Bands, oscillator extremes    â”‚ due to crash risk         â”‚ mean stability checks       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transition/Uncertain     â”‚ Capital preservation /        â”‚ Reduced gross, lower      â”‚ Reduce turnover,            â”‚
â”‚ (U high or |C_t| small)  â”‚ hedged, robust overlays      â”‚ concentration, optional   â”‚ volatility shock controls   â”‚
â”‚                         â”‚                               â”‚ market neutrality         â”‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random-walk-like         â”‚ Low expected directional edge â”‚ Minimal directional risk  â”‚ Cost minimization,          â”‚
â”‚ (signals weak, costs high)â”‚ or alternative premia       â”‚ or factor/relative value  â”‚ diversify, avoid overtradingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Risk management must be regime-conditional because the distribution of adverse excursions changes across regimes. In trending regimes, adverse excursions are often dominated by sharp reversals and gap risks at the end of trends; thus, trailing stops and volatility targeting are effective, and one should avoid tight mean-reversion-style stops that prematurely exit profitable trends. In mean-reverting regimes, adverse excursions are dominated by range breaks and trend initiations; thus, stop placement should explicitly account for the probability of breakout, often using volatility-scaled stops and regime-aware filters that reduce exposure when early breakout signals appear. In transition regimes, the first-order risk objective is drawdown containment; thus, exposure reduction, hedging, and turnover reduction are typically superior to aggressive strategy switching.

Portfolio-level â€œheatâ€ should also be adjusted by regime. When multiple assets share the same macro-driven regime, correlations rise, and the effective portfolio leverage increases. A regime system should therefore include cross-asset regime coherence measures, such as the fraction of assets classified as trending with high confidence, and reduce aggregate risk when coherence is high and uncertainty is rising, because that combination often precedes systemic de-risking episodes.

---

## 6. Limitations, Open Questions, and Recommendations

The fundamental limitation of any regime identification approach is that it is, in practice, backward-looking. Regimes are inferred from historical data, and the more stringent the confirmation criteria, the greater the latency. This produces an unavoidable trade-off: reducing false signals requires more evidence and thus increases the probability that the detected regime is already ending. This is not a flaw of a particular model but a consequence of inference under nonstationarity. The operational objective must therefore be reframed: rather than attempting to â€œcallâ€ regimes early with high confidence, one should aim to avoid catastrophic misclassification and to manage exposure adaptively as confidence evolves.

Look-ahead bias is a pervasive risk in regime classification research, especially when labels are implicitly derived from future performance. Even unsupervised regime models can leak information if parameters are fit on full-sample data and then used to infer past states. Therefore, any empirical evaluation must enforce forward-only estimation, walk-forward validation, and rigorous accounting for overlapping windows. For supervised learning, leakage is especially insidious because engineered features can include future information through normalization or through target leakage in labeling; purging and embargo techniques are necessary but not sufficient without careful experimental design.

Another limitation arises from multi-scale regime structure. Trending versus mean-reverting behavior can differ across horizons due to participant heterogeneity, market microstructure, and macro flows. A single regime label at a single time may therefore be an oversimplification. A more faithful representation is a regime surface over horizons, effectively a function \(R(t,\tau)\) that maps time and horizon to regime propensity. Implementing such a surface in production increases complexity but may be necessary for multi-horizon portfolios.

Model mis-specification is a dominant practical risk for probabilistic methods. Gaussian HMMs and simple Markov-switching models can produce misleading confidence when data exhibit heavy tails, jumps, and time-varying transition probabilities. Robust emissions, such as Student-t distributions, mixture emissions, or nonparametric approaches, can help, but they increase estimation complexity and may still be inadequate under structural breaks. Moreover, the notion that regimes follow a stationary Markov chain is itself an approximation; transition probabilities can be time-varying, especially around macro events, liquidity shocks, and policy changes.

The chaos-based and fractal-dimension approaches are conceptually attractive but empirically fragile in noisy, finite samples. In particular, Lyapunov exponent estimation in financial series is difficult to interpret because stochastic volatility and noise can mimic deterministic divergence. The authorâ€™s knowledge of the strongest consensus empirical results on trading utility for Lyapunov-based regime detection is incomplete, and any institutional deployment would require substantial dedicated validation and skepticism.

Despite these limitations, several recommendations follow. First, regime identification should be treated as probabilistic, with explicit uncertainty outputs that directly control leverage and switching aggressiveness. Second, composite ensembles that fuse structurally different signals are preferable to single-indicator gating because they reduce correlated failure modes. Third, transition-aware de-risking is likely to contribute more to long-run performance than marginal improvements in regime classification accuracy, because drawdowns in transitions disproportionately harm compound growth. Fourth, research should prioritize evaluation protocols that are robust to nonstationarity, including walk-forward testing across multiple market eras and stress tests that simulate volatility shocks and liquidity deterioration.

Finally, deep learning approaches such as LSTMs and Transformers may offer improved regime prediction by learning nonlinear temporal dependencies and interactions among features, but they also amplify overfitting risk and interpretability challenges. If pursued, such models should be constrained by careful labeling, strong regularization, calibrated probability outputs, and strict forward-only validation. Hybrid approaches that embed structural econometric constraints into neural models, or that use deep models for feature extraction feeding simpler probabilistic regime filters, may provide a more controlled path. This is an active research area, and my knowledge of the most robust institutional-scale empirical findings up to the present may be incomplete due to the rapid evolution and the proprietary nature of many results.

---

## Appendix: Glossary of Key Terms

**Autocorrelation function (ACF).** The function \(\rho(k)\) measuring linear dependence between \(r_t\) and \(r_{t-k}\). Positive values at relevant lags indicate persistence that is often favorable for momentum strategies, while negative values indicate anti-persistence that is often favorable for mean-reversion strategies.

**Bayesian Online Change-Point Detection (BOCPD).** An online Bayesian method that infers the posterior distribution of the run length since the last change point, enabling sequential detection of structural breaks in the distribution of observations.

**Composite regime score.** A scalar such as \(C_t\in(-1,1)\) formed by fusing normalized regime indicators, intended to summarize trending versus mean-reverting propensity, typically paired with an uncertainty measure \(U_t\).

**Entropy (regime uncertainty).** A measure of uncertainty in a probabilistic regime assignment. For a two-state posterior \(p_t\), entropy is \( -p_t\ln p_t - (1-p_t)\ln(1-p_t)\), which is maximized at \(p_t=0.5\).

**Fractional Brownian motion (fBm).** A self-similar Gaussian process \(B_H(t)\) parameterized by Hurst exponent \(H\), with correlated increments when \(H\neq 0.5\). Used as a conceptual model linking scaling behavior to persistence.

**Fractal Market Hypothesis (FMH).** A view that market dynamics arise from heterogeneous participants across horizons, producing fractal scaling and regime-like behavior inconsistent with a single stationary random-walk model (Peters, 1994).

**Hidden Markov Model (HMM).** A probabilistic model in which an unobserved discrete state \(S_t\) evolves as a Markov chain and generates observed data \(x_t\) via a state-dependent emission distribution; filtering produces regime posterior probabilities \(p(S_t\mid x_{1:t})\).

**Hurst exponent.** A scaling exponent \(H\) governing variance growth with horizon, often estimated from data to infer persistence (\(H>0.5\)) versus anti-persistence (\(H<0.5\)) at a chosen scale range.

**Ljungâ€“Box test.** A statistical test that assesses whether a group of autocorrelations up to lag \(m\) differs from zero, used as a diagnostic for serial dependence.

**Markov regime-switching model.** An econometric model in which parameters such as mean, variance, or autoregressive coefficients depend on a latent Markov state, enabling probabilistic regime inference (Hamilton, 1989).

**Mean reversion.** The tendency for a variable to revert toward a central tendency, often associated with negative serial correlation in returns or stationarity in a spread process.

**Momentum (trend-following).** A strategy family that aligns positions with recent price movement, implicitly exploiting positive serial correlation or persistence.

**Regime transition.** A period in which the statistical structure governing returns, volatility, or liquidity changes, often producing elevated uncertainty and high drawdown risk for both momentum and mean-reversion strategies.

**Variance Ratio (VR).** A statistic comparing the variance of aggregated returns to the variance implied by an i.i.d. random walk; \(\mathrm{VR}(q)>1\) suggests persistence at horizon \(q\), while \(\mathrm{VR}(q)<1\) suggests anti-persistence (Lo and MacKinlay, 1988).

---

## References (selected, cited in text)

Adams, R. P., and MacKay, D. J. C., 2007, â€œBayesian Online Changepoint Detection,â€ arXiv preprint (publication venue varies across versions; archival citation may differ).

Bollinger, J., 2001, *Bollinger on Bollinger Bands*, McGrawâ€“Hill.

Breiman, L., 2001, â€œRandom Forests,â€ *Machine Learning*.

Chen, T., and Guestrin, C., 2016, â€œXGBoost: A Scalable Tree Boosting System,â€ *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*.

Dempster, A. P., Laird, N. M., and Rubin, D. B., 1977, â€œMaximum Likelihood from Incomplete Data via the EM Algorithm,â€ *Journal of the Royal Statistical Society: Series B*.

Dickey, D. A., and Fuller, W. A., 1979, â€œDistribution of the Estimators for Autoregressive Time Series with a Unit Root,â€ *Journal of the American Statistical Association*.

Fama, E. F., 1970, â€œEfficient Capital Markets: A Review of Theory and Empirical Work,â€ *Journal of Finance*.

Hamilton, J. D., 1989, â€œA New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle,â€ *Econometrica*.

Higuchi, T., 1988, â€œApproach to an Irregular Time Series on the Basis of the Fractal Theory,â€ *Physica D*.

Hurst, H. E., 1951, â€œLong-Term Storage Capacity of Reservoirs,â€ *Transactions of the American Society of Civil Engineers*.

Kelly, J. L., 1956, â€œA New Interpretation of Information Rate,â€ *Bell System Technical Journal*.

Kim, C.-J., and Nelson, C. R., 1999, *State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications*, MIT Press.

Katz, M. J., 1988, â€œFractals and the Analysis of Waveforms,â€ *Computers in Biology and Medicine*.

Lo, A. W., and MacKinlay, A. C., 1988, â€œStock Market Prices Do Not Follow Random Walks: Evidence from a Simple Specification Test,â€ *Review of Financial Studies*.

Lopez de Prado, M., 2018, *Advances in Financial Machine Learning*, Wiley.

Ljung, G. M., and Box, G. E. P., 1978, â€œOn a Measure of Lack of Fit in Time Series Models,â€ *Biometrika*.

MacQueen, J., 1967, â€œSome Methods for Classification and Analysis of Multivariate Observations,â€ *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability*.

Page, E. S., 1954, â€œContinuous Inspection Schemes,â€ *Biometrika*.

Peng, C.-K., Buldyrev, S. V., Havlin, S., Simons, M., Stanley, H. E., and Goldberger, A. L., 1994, â€œMosaic Organization of DNA Nucleotides,â€ *Physical Review E*.

Peters, E. E., 1994, *Fractal Market Analysis: Applying Chaos Theory to Investment and Economics*, Wiley.

Rabiner, L. R., 1989, â€œA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,â€ *Proceedings of the IEEE*.

TerÃ¤svirta, T., 1994, â€œSpecification, Estimation, and Evaluation of Smooth Transition Autoregressive Models,â€ *Journal of the American Statistical Association*.

Tong, H., 1983, *Threshold Models in Non-linear Time Series Analysis*, Springer.

Wilder, J. W., 1978, *New Concepts in Technical Trading Systems*, Trend Research.

Wolf, A., Swift, J. B., Swinney, H. L., and Vastano, J. A., 1985, â€œDetermining Lyapunov Exponents from a Time Series,â€ *Physica D*.


# LONG-LASTING ALPHA FACTORS: An In-Depth Analysis of Volume, Small-Cap, and Low-Price Premiums â€” Persistence, Mechanisms, and Portfolio Implementation  
Prepared for: \[Supervisor Name\]  
Date: February 19, 2026  
Classification: Internal â€” Research Division  

## Abstract

This report evaluates three candidate equity return predictors that are frequently described as â€œlong-lastingâ€ in practice: trading volumeâ€“based signals, the small-cap premium (size effect), and nominal share-priceâ€“based effects. The analysis is structured around a persistence criterion that is stricter than statistical significance in a single backtest and instead emphasizes multi-period stability, cross-market robustness, survivability after realistic implementation frictions, and resilience to post-publication decay. The central conclusion is that each factor contains a defensible empirical core, but that the economically realizable premium is conditional and implementation-constrained. Trading-volume signals are best interpreted as a heterogeneous family of short-horizon predictors whose net alpha is highly sensitive to turnover, microstructure, and the attentionâ€“liquidity channel. The size premium remains controversial in unconditional form but has been substantially â€œrehabilitatedâ€ by quality controls and by newer interpretations that embed corporate events (notably takeover exposure) as a primary driver. ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3)) Nominal price effects are robust as behavioral comovement and catering phenomena, but the return premiumâ€™s sign appears market-dependent, with evidence of a positive low-nominal-price premium in China and a negative low-nominal-price premium in the United States under comparable constructions. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855)) A composite portfolio that combines (i) volume shocks as a visibility/attention proxy, (ii) size conditioned on â€œjunkâ€ controls or takeover exposure, and (iii) a nominal-price signal whose direction is chosen by market microstructure and investor-base diagnostics can be implemented in a capacity-aware manner via residualization, bucket-neutralization, and multi-horizon rebalancing.

---

## 1. Introduction to Factor Investing and the Persistence Question

Modern factor investing can be viewed as an empirical response to the insufficiency of single-factor equilibrium models to explain cross-sectional average returns. The canonical starting point is the Capital Asset Pricing Model (CAPM), formalized in Sharpe (1964, â€œCapital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk,â€ *The Journal of Finance*) and closely related contributions such as Lintner (1965, â€œThe Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets,â€ *The Review of Economics and Statistics*). ([web.stanford.edu](https://web.stanford.edu/~wfsharpe/art/art2.htm)) The empirical â€œfactor eraâ€ in equities is conventionally dated to Fama and Frenchâ€™s integration of size and value into the three-factor model, which formalized SMB (Small Minus Big) and HML (High Minus Low) as systematic return components (Fama and French, 1993, â€œCommon Risk Factors in the Returns on Stocks and Bonds,â€ *Journal of Financial Economics*), and provided publicly replicable constructions that remain widely used in research and practice. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/0304405X93900235)) Carhart (1997, â€œOn Persistence in Mutual Fund Performance,â€ *The Journal of Finance*) then introduced momentum as a fourth factor in the mutual-fund performance context, reinforcing the idea that many apparent â€œalphasâ€ are exposures to systematic return regularities. ([sciepub.com](https://www.sciepub.com/reference/405143)) The later Famaâ€“French five-factor model added profitability and investment factors (Fama and French, 2015, â€œA Five-Factor Asset Pricing Model,â€ *Journal of Financial Economics*), while simultaneously the literature expanded into a broad â€œfactor zooâ€ in which hundreds of predictors have been proposed, triggering formal multiple-testing and false-discovery concerns (Harvey, Liu, and Zhu, 2016, â€œâ€¦ and the Cross-Section of Expected Returns,â€ *The Review of Financial Studies*). ([mgetit.lib.umich.edu](https://mgetit.lib.umich.edu/resolve?ctx_enc=info%3Aofi%2Fenc%3AUTF-8&ctx_id=10_1&ctx_tim=2025-05-15+06%3A29%3A29&ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fprimo.exlibrisgroup.com-proquest_cross&rft.atitle=A+five-factor+asset+pricing+model&rft.au=Fama%2C+Eugene+F.&rft.coden=JFECDT&rft.date=2015-04-01&rft.eissn=1879-2774&rft.epage=22&rft.genre=article&rft.issn=0304-405X&rft.issue=1&rft.jtitle=Journal+of+financial+economics&rft.pages=1-22&rft.place=Amsterdam&rft.pub=Elsevier+B.V&rft.spage=1&rft.volume=116&rft_dat=%3Cproquest_cross%3E2778405719%3C%2Fproquest_cross%3E&rft_id=info%3Adoi%2F10.1016%2Fj.jfineco.2014.10.010&rft_pqid=1667757709&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&svc_dat=viewit&url_ctx_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Actx&url_ver=Z39.88-2004))

A persistence-focused research process must therefore distinguish between a statistically detectable in-sample anomaly and an economically implementable, long-lived source of excess return. In this report, â€œpersistenceâ€ is treated as a joint property of (i) statistical stability across multiple subperiods and market environments, (ii) robustness across geographies and institutional settings, (iii) survival after transaction costs, financing costs, and market impact, and (iv) resistance to post-discovery arbitrage and factor crowding. The practical object is the realized, after-cost factor return time series \(f_t\), typically defined as a longâ€“short spread \(f_t = r^{L}_t - r^{S}_t\) for characteristic-sorted portfolios or as the payoff to a linear factor-mimicking portfolio. A factor is â€œpersistentâ€ only if \(E[f_t]\) remains economically material and statistically reliable across regimes, and if the sign and rank-ordering implied by the characteristic do not reverse with minor specification changes.

Two empirical regularities motivate heightened skepticism. First, the multiple-hypothesis-testing problem implies that conventional \(t\)-statistics are not a sufficient hurdle for discovery, especially in a literature with correlated predictors and repeated specification search (Harvey, Liu, and Zhu, 2016). ([academic.oup.com](https://academic.oup.com/rfs/article-abstract/29/1/5/1843824)) Second, post-publication decay is a documented phenomenon: McLean and Pontiff (2016, â€œDoes Academic Research Destroy Stock Return Predictability?,â€ *The Journal of Finance*) show that published predictors tend to exhibit weaker returns after dissemination, consistent with markets learning about mispricing signals and competing away part of the premium. ([dialnet.unirioja.es](https://dialnet.unirioja.es/ejemplar/414389)) Complementary perspectives emphasize that early estimates of â€œtrueâ€ effects are often inflated due to power, selection, and reporting issues (Ioannidis, 2008, â€œWhy Most Discovered True Associations Are Inflated,â€ *Epidemiology*), and that early extreme contradictory estimates can appear during evidence accumulation (Ioannidis and Trikalinos, 2005, â€œEarly Extreme Contradictory Estimates May Appear in Published Research: The Proteus Phenomenonâ€¦,â€ *Journal of Clinical Epidemiology*). ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/18633328/)) These metascience results are not asset-pricing-specific, but they provide an analytically useful lens for understanding why factor returns may â€œdeclineâ€ after discovery, and why implementation should be grounded in economically motivated mechanisms rather than purely statistical patterns.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Signal Discovery â”‚â†’â†’â”‚  In-sample Evidence &     â”‚â†’â†’â”‚  Publication /         â”‚
â”‚  (data mining,    â”‚   â”‚  Specification Search     â”‚   â”‚  Diffusion             â”‚
â”‚  theory, intuition)â”‚  â”‚  (robustness checks)      â”‚   â”‚  (citations, media)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                         â”‚                           â”‚
            â”‚                         â”‚                           â”‚
            â†“                         â†“                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Implementation   â”‚â†â†â”‚  Crowding / Arbitrage     â”‚â†â†â”‚  Post-publication      â”‚
â”‚  Frictions        â”‚   â”‚  (capacity limits,       â”‚   â”‚  Decay / Re-estimation â”‚
â”‚  (costs, impact)  â”‚   â”‚  factor flows)           â”‚   â”‚  (out-of-sample tests) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The remainder of this report therefore treats the three nominated factors as hypotheses about return-generating mechanismsâ€”attention and liquidity provision (volume), limits-to-arbitrage and corporate structure (size), and nominal-price cognition and microstructure (low price)â€”and evaluates persistence in light of mechanism stability and implementation feasibility rather than in-sample significance alone.

```text
Factor-model lineage (stylized timeline)

1964â€“1965        1992â€“1993                 1997                     2015
CAPM             FF cross-section          Carhart                  FF5
(Sharpe;         evidence & FF3            4-factor                 (profitability,
Lintner)         (size + value)            (+ momentum)             investment)

2016â†’
Factor zoo / multiple-testing discipline (Harvey, Liu, Zhu)
```

([web.stanford.edu](https://web.stanford.edu/~wfsharpe/art/art2.htm))

---

## 2. FACTOR 1 â€” Trading Volume

### 2.A. Academic Foundation and Theoretical Mechanisms

Trading volume is not a single â€œfactorâ€ so much as a family of signals that proxy for underlying economic states: information flow, investor attention, disagreement, and liquidity shocks. A cornerstone theoretical linkage between volume and return dynamics is Campbell, Grossman, and Wang (1993, â€œTrading Volume and Serial Correlation in Stock Returns,â€ *The Quarterly Journal of Economics*), who study how return autocorrelation varies with volume and interpret the pattern through a model in which risk-averse liquidity providers (market makers) accommodate noninformational trading pressure. ([academic.oup.com](https://academic.oup.com/qje/article/108/4/905/1899978)) Their mechanism frames volume as a state variable for intermediation risk: high-volume order flow indicates inventory risk absorption, which can move expected returns in a way that is observable as state-dependent serial correlation.

In the cross-section, Gervais, Kaniel, and Mingelgrin (2001, â€œThe High-Volume Return Premium,â€ *The Journal of Finance*) document that stocks with unusually high (low) volume over short windows tend to appreciate (depreciate) in the subsequent month, and interpret the effect through a visibility/recognition channel in which volume shocks increase investor awareness and expand the potential buyer base. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919)) This mechanism is naturally linked to imperfect-information and limited-recognition models such as Mertonâ€™s investor recognition hypothesis (Merton, 1987, â€œA Simple Model of Capital Market Equilibrium with Incomplete Information,â€ *The Journal of Finance*), in which incomplete diffusion of information can generate return premia that depend on the â€œshadow costâ€ of being less widely recognized. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0304405X08001839)) In a closely related empirical microstructure direction, Chordia, Subrahmanyam, and Anshuman (2001, â€œTrading Activity and Expected Stock Returns,â€ *Journal of Financial Economics*) relate trading activity, liquidity, and expected returns, reinforcing the idea that volume is inseparable from the price of immediacy and from cross-sectional variation in trading costs and liquidity risk. ([bibsonomy.org](https://www.bibsonomy.org/bibtex/1ee425f08bd1ee0680d3d2d06d12d9fb3/smicha)) Loh (2010, â€œInvestor Inattention and the Underreaction to Stock Recommendations,â€ *Financial Management*) provides a behavioral bridge by using turnover as an attention proxy and showing that â€œlow attentionâ€ stocks underreact more to information events and exhibit larger post-event drift, implying that volume can proxy for attention constraints that slow incorporation of information into prices. ([ink.library.smu.edu.sg](https://ink.library.smu.edu.sg/lkcsb_research/3031/))

Three canonical theoretical explanations recur across this literature. The sequential information arrival hypothesis (Copeland, 1976, â€œA Model of Asset Trading under the Assumption of Sequential Information Arrival,â€ *The Journal of Finance*) posits that information diffuses sequentially among traders, generating leadâ€“lag relations between volume and volatility/returns as the market transitions through partial-equilibrium states. ([law-journals-books.vlex.com](https://law-journals-books.vlex.com/vid/model-of-asset-trading-855652511)) The mixture of distributions hypothesis (Clark, 1973, â€œA Subordinated Stochastic Process Model with Finite Variance for Speculative Prices,â€ *Econometrica*) views both volume and volatility as jointly driven by an underlying stochastic information-arrival intensity; Andersen (1996, â€œReturn Volatility and Trading Volume: An Information Flow Interpretation of Stochastic Volatility,â€ *The Journal of Finance*) provides an influential empirical formulation of this information-flow perspective. ([tesnewdev.econometricsociety.org](https://tesnewdev.econometricsociety.org/publications/econometrica/1973/01/01/subordinated-stochastic-process-model-finite-variance)) The visibility/attention hypothesis, while expressed in different forms, connects disagreement, short-sale constraints, and limited attention to price formation; Miller (1977, â€œRisk, Uncertainty, and Divergence of Opinion,â€ *The Journal of Finance*) is a canonical model in which divergence of opinion combined with short-sale constraints can bias prices upward by excluding pessimists, implying that proxies for disagreement and attention can forecast returns when arbitrage is constrained. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A32%3Ay%3A1977%3Ai%3A4%3Ap%3A1151-68)) The reportâ€™s speaker also referenced Mayshar (1983) in connection with attention/optimism; within the constraints of the sources retrieved for this report, I can verify Mayshar (1978, â€œInvestorsâ€™ Time Horizon and the Inefficiency of Capital Markets,â€ *The Quarterly Journal of Economics*) as a closely related contribution on horizon and inefficiency, but I have not independently verified the exact bibliographic details of a 1983 Mayshar paper specifically tied to volume-based visibility mechanisms. ([academic.oup.com](https://academic.oup.com/qje/article-abstract/92/2/187/1853550)) This gap does not undermine the central attention mechanism, which is well supported through the volume-as-visibility literature and subsequent attention-proxy work.

A final mechanism of first-order practical importance is liquidity provision itself. In institutional trading, volume shocks are precisely the episodes in which dealers, market makers, and liquidity suppliers must absorb inventory imbalances. The Campbellâ€“Grossmanâ€“Wang framework formalizes why expected returns can adjust to compensate intermediation capacity, implying that â€œvolume factorsâ€ may partly be compensation for providing liquidity during transient demand shocks rather than pure mispricing. ([academic.oup.com](https://academic.oup.com/qje/article/108/4/905/1899978))

```text
Volume literature evolution (selected milestones)

1973   Clark: MDH / information-arrival intensity  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
1976   Copeland: SIAH (sequential diffusion)       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
1993   Campbellâ€“Grossmanâ€“Wang: volume & autocorr   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
2001   Chordiaâ€“Subrahmanyamâ€“Anshuman: activity & ER â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚  â”‚
2001   Gervaisâ€“Kanielâ€“Mingelgrin: high-volume premium â”€â”€â”€â” â”‚â”‚   â”‚  â”‚
2010   Loh: turnover as attention proxy                â”€â”â”‚ â”‚â”‚   â”‚  â”‚
2012   Kaniel et al.: cross-country high-volume premium â”€â”¤â”‚ â”‚â”‚   â”‚  â”‚
2021   High-volume premium linked to macro fundamentals  â”€â”´â”´â”€â”´â”´â”€â”€â”´â”€â”€â”˜
```

([tesnewdev.econometricsociety.org](https://tesnewdev.econometricsociety.org/publications/econometrica/1973/01/01/subordinated-stochastic-process-model-finite-variance))

---

### 2.B. Taxonomy of Volume-Based Signals

Because â€œvolumeâ€ is multidimensional, persistence depends strongly on signal definition. Let \(V_{i,t}\) denote the number of shares traded for stock \(i\) over horizon \(t\) (day, week, month), \(P_{i,t}\) the price, \(SO_{i,t}\) shares outstanding, and \(DV_{i,t} = P_{i,t} V_{i,t}\) dollar volume. A core empirical lesson is that raw volume is mechanically correlated with firm size, price level, and index membership; therefore most implementable signals operate on normalized, abnormal, or residualized measures.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TAXONOMY OF VOLUME-BASED RETURN SIGNALS                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Volume family
â”œâ”€ 1) Level measures (normalization choices)
â”‚   â”œâ”€ Raw share volume:            V
â”‚   â”œâ”€ Dollar volume:               DV = PÂ·V
â”‚   â””â”€ Turnover / liquidity usage:  TO = V / SO
â”‚
â”œâ”€ 2) Abnormal / relative measures (own-history baselines)
â”‚   â”œâ”€ Relative volume:             RV = V / Avg(V)
â”‚   â”œâ”€ Abnormal log-volume:         AV = log(V) âˆ’ Avg(log(V))
â”‚   â””â”€ Abnormal turnover:           ATO = TO âˆ’ Avg(TO)
â”‚
â”œâ”€ 3) Volume dynamics (â€œvolume momentumâ€)
â”‚   â”œâ”€ Trend in turnover:           Î”TO, MA-crossovers
â”‚   â””â”€ Acceleration / surprise:     TO âˆ’ EWMA(TO)
â”‚
â”œâ”€ 4) Joint priceâ€“volume constructs
â”‚   â”œâ”€ Volumeâ€“price divergence:     sign(r) vs sign(Î”V)
â”‚   â”œâ”€ OBV (On-Balance Volume):     cum(sign(r)Â·V)
â”‚   â”œâ”€ VWAP deviation:              (P âˆ’ VWAP)/VWAP
â”‚   â””â”€ Liquidity stress proxies:    volume spikes with spread widening
â”‚
â””â”€ 5) Order-flowâ€“signed volume (data-permitting)
    â”œâ”€ Cumulative delta:            Î£(V_buy âˆ’ V_sell)
    â””â”€ Imbalance metrics:           (V_buy âˆ’ V_sell)/(V_buy + V_sell)
```

The remainder of this subsection discusses construction, interpretation, evidence, and decay patterns for each major branch, with explicit attention to what survives realistic institutional trading constraints.

Raw share volume \(V_{i,t}\) is the most direct observable but is also the least interpretable cross-sectionally, because high \(V\) can reflect either high attention in a large liquid stock or simply high shares outstanding and index-fund turnover. In practice, raw \(V\) is primarily useful for within-stock changes (event detection) rather than as a cross-sectional ranking signal. Its interpretation is therefore typically â€œabnormal participation,â€ which motivates log transforms and baselines. Abnormal log-volume can be defined as
\[
AV_{i,t} = \log(V_{i,t}) - \frac{1}{L}\sum_{k=1}^{L}\log(V_{i,t-k}),
\]
where \(L\) is a trailing lookback window. The log specification reduces scale effects and aligns with empirical regularities in trading activity distributions. A key empirical use case is the high-volume return premium framework, which operationalizes â€œunusualâ€ volume shocks and studies their subsequent returns (Gervais, Kaniel, and Mingelgrin, 2001). ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919)) From an implementation standpoint, abnormal measures tend to be more stable than levels because they reduce the confounding impact of secular trends in market-wide activity and structural changes in trading technology.

Turnover \(TO_{i,t}\) is defined as
\[
TO_{i,t}=\frac{V_{i,t}}{SO_{i,t}},
\]
and is often the preferred normalization when one wants volume to measure â€œownership churnâ€ or the intensity with which the float is traded. Turnover is also the most natural bridge to attention hypotheses, because it scales trading activity by the potential investor base. Loh (2010) uses prior turnover as an attention proxy in the context of recommendation underreaction, suggesting that turnover captures a stockâ€™s salience and investor engagement. ([ink.library.smu.edu.sg](https://ink.library.smu.edu.sg/lkcsb_research/3031/)) The decay characteristic of turnover-based attention signals is generally short-horizon: attention shocks are mean-reverting, and the economic content is strongest near information events, implying higher turnover in the strategy itself if exploited mechanically.

Abnormal turnover is constructed analogously to abnormal volume, for example
\[
ATO_{i,t}=TO_{i,t}-\frac{1}{L}\sum_{k=1}^{L}TO_{i,t-k},
\]
or with an exponential baseline to emphasize recent history. Abnormal turnover is a natural candidate for persistent signals because it is less sensitive to corporate actions (splits change \(P\), not \(TO\)), and because it partially controls for time-varying shares outstanding. Empirically, the core question is whether abnormal turnover is information about mispricing (attention-driven underreaction) or compensation for liquidity provision and inventory risk. Campbell, Grossman, and Wang (1993) support the interpretation that volume states change expected returns through market-maker accommodation of order flow. ([academic.oup.com](https://academic.oup.com/qje/article/108/4/905/1899978))

Volume momentum signals treat trading activity as a trending state variable rather than a one-off shock. A generic construction is a moving-average differential on log-volume or turnover,
\[
VM_{i,t}=\left(\frac{1}{L_{1}}\sum_{k=1}^{L_{1}}\log(TO_{i,t-k})\right)-\left(\frac{1}{L_{2}}\sum_{k=1}^{L_{2}}\log(TO_{i,t-k})\right),
\]
with \(L_{1}<L_{2}\). The interpretation is that rising participation may indicate increasing recognition, sustained information flow, or gradual diffusion of a thesis among investors. The main decay risk is that volume trends are strongly endogenous to price trends, index inclusions, and corporate news cycles, and therefore can inadvertently become a noisy proxy for momentum or for liquidity regime shifts. This is precisely why volume momentum is often implemented jointly with price controls (for example, constructing within-momentum buckets, or explicitly neutralizing exposure to short-term returns).

Volumeâ€“price divergence signals attempt to detect â€œattention without confirmationâ€ or â€œprice move without participation.â€ A simple divergence indicator can be written as
\[
D_{i,t}=\operatorname{sign}(r_{i,t})\cdot \operatorname{sign}(AV_{i,t}),
\]
where \(r_{i,t}\) is the contemporaneous return. The economic hypothesis is that price moves on low participation may be more fragile (higher reversal probability), whereas high-participation moves may be more persistent (higher continuation probability) because they reflect broader belief updating. While this intuition is widespread among practitioners, the academically established evidence base is more clearly anchored in the high-volume premium framework (Gervais et al., 2001) and in volume-conditioned return dynamics (Campbell et al., 1993), rather than in any single canonical â€œdivergenceâ€ statistic. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919)) The practical decay characteristic is that divergence signals are often subsumed by microstructure noise at very short horizons and by momentum/reversal effects at longer horizons, so stability typically requires careful horizon separation and trading-cost control.

OBV (On-Balance Volume) is a cumulative construct,
\[
OBV_{i,t}=OBV_{i,t-1}+\operatorname{sign}(r_{i,t})V_{i,t},
\]
and is most interpretable as a coarse proxy for persistent net buying pressure. The implementability caveat is that OBV is not signed volume in the microstructure sense; it signs volume using price changes, which can confound the construct with volatility and bidâ€“ask bounce. Cumulative delta and related signed-volume measures are more microstructure-accurate:
\[
CD_{i,t}=\sum_{u\in t}\left(V^{buy}_{i,u}-V^{sell}_{i,u}\right),
\]
but require trade classification (or order-book data) and are therefore dataset-dependent. Because this report is scoped to institutionally implementable equity strategies across broad universes, signed-volume features are best treated as â€œoptional enhancementsâ€ in high-frequency or single-market implementations, rather than as the core definitional anchor of a long-horizon alpha factor.

VWAP deviation constructs measure execution-relevant price pressure. If \(VWAP_{i,t}\) is the volume-weighted average price over the day,
\[
VWAP_{i,t}=\frac{\sum_{u\in t}P_{i,u}V_{i,u}}{\sum_{u\in t}V_{i,u}},
\]
then a deviation signal is
\[
DEV_{i,t}=\frac{P^{close}_{i,t}-VWAP_{i,t}}{VWAP_{i,t}}.
\]
The interpretation is that closes far above VWAP on high volume may indicate systematic demand imbalance, whereas closes below VWAP may indicate systematic supply. These are often exploited by short-horizon mean-reversion and liquidity-provision strategies. Their decay is typically fast because they are closely tied to intraday execution patterns that are heavily arbitraged in modern electronic markets.

In summary, â€œvolumeâ€ becomes a candidate persistent factor only after normalization and careful horizon selection. The empirical record supports the existence of high-volume return premia in several settings, but the family resemblance among signals masks substantial heterogeneity in implementability and decay.

---

### 2.C. Factor Decay and Current Status

The central persistence question for volume-based alpha is whether the predictive content documented in the classic literature remains tradeable after modern market structure changes. Two countervailing arguments apply. On one hand, electronic markets, algorithmic execution, and faster information dissemination plausibly reduce underreaction and compress short-horizon anomalies, implying that naÃ¯ve high-turnover volume strategies should suffer from both alpha decay and rising adverse selection. On the other hand, the mechanism emphasized by the canonical papersâ€”visibility, recognition, intermediation risk, and attention constraintsâ€”has not disappeared. If anything, attention allocation may have become more episodic and more uneven across the cross-section, because the information environment is now dominated by high-volume, high-salience names while a long tail of smaller, less-covered firms remains attention-constrained.

Recent evidence supports the view that the high-volume return premium is not merely a U.S.-specific historical artifact. Kaniel et al. (2012, â€œThe High Volume Return Premium: Cross-Country Evidence,â€ *Journal of Financial Economics*) find that the phenomenon appears across many developed and emerging markets and report that risk and liquidity differences do not fully explain it, with patterns consistent with the investor-recognition/visibility channel. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0304405X11001954)) More recently, work in *Journal of Financial Economics* links the high-volume return premium to macroeconomic fundamentals, reporting predictive power for industrial production growth and related indicators while noting that standard factor models do not fully subsume the anomaly (2021, â€œThe High Volume Return Premium and Economic Fundamentals,â€ *Journal of Financial Economics*). ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304405X20302816)) These findings matter for persistence because they suggest that volume-based premia may be partly state-dependent compensation or a systematic mispricing pattern tied to macro conditions, rather than purely a transient microstructure quirk.

Nevertheless, the post-publication decay lens is particularly relevant for volume signals because the literature has been widely disseminated and because many volume strategies are computationally straightforward. McLean and Pontiff (2016) provide general evidence of post-publication return compression across predictors, implying that any simple rule that converts â€œabnormal volumeâ€ into a one-month longâ€“short trade should be assumed to have a smaller after-cost Sharpe today than early-sample estimates suggest. ([dialnet.unirioja.es](https://dialnet.unirioja.es/ejemplar/414389)) The broader metascience evidence that early effects are often inflated (Ioannidis, 2008) and that early contradictory extremes can appear (Ioannidis and Trikalinos, 2005) further motivates conservative priors on the size of any surviving premium. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/18633328/))

The practical assessment, therefore, is conditional. The volume factor is still plausibly â€œaliveâ€ as an informational state variable and as a proxy for attention, but its tradeability depends on implementation design. In particular, volume signals are most credible when they (i) are normalized to reduce mechanical size/price confounds, (ii) are implemented in liquid universes where execution frictions do not dominate, and (iii) are paired with explicit microstructure-aware cost models and capacity constraints. When these conditions are not met, the realized premium is likely to be dominated by spreads, impact, and selection into precisely those names whose trading costs are highest.

Given the above, the recommended institutional stance is that volume is not a standalone â€œlong-lasting alpha factorâ€ in the same sense as value or momentum, but rather a high-dimensional feature set that is best used as a conditional modifier: it improves other signals by identifying attention regimes, liquidity stress, and the likely speed of information incorporation. This stance aligns with both the theoretical foundations (volume as state) and the empirical evidence (premia exist but are horizon- and implementation-sensitive). ([academic.oup.com](https://academic.oup.com/qje/article/108/4/905/1899978))

---

## 3. FACTOR 2 â€” Small-Cap Premium (Size Effect)

### 3.A. Academic Lineage

The size effect enters modern asset pricing through Banz (1981, â€œThe Relationship between Return and Market Value of Common Stocks,â€ *Journal of Financial Economics*), which documents that smaller firms earned higher risk-adjusted returns in historical U.S. data and that the effect is concentrated among the very smallest firms. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/0304405X81900180)) Fama and French (1992, â€œThe Cross-Section of Expected Stock Returns,â€ *The Journal of Finance*) then establish that size and book-to-market jointly capture cross-sectional variation that CAPM beta does not, providing the empirical justification for SMB and HML as systematic factors; Fama and French (1993) operationalize these factors in the three-factor model. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A47%3Ay%3A1992%3Ai%3A2%3Ap%3A427-65))

A persistent controversy is whether size is an enduring risk premium, a historically contingent anomaly, or a proxy for omitted characteristics. Horowitz, Loughran, and Savin (2000, â€œThe Disappearing Size Effect,â€ *Research in Economics*) document a sharp weakening of the size effect after the early 1980s, with the implication that the unconditional premium may have been period-specific and that the effectâ€™s economic significance is driven by microcaps that are difficult to trade. ([scholars.northwestern.edu](https://www.scholars.northwestern.edu/en/publications/the-disappearing-size-effect)) Schwert (2003, â€œAnomalies and Market Efficiency,â€ *Handbook of the Economics of Finance*) places size among a set of anomalies that appear to weaken after publication and after the creation of implementable vehicles, suggesting a learning/crowding mechanism consistent with market efficiency in the limit. ([nber.org](https://www.nber.org/papers/w9277))

A major â€œrehabilitationâ€ of size in the modern factor literature is Asness, Frazzini, Israel, Moskowitz, and Pedersen (2018, â€œSize Matters, If You Control Your Junk,â€ *Journal of Financial Economics*), which argues that many failures of size are attributable to the poor quality of the smallest firms and that controlling for â€œjunkâ€ characteristics restores a stronger and more stable size premium. ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3)) More recently, Easterwood, Netter, Paye, and Stegemoller (2024, â€œTaking Over the Size Effect: Asset Pricing Implications of Merger Activity,â€ *Journal of Financial and Quantitative Analysis*) propose that merger announcement returns account for virtually all of the measured size premium and that an ex ante takeover exposure characteristic explains expected returns better than conventional size, with the provocative implication that a takeover factor could replace SMB in benchmark models. ([cambridge.org](https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2))

A practical institutional perspective on the â€œcurrentâ€ size premium also appears in forward-looking capital market assumptions. Vanguardâ€™s research (August 20, 2025, â€œFading small-cap premium and softer U.S. labor marketâ€) notes a decade of U.S. small-cap underperformance, highlights structural quality headwinds in the Russell 2000, and nevertheless reports a capital markets forecast in which small caps are expected to outpace large caps by approximately 1.9 percentage points annualized over the subsequent decade, while emphasizing caution due to weak profitability prevalence. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) Vanguardâ€™s broader 10-year outlook (based on a November 8, 2024 VCMM run) similarly projects higher expected returns and higher volatility for U.S. small caps relative to U.S. large caps. ([advisors.vanguard.com](https://advisors.vanguard.com/insights/article/2025-economic-and-market-outlook))

```text
Size literature evolution (selected milestones)

1981  Banz (JFE): size effect discovery
1992  Famaâ€“French (JOF): size & B/M dominate beta cross-section
1993  Famaâ€“French (JFE): FF3; SMB formalized
2000  Horowitzâ€“Loughranâ€“Savin: â€œdisappearingâ€ size effect
2003  Schwert (HBFE): anomalies weaken post-publication
2018  Asness et al. (JFE): size matters once â€œjunkâ€ is controlled
2024  Easterwood et al. (JFQA): takeover exposure explains size premium
2025  Vanguard: decade underperformance, quality concerns, yet positive long-horizon expectation
```

([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/0304405X81900180))

---

### 3.B. Mechanisms and Explanations

The risk-based interpretation of size is that smaller firms are exposed to systematic risks not captured by CAPM beta, such as distress risk, financing constraints, and sensitivity to macro conditions. This view is consistent with the inclusion of SMB in the Famaâ€“French framework as a risk proxy, but it faces a challenge: if the premium is compensation for risk, it should be more stable than the empirical record suggests. The â€œdisappearanceâ€ debate highlights that the premium is not reliably present in all post-1980s U.S. samples, at least in unconditional form and after excluding microcaps. ([scholars.northwestern.edu](https://www.scholars.northwestern.edu/en/publications/the-disappearing-size-effect))

Behavioral explanations emphasize investor preferences and limits to arbitrage. Small firms have historically had lower analyst coverage, higher information asymmetry, and higher idiosyncratic volatility, conditions under which mispricing can persist. The analyst-coverage channel also suggests a direct interaction with attention proxies such as turnover and volume: if attention is scarce, then a â€œsmall and neglectedâ€ stock can earn a higher return either because it is riskier to hold or because it is more likely to be mispriced and later corrected. Although this reportâ€™s scope is not the full neglected-firm literature, the investor-recognition mechanism formalized by Merton (1987) provides a coherent theoretical scaffold: securities held by a narrower investor base can require higher expected returns. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0304405X08001839))

Structural explanations have become increasingly important in modern samples. One structural argument is that the public small-cap universe has become â€œjunkierâ€ because many high-quality growth firms remain private longer, while lower-quality, cash-burning firms constitute a larger share of the listed small-cap set. This is not merely a narrative claim; it is visible in the elevated fraction of unprofitable firms in small-cap indices. Vanguard reports that through July 2025 almost a third of Russell 2000 constituents were loss-making on annual earnings-per-share measures, and other contemporaneous commentary places the fraction above one-third in some periods and definitions. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) The â€œcontrol your junkâ€ finding of Asness et al. (2018) is directly consistent with this mechanism: the unconditional SMB factor can be diluted or even inverted when the small leg is dominated by low-quality names with poor expected returns net of distress and financing risk. ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3))

Finally, Easterwood et al. (2024) introduce a mechanism that reinterprets what appears to be a size premium as event-driven compensation tied to corporate control. If takeover announcement returns account for most of the measured size premium, then the persistence of â€œsizeâ€ becomes a question about the persistence of takeover exposure and the distribution of M&A activity across firm sizes. This interpretation is especially relevant for portfolio implementation, because it suggests an investable characteristic (takeover exposure) that may have higher signal-to-noise and clearer economic content than market capitalization alone. ([cambridge.org](https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2))

---

### 3.C. Practical Constraints (Implementation-Critical)

The implementability of size is dominated by liquidity and capacity. Banzâ€™s original finding emphasizes that the size effect is not linear and is concentrated among the smallest firms. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/0304405X81900180)) This empirical concentration immediately creates a feasibility issue for institutional portfolios: the segment of the market with the highest theoretical size premium is also the segment with the highest proportional transaction costs, widest spreads, and highest market impact per dollar traded. Horowitz, Loughran, and Savin (2000) explicitly show that removing the smallest firms can eliminate statistical significance, which can be read as an econometric point or as a practical point: the part of the premium that is strongest is exactly where real-world trading is hardest. ([scholars.northwestern.edu](https://www.scholars.northwestern.edu/en/publications/the-disappearing-size-effect))

Capacity limitations are not incidental; they are structural. For a longâ€“short size strategy, the small leg can be difficult to scale without pushing into microcaps, while the short leg can be implemented in large caps at scale. The result is that the strategyâ€™s net capacity is effectively determined by the ability to hold and rebalance the small leg with acceptable impact. For long-only size tilts, capacity is less constrained but the signal purity is weaker because one cannot easily short away the big-cap exposure. For a market-neutral implementation, the portfolio must also manage borrow availability and shorting costs, which are typically more favorable in large caps than in small caps, again pushing the portfolio toward asymmetric implementation risk.

Survivorship bias and index composition effects also matter. Russell-style indices reconstitute annually and have nontrivial turnover, implying that historical â€œsmall-cap indexâ€ returns can embed mechanical reconstitution effects and trading frictions that differ from academic size-sorted portfolios. Although the report does not attempt a full decomposition, it is operationally important that size strategies be tested on survivorship-bias-free universes and be simulated with realistic reconstitution and corporate action handling. The Famaâ€“French factor construction documentation provides a transparent benchmark for academic SMB definitions, which can serve as a reference point for internal replication and diagnostics. ([mba.tuck.dartmouth.edu](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library/f-f_factors.html))

The â€œjunk problemâ€ is now arguably the dominant implementation issue. Vanguard highlights deteriorated profitability in small caps relative to large caps and explicitly cautions that weak profitability and balance-sheet risk warrant careful selection even if valuations imply higher forward returns. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) Pzena similarly discusses that profitability and leverage issues are concentrated within the Russell 2000, and reports that more than 40% of the Russell 2000 can be unprofitable depending on definitions and time, arguing that active selection can mitigate these exposures. ([pzena.com](https://www.pzena.com/americas/institutional-investors/insights/extreme-valuations-durable-fundamentals-the-case-for-small-caps-globally-3q-2025/)) The practical implication for alpha implementation is that a naÃ¯ve â€œsmall minus bigâ€ trade is unlikely to be the dominant institutional expression of the size premium; instead, the implementable version is typically â€œsmall conditioned on quality, liquidity, and sometimes takeover exposure.â€

---

### 3.D. Current Assessment

The question â€œis the small-cap premium dead?â€ is best reframed as â€œunder what conditioning information is size expected to pay today, and can it be captured after costs?â€ Recent evidence in practitioner research indicates that the last decade has been a pronounced period of U.S. small-cap underperformance relative to large caps. Vanguard notes that U.S. small caps have underperformed large caps for a decade and that a narrow set of mega-cap stocks accounts for a large share of large-cap outperformance, while also pointing to structural differences such as sector composition and interest-rate sensitivity. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) RBC Wealth Managementâ€™s discussion of a possible small-cap â€œrenaissanceâ€ shows a large cumulative return gap between large- and small-cap benchmarks over roughly the March 2015 to December 2024 window, with large-cap cumulative returns materially higher than small-cap cumulative returns in that period. ([rbcwealthmanagement.com](https://www.rbcwealthmanagement.com/en-us/insights/is-2025-the-beginning-of-a-small-cap-renaissance))

At the same time, â€œdeadâ€ is too strong a conclusion. First, conditional evidence suggests that size can be resurrected when the small leg excludes low-quality names (Asness et al., 2018). ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3)) Second, the takeover-based reinterpretation implies that what looks like a missing size premium may be a mismeasurement problem: if the premium is in takeover exposure rather than in size itself, then SMB can appear weak even if the underlying event-driven premium persists. ([cambridge.org](https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2)) Third, forward-looking capital market assumptions from Vanguard explicitly forecast a positive small-over-large return differential over the coming decade, while highlighting quality as the primary caveat. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html))

Because this report is prepared on February 19, 2026, it is also important to note that recent short-horizon rotations can occur even during long-horizon underperformance regimes. Market commentary in early 2026 describes episodes of small-cap relative strength, but these should not be conflated with the structural existence of a long-run size premium; they are more plausibly interpreted as regime-dependent reversals or valuation-driven mean reversion that can be episodic. ([marketwatch.com](https://www.marketwatch.com/story/small-stocks-have-crushed-big-ones-this-year-but-that-may-just-be-a-mirage-dcd84284))

The following timeline is a presentation-oriented stylization of U.S. size leadership regimes since 1981. It should be interpreted as a regime map rather than as a precise performance decomposition; a rigorous version should be constructed internally using consistent total-return series and a survivorship-bias-free universe.

```text
U.S. small vs large leadership (stylized regime map, 1981â€“2025)

1981   1987   1994   2000   2007   2009   2013   2018   2020   2025
â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ S> L â”‚  L>S â”‚  L>S â”‚  S>L â”‚  L~S â”‚  S>L â”‚  S>L â”‚  L>S â”‚  S>L â”‚  L>S â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Legend: â€œS>Lâ€ indicates periods commonly characterized by small-cap outperformance,
â€œL>Sâ€ indicates large-cap dominance, and â€œL~Sâ€ indicates mixed leadership.
```

The strategic conclusion is that size is plausibly persistent only in a conditional form that (i) controls â€œjunk,â€ (ii) respects liquidity and capacity, and (iii) potentially replaces raw size with takeover exposure where the objective is to capture the event-driven component highlighted by Easterwood et al. (2024). ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3))

---

## 4. FACTOR 3 â€” Low Nominal Price Effect

### 4.A. Academic Foundation

Nominal share price is theoretically irrelevant in frictionless markets because firms can change price levels through splits and reverse splits without changing fundamentals. The persistence of price-level effects therefore provides a direct diagnostic of behavioral categorization, market frictions, or institutional constraints.

Green and Hwang (2009, â€œPrice-Based Return Comovement,â€ *Journal of Financial Economics*) document that investors appear to group firms by stock price levels, generating comovement patterns that shift after splits in ways inconsistent with purely fundamental explanations. ([bhwang.com](https://bhwang.com/papers.html)) Baker, Greenwood, and Wurgler (2009, â€œCatering through Nominal Share Prices,â€ *The Journal of Finance*) develop and test a catering theory in which managers manage nominal prices to meet investor demand for low- or high-price categories, using time-varying valuation incentives and split behavior as evidence. ([nber.org](https://www.nber.org/papers/w13762)) Birru and Wang (2016, â€œNominal Price Illusion,â€ *Journal of Financial Economics*) provide a behavioral mechanism: investors overestimate the â€œroom to growâ€ and the skewness of low-priced stocks, overweight price levels when forming skewness expectations, and this misperception has tradable implications in options markets. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Aeee%3Ajfinec%3Av%3A119%3Ay%3A2016%3Ai%3A3%3Ap%3A578-598))

A crucial nuance for the â€œlow-price premiumâ€ is that the sign of return predictability is not universal. Zhang (2023, â€œBetting against low nominal prices: Evidence from China,â€ *International Review of Economics & Finance*) reports evidence of a positive low-nominal-price return premium in Chinaâ€™s stock markets and explicitly contrasts it with a negative low-nominal-price return premium in U.S. markets, proposing an explanation via cash-flow and discount-rate betas (Campbell and Vuolteenahoâ€™s two-beta framework) and differences in cash-flow beta ranking across price deciles. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855)) This finding is central for institutional implementation because it implies that nominal price is not a single global factor; rather, it is an interaction between investor-base composition, information environment, and microstructure constraints that can flip the direction of expected returns.

```text
Nominal-price literature evolution (selected milestones)

2009  Greenâ€“Hwang (JFE): price-level categorization â†’ excess comovement
2009  Bakerâ€“Greenwoodâ€“Wurgler (JOF): managerial catering via splits/price targets
2016  Birruâ€“Wang (JFE): nominal price illusion; skewness expectations; options implications
2023  Zhang (IREF): low-price premium positive in China, negative in U.S.; two-beta explanation
```

([bhwang.com](https://bhwang.com/papers.html))

---

### 4.B. Disentangling Price from Size

Nominal price and market capitalization are mechanically correlated because market cap is the product of price and shares outstanding, \(MC_{i,t} = P_{i,t} \cdot SO_{i,t}\). However, the mapping is not one-to-one because shares outstanding vary widely across firms and over time, particularly around splits, buybacks, and equity issuance. The empirical literature therefore treats price-level effects as distinct from size by controlling for market cap and other characteristics, or by using â€œresidual priceâ€ measures that remove the portion of price explained by size and other covariates. Zhang (2023) explicitly discusses residual-price sorting as a robustness device and reports that the main conclusions remain under such controls. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855))

The distinction is implementation-critical. If one mechanically longs â€œlow-priceâ€ stocks in a U.S. universe, the portfolio will typically load heavily on (i) small and microcap names, (ii) high idiosyncratic volatility and distress, and (iii) wider relative tick sizes and spreads. Conversely, in some markets, low price stocks can represent larger, older firms with different share structures, which can change the risk story. Zhang (2023) explicitly notes that Chinese markets exhibit different relationships between nominal prices and market values than those typically observed internationally, which is one reason the premium can differ by market. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855))

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Venn: SIZE vs NOMINAL PRICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                                         â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚            â”‚         SMALL-CAP         â”‚            â”‚       LOW NOMINAL PRICE   â”‚       â”‚
â”‚            â”‚   (low market cap)        â”‚            â”‚   (low share price)       â”‚       â”‚
â”‚            â”‚                           â”‚            â”‚                           â”‚       â”‚
â”‚            â”‚    Often: lower           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Often: wider           â”‚       â”‚
â”‚            â”‚    liquidity,             â”‚   overlap  â”‚    relative tick size,    â”‚       â”‚
â”‚            â”‚    higher idio vol        â”‚            â”‚    delisting risk tail    â”‚       â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                            â”‚                                        â”‚                   â”‚
â”‚                            â”‚   Intersection: â€œpenny microcapsâ€      â”‚                   â”‚
â”‚                            â”‚   (triple-risk region for capacity)    â”‚                   â”‚
â”‚                            â”‚                                        â”‚                   â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                                         â”‚
â”‚ Distinct region examples: a low-price stock can be non-small if shares outstanding is   â”‚
â”‚ high; a small-cap stock can be high-priced if shares outstanding is low (or reverse     â”‚
â”‚ splits occurred).                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The recommended econometric and portfolio-construction practice is to isolate the nominal-price characteristic by residualizing it against size and liquidity variables. A generic cross-sectional residualization is:
\[
\log(P_{i,t})=\alpha_t+\beta_t\log(MC_{i,t})+\gamma_t\log(DV_{i,t})+u_{i,t},
\]
and the nominal-price signal is then \(u_{i,t}\) (or its rank), which by construction removes first-order size and liquidity components.

---

### 4.C. Behavioral Mechanisms

The nominal price illusion mechanism articulated by Birru and Wang (2016) is that investors overweight nominal price levels when forming skewness expectations, leading them to overestimate the upside potential of low-priced stocks and to price associated derivatives accordingly. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Aeee%3Ajfinec%3Av%3A119%3Ay%3A2016%3Ai%3A3%3Ap%3A578-598)) The qualitative heuristic often summarized as â€œcloser to zero, farther from infinityâ€ appears in the broader nominal-price literature and is explicitly referenced in more recent research that links nominal-price measures to momentum profitability, though this report focuses on the foundational JFE evidence and the broader pricing implications. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1544612324009292))

A closely related mechanism is lottery preference. If retail investors demand positively skewed payoffs, they may overpay for securities that appear to offer â€œcheap optionality,â€ which includes low-priced stocks and high-idiosyncratic-volatility names. In markets where retail dominates trading, this channel can be particularly relevant. While the exact retail share varies by measure (trades versus value, and by period), multiple investment-industry sources characterize China A-shares as retail-dominated, often citing that retail accounts for roughly 80% of trading activity by volume. ([fssaim.com](https://www.fssaim.com/lu/en/professional-investor/insights/greater-china/fssaim-tapping-into-a-broader-range-of-investment-opportunities-as-china-market-opens-up.html)) The persistence implication is subtle: retail dominance can strengthen behavioral categorization and attention-driven mispricing, but it can also amplify tail risks and manipulation hazards.

Affordability constraints historically provided a non-behavioral rationale for nominal-price effects: if investors cannot buy fractional shares, low-priced stocks may attract more marginal retail demand. The growth of fractional-share brokerage reduces that channel in some markets, but the existence of nominal price illusion in settings where affordability is less binding suggests that cognition-based mechanisms can persist beyond pure affordability. Birru and Wangâ€™s findings are consistent with a cognition mechanism rather than a simple budget constraint, because the distortion is present in skewness expectations and options pricing rather than only in share-demand quantities. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Aeee%3Ajfinec%3Av%3A119%3Ay%3A2016%3Ai%3A3%3Ap%3A578-598))

---

### 4.D. Risks and Pitfalls

Nominal-price strategies concentrate several distinct risk channels. Delisting risk is a first-order concern because exchanges impose minimum bid/price requirements for continued listing. Nasdaqâ€™s rulebook describes a \(US\$1\) minimum bid price requirement for continued listing, and recent regulatory updates have tightened the consequences for extreme low-price episodes, with accelerated delisting determinations for securities trading at \(US\$0.10\) or below for sustained periods becoming operative on January 19, 2026. ([listingcenter.nasdaqomx.com](https://listingcenter.nasdaqomx.com/rulebook/nasdaq/rules/Nasdaq%205600%20Series)) NYSE rules similarly treat an average closing price below \(US\$1\) over a consecutive 30-trading-day period as a compliance issue that can lead to suspension/delisting if not cured. ([resourcehub.bakermckenzie.com](https://resourcehub.bakermckenzie.com/en/resources/cross-border-listings-guide/north-america/new-york-stock-exchange/topics/principal-listing-and-maintenance-requirements-and-procedures)) These requirements imply that a â€œdeep low-priceâ€ portfolio mechanically embeds a latent delisting-option exposure that is not captured by simple return backtests unless delisting returns and forced liquidation are modeled correctly.

Transaction costs are disproportionately large for low-priced stocks because tick size is discrete while the economic spread is often better measured in percentage terms. If the tick is one cent, then a one-cent minimum spread is \(1\%\) for a \(US\$1\) stock but only \(0.01\%\) for a \(US\$100\) stock. Harris (1994, â€œMinimum Price Variations, Discrete Bidâ€“Ask Spreads, and Quotation Sizes,â€ *The Review of Financial Studies*) formalizes how minimum price variation regulations create discrete spreads and how these effects are particularly relevant for lower-priced stocks. ([academic.oup.com](https://academic.oup.com/rfs/article-abstract/7/1/149/1568491)) This microstructure channel can either create a premium (as compensation for liquidity provision and adverse selection) or destroy implementability (by making round-trip costs too high), depending on turnover and portfolio design.

Finally, low-price portfolios are more exposed to manipulation risk (pump-and-dump dynamics) and to extreme left-tail events. These risks are not ancillary; they are integral to why nominal price can â€œmatter,â€ because the same investor-base and friction mechanisms that create price-level effects also create environments where arbitrage is difficult and where tail events are more frequent.

The actionable inference is that nominal-price signals should be treated as a controlled tilt rather than as a naÃ¯ve â€œbuy the lowest priced stocksâ€ rule. The implementable version typically imposes explicit price floors, liquidity floors, and quality screens, and uses residualization to remove mechanical size and microstructure artifacts.

---

## 5. Cross-Factor Interactions and Multi-Factor Portfolio Construction

### 5.1 Interaction Logic: Why These Three Factors Are Not Additive by Default

The three factors interact through shared underlying primitives: attention, liquidity, and limits to arbitrage. Small-cap stocks tend to trade with lower dollar volume, wider spreads, and lower analyst coverage; low-priced stocks often overlap with small caps and inherit similar microstructure frictions; volume signals are partially measures of attention and liquidity shocks that are amplified precisely where baseline attention is scarce and liquidity is thin. The immediate risk is therefore â€œtriple-loadingâ€: a portfolio that simultaneously tilts to small size, low nominal price, and low baseline liquidity (or high abnormal volume in an illiquid name) can concentrate in a narrow segment of microcap penny stocks with severe execution and tail risks. The implementation goal is thus to capture orthogonal componentsâ€”attention shocks, conditional size, and residual price effectsâ€”rather than simply stacking raw characteristics.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Factor Interaction Matrix (3Ã—3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚   VOLUME (attention/liquidity)  â”‚   SIZE (small-cap)    â”‚  NOMINAL PRICE â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VOLUME               â”‚   Self: horizon choice drives     â”‚ VolumeÃ—Size: attentionâ”‚ VolumeÃ—Price:   â”‚
â”‚                      â”‚   whether signal is reversal or   â”‚ shocks more informativeâ”‚ low-price names â”‚
â”‚                      â”‚   continuation; costs dominate    â”‚ when baseline coverageâ”‚ have large %tick â”‚
â”‚                      â”‚   at high turnover               â”‚ is low; but impactâ†‘    â”‚ & spread â†’ false â”‚
â”‚                      â”‚                                  â”‚                       â”‚ positives riskâ†‘  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SIZE                 â”‚ SizeÃ—Volume: liquidity provision  â”‚ Self: unconditional SMBâ”‚ SizeÃ—Price: high â”‚
â”‚                      â”‚ channel; abnormal volume in small â”‚ controversial; â€œjunkâ€  â”‚ correlation; needâ”‚
â”‚                      â”‚ names may reflect genuine news or â”‚ control & takeover exp.â”‚ residualization  â”‚
â”‚                      â”‚ liquidity shocks                 â”‚ improve persistence     â”‚ to avoid microcapâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NOMINAL PRICE        â”‚ PriceÃ—Volume: manipulation/retail â”‚ PriceÃ—Size: overlap     â”‚ Self: sign differsâ”‚
â”‚                      â”‚ attention channel; regime-dependentâ”‚ region (penny microcaps)â”‚ by market; micro- â”‚
â”‚                      â”‚ sign plausible (US vs China)      â”‚ is where costs explode  â”‚ structure & delistâ”‚
â”‚                      â”‚                                  â”‚                       â”‚ risk dominate     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The matrix implies that the correct portfolio design is not â€œthree independent alphas,â€ but rather a multi-factor system with explicit interaction controls. In particular, the size leg must be protected from junk exposure (Asness et al., 2018) and potentially augmented or replaced by takeover exposure (Easterwood et al., 2024), while the nominal-price leg must be residualized and constrained to avoid delisting and tick-size pathologies. ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3))

---

### 5.2 A Composite Multi-Factor Framework (Actionable Specification)

A robust institutional approach begins by defining the investable universe and specifying the objective function with explicit implementation constraints. Consider a universe \(U_t\) of equities satisfying minimum liquidity and investability filters. Let \(w_{i,t}\) be portfolio weights. A generic longâ€“short, approximately market-neutral objective can be written as:
\[
\max_{w_t}\; w_t^\top \hat{\mu}_t - \frac{\lambda}{2} w_t^\top \hat{\Sigma}_t w_t
\]
subject to neutrality and feasibility constraints such as:
\[
\sum_{i\in U_t} w_{i,t}=0,\qquad
\beta_{m,t}^\top w_t=0,\qquad
|w_{i,t}|\le \bar{w},\qquad
\sum_{i}|w_{i,t}-w_{i,t-1}|\le \text{TurnoverMax},
\]
and liquidity constraints tied to average daily dollar volume \(ADV_{i,t}\), e.g.:
\[
|w_{i,t}|\cdot AUM \le \kappa\cdot ADV_{i,t},
\]
with \(\kappa\) chosen to cap participation.

Expected returns \(\hat{\mu}_t\) are generated from standardized factor scores. Let the three core scores be \(z^{(V)}_{i,t}\) (volume-based), \(z^{(S)}_{i,t}\) (size-based), and \(z^{(P)}_{i,t}\) (nominal-price-based). A composite score can be written as:
\[
s_{i,t}= \omega_V \tilde{z}^{(V)}_{i,t} + \omega_S \tilde{z}^{(S)}_{i,t} + \omega_P \tilde{z}^{(P)}_{i,t},
\]
where the tildes denote orthogonalized versions that remove unintended overlap, for example via cross-sectional regression residuals:
\[
z^{(P)}_{i,t} = a_t + b_t z^{(S)}_{i,t} + c_t z^{(liq)}_{i,t} + \varepsilon^{(P)}_{i,t},
\qquad
\tilde{z}^{(P)}_{i,t}=\varepsilon^{(P)}_{i,t}.
\]
This step is essential given the sizeâ€“price correlation.

For the volume score, an implementable baseline is abnormal turnover:
\[
z^{(V)}_{i,t}=\operatorname{zscore}\!\left(\log(TO_{i,t})-\frac{1}{L}\sum_{k=1}^{L}\log(TO_{i,t-k})\right),
\]
computed within industry-neutral buckets to avoid sectoral trading-activity confounds. The sign should be chosen consistent with the intended horizon; for a one-month horizon consistent with Gervais et al. (2001), the long leg loads on unusually high volume shocks. ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919)) For very short horizons, one would typically invert or refine the signal to capture liquidity-provision mean reversion, but such intraday implementations are beyond the scope of a â€œlong-lasting factorâ€ mandate.

For the size score, raw \(-\log(MC)\) is a starting point, but the implementable version incorporates junk control. A generic formulation is:
\[
z^{(S)}_{i,t} = \operatorname{zscore}\!\left(-\log(MC_{i,t})\right)
\quad\text{computed after conditioning on quality},
\]
where conditioning can be implemented as (i) excluding the bottom tail of profitability and balance-sheet health, or (ii) building the size rank within â€œquality bins.â€ This is directly motivated by Asness et al. (2018). ([sciencedirect.com](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3)) A further enhancement is to replace or augment size with takeover exposure characteristics per Easterwood et al. (2024), which can be treated as an additional score \(z^{(T)}_{i,t}\) whose inclusion reduces reliance on raw size. ([cambridge.org](https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2))

For the nominal price score, the first design choice is whether the strategy is U.S.-centric or global. Zhang (2023) provides direct evidence that the low-nominal-price premium is positive in China but negative in U.S. markets under their tested constructions. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855)) Therefore, a global composite strategy should not hard-code the same sign everywhere. A practical approach is to treat nominal price as a market-specific factor with direction selected by diagnostics of investor-base composition and price informativeness. In retail-dominated settings such as China A-shares, multiple sources characterize retail participation as roughly 80% of trading activity, suggesting that cognition-based categorization may be salient. ([fssaim.com](https://www.fssaim.com/lu/en/professional-investor/insights/greater-china/fssaim-tapping-into-a-broader-range-of-investment-opportunities-as-china-market-opens-up.html)) In U.S. markets, the negative premium interpretation aligns more naturally with lottery-demand overpricing and subsequent underperformance of low-priced stocks, as discussed in Zhang (2023). ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855))

---

### 5.3 Factor Weighting: Equal, Risk-Parity, and Conditional Regime Weighting

An equal-weight composite \(\omega_V=\omega_S=\omega_P\) is appealing as a baseline because it avoids ex post overfitting to historical relative Sharpe. However, equal weighting is not risk-neutral: if one factor has materially higher volatility or tail risk, it will dominate portfolio risk. A risk-parity alternative equalizes marginal contribution to risk across factor sleeves. If the composite is implemented as three sub-portfolios \(w^{(V)}\), \(w^{(S)}\), \(w^{(P)}\), the risk-parity weights satisfy:
\[
\omega_k \cdot \left(\frac{\partial \sqrt{w^\top \Sigma w}}{\partial \omega_k}\right) \approx \text{constant},
\]
which is typically solved numerically under a covariance estimate for sleeve returns. This approach is institutionally sensible when factor volatilities differ, but it can inadvertently overweight low-volatility factors that are capacity constrained, so it must be paired with liquidity-aware caps.

Conditional (regime-dependent) weighting is justified when factor premia are economically state-dependent. The size premium is plausibly stronger in recovery regimes and weaker in late-cycle or high-rate regimes; Vanguardâ€™s discussion of interest-rate sensitivity and deteriorated small-cap quality supports the notion that macro regime and balance-sheet conditions matter for expected size performance. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) Volume-based attention premia may be stronger when attention constraints bind and when liquidity provision is priced; nominal-price effects may strengthen when retail participation rises and when microstructure frictions (tick size, delisting risk) become more binding. Harris (1994) provides a formal basis for why tick-size discreteness matters more at low prices, implying that the nominal-price factorâ€™s risk and implementability are regime- and price-level-dependent. ([academic.oup.com](https://academic.oup.com/rfs/article-abstract/7/1/149/1568491))

Given the data-mining concerns articulated by Harvey, Liu, and Zhu (2016), conditional weighting should be treated as a hypothesis-driven overlay rather than as an unconstrained factor-timing exercise. ([academic.oup.com](https://academic.oup.com/rfs/article-abstract/29/1/5/1843824)) The recommended institutional protocol is to define a small number of economically interpretable regimes (for example, high versus low funding stress; high versus low dispersion/retail participation proxies) and to pre-commit to simple weight rules with explicit guardrails.

---

### 5.4 Portfolio Construction Pipeline (Implementation Diagram)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1) Universe definitionâ”‚  Investable set U_t with liquidity & price floors
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2) Data QC & cleaning â”‚  Corporate actions, split adjustment, survivorship-free
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3) Signal engineering â”‚  z^(V), z^(S|quality), z^(P residual), takeover exposure
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4) Orthogonalization  â”‚  Residualize price vs size/liquidity; bucket-neutralize
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5) Composite scoring  â”‚  s_i = Ï‰_V zÌƒ^V + Ï‰_S zÌƒ^S + Ï‰_P zÌƒ^P (+ Ï‰_T z^T)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6) Optimization       â”‚  Max w'Î¼ âˆ’ (Î»/2)w'Î£w with neutrality & capacity constraints
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7) Trading & executionâ”‚  Cost model, participation caps, staged rebalancing
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8) Monitoring         â”‚  Exposure drift, crowding proxies, realized slippage, decay
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This pipeline is designed to operationalize persistence: the model is not merely a predictor but a system that enforces implementation feasibility and continuously tests whether the hypothesized mechanism remains present after trading frictions and crowding.

---

### 5.5 Rebalancing Frequency and Vehicle Choice

Because the three signals operate on different natural horizons, the recommended approach is multi-horizon rebalancing. Volume-based signals are strongest at short horizons and can justify weekly or even more frequent updates, but turnover must be capped to avoid transforming a potential premium into a transaction-cost transfer. Size and nominal-price characteristics, in contrast, evolve more slowly and are more naturally rebalanced monthly or quarterly, with explicit handling of index reconstitutions and corporate actions.

Regarding vehicles, long-only implementations can capture size and quality-conditioned size through liquid small-cap and small-cap-quality mandates, but they cannot isolate factor returns cleanly because they remain exposed to market beta and benchmark constraints. Longâ€“short implementations are more suitable for extracting pure factor payoffs but face greater operational complexity, shorting costs, and capacity constraintsâ€”especially if the signal pushes into microcaps and low-priced names with limited borrow and high financing costs. A pragmatic institutional compromise is a â€œconstrained longâ€“shortâ€ portfolio that restricts the tradeable universe to sufficiently liquid names and allows residual beta exposure within a tight band, thereby preserving capacity while still harvesting characteristic premia.

---

## 6. Limitations, Controversies, and Recommendations

The core controversy across all three factors is whether the premium is â€œalphaâ€ (mispricing) or â€œbetaâ€ (risk compensation). If a return spread is compensation for bearing liquidity risk, distress risk, or intermediation risk, then it should be treated as a structural risk premium and priced accordingly in client portfolios, not as a pure alpha that should be arbitraged away. Conversely, if it is primarily a behavioral mispricing, then persistence depends on limits to arbitrage and investor composition, and the premium can compress as capital and attention migrate toward it. Schwert (2003) and McLean and Pontiff (2016) provide strong evidence consistent with the view that publication and practitioner adoption can weaken anomalies. ([nber.org](https://www.nber.org/papers/w9277))

The factor-zoo critique is also directly relevant. Harvey, Liu, and Zhu (2016) argue that conventional significance thresholds are too low given widespread data mining and propose higher hurdles for new factor claims. ([academic.oup.com](https://academic.oup.com/rfs/article-abstract/29/1/5/1843824)) Ioannidis (2008) provides a broader methodological warning that newly discovered effects are often inflated, which is consistent with a disciplined approach that prioritizes mechanisms, out-of-sample tests, and conservative expectations for Sharpe ratios net of costs. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/18633328/))

A further limitation is that some components requested by the speaker (notably the specific â€œPrometheus effectâ€ terminology and Mayshar (1983) as cited) could not be fully verified to the exact bibliographic form within the retrieved sources; the closest validated constructs are the Proteus phenomenon (Ioannidis and Trikalinos, 2005) and the more general decline/inflation dynamics in Ioannidis (2008). ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0895435605000235)) In addition, while this report cites cross-country evidence and recent JFE work on the high-volume premium, it does not include a formal meta-analysis of volumeâ€“return predictability because no single authoritative â€œmeta-analysisâ€ source was retrieved within the browsing budget; this is a recommended follow-up item.

The recommended next research steps, consistent with institutional best practice, are to (i) replicate each factor in-house on a survivorship-bias-free dataset with explicit delisting return treatment and corporate action handling, (ii) implement a unified transaction-cost and market-impact model and re-estimate net premia, (iii) measure factor crowding proxies and the sensitivity of premia to AUM scaling, and (iv) test interaction terms explicitly, such as volume shocks conditional on size and analyst-coverage proxies, and nominal-price signals conditional on retail participation regimes.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Final Factor Scorecard (Indicative, Post-Cost) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Factor                  â”‚ Indicative Sharpe â”‚ Capacity (AUM) â”‚ Decay Risk â”‚ Implementability         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Trading Volume signals   â”‚ 0.2â€“0.5           â”‚ Mediumâ€“High*    â”‚ Medium     â”‚ Medium (cost-sensitive)  â”‚
â”‚ Small-cap (Size)         â”‚ 0.1â€“0.4           â”‚ Medium          â”‚ Mediumâ€“High â”‚ Medium (liquidity binds) â”‚
â”‚ Size (quality-controlled)â”‚ 0.3â€“0.7           â”‚ Medium          â”‚ Medium      â”‚ Medium (requires screens)â”‚
â”‚ Takeover exposure (size  â”‚ 0.3â€“0.6           â”‚ Medium          â”‚ Medium      â”‚ Medium (data + events)   â”‚
â”‚ reinterpretation)        â”‚                   â”‚                â”‚             â”‚                          â”‚
â”‚ Nominal price effect     â”‚ 0.1â€“0.5           â”‚ Lowâ€“Medium      â”‚ Medium      â”‚ Lowâ€“Medium (tail risks)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*Capacity for volume signals is high only if the implementation remains in liquid names; signal strength often
increases in less-liquid segments where capacity collapses.
```

The Sharpe and capacity entries are intentionally expressed as indicative ranges rather than point estimates because the realized values are dominated by universe selection, turnover limits, trading-cost modeling, and the sign choice for the nominal-price premium in different markets (Zhang, 2023). ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855)) The tableâ€™s purpose is therefore not to assert a single â€œtrue Sharpe,â€ but to provide an internal decision framework: volume signals are viable as conditional overlays; size is viable when conditioned on quality or replaced with takeover exposure; nominal price is viable only under tight microstructure and delisting-risk controls and with explicit market-dependent sign.

The final institutional recommendation is to proceed with a composite strategy that (i) uses abnormal turnover as an attention/visibility feature rather than as a standalone high-turnover factor, anchored in the high-volume premium literature; (ii) expresses size via quality-controlled small-cap exposure and evaluates takeover exposure as an alternative factor definition; and (iii) includes a residual nominal-price sleeve with strict investability constraints and market-dependent direction. This portfolio should be monitored explicitly for post-publication decay dynamics in the spirit of McLean and Pontiff (2016) and for multiple-testing overfitting risk in the spirit of Harvey, Liu, and Zhu (2016). ([econpapers.repec.org](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919))

---

## Appendix: Glossary of Key Terms and Factor Definitions

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Term                         â”‚ Definition (operational)                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Factor                       â”‚ A systematic return component captured by a longâ€“short payoff series   â”‚
â”‚                              â”‚ or a factor-mimicking portfolio, often tied to a characteristic.        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Persistence                  â”‚ Stability of the sign and economic magnitude of after-cost factor       â”‚
â”‚                              â”‚ payoffs across subperiods, markets, and implementation assumptions.     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Raw volume \(V_{i,t}\)        â”‚ Shares traded for stock \(i\) over horizon \(t\).                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dollar volume \(DV_{i,t}\)    â”‚ \(DV_{i,t} = P_{i,t} V_{i,t}\).                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Turnover \(TO_{i,t}\)         â”‚ \(TO_{i,t} = V_{i,t}/SO_{i,t}\), shares traded scaled by shares out.     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Abnormal volume               â”‚ Deviation of \(\log(V)\) or \(TO\) from a trailing baseline, e.g.        â”‚
â”‚                              â”‚ \(AV_{i,t}=\log(V_{i,t})-\frac{1}{L}\sum_{k=1}^{L}\log(V_{i,t-k})\).      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ High-volume return premium    â”‚ Cross-sectional tendency for unusually high-volume stocks to outperform  â”‚
â”‚                              â”‚ unusually low-volume stocks over a subsequent short horizon.            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SMB                           â”‚ Famaâ€“French size factor: return on small portfolios minus big portfoliosâ”‚
â”‚                              â”‚ per the French Data Library construction.                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€œJunkâ€ (quality) control      â”‚ Conditioning size exposure on profitability/quality to avoid low-quality â”‚
â”‚                              â”‚ small firms that dilute or invert the size premium.                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nominal price effect          â”‚ Return comovement and/or return predictability associated with share     â”‚
â”‚                              â”‚ price levels, despite theoretical irrelevance under splits.              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tick-size effect              â”‚ Discrete minimum price variation implies larger percentage spreads for   â”‚
â”‚                              â”‚ low-priced stocks, affecting costs and potentially expected returns.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Knowledge limitations and â€œas-ofâ€ notes (explicit)

This report is prepared as of February 19, 2026. The empirical claims about small-cap underperformance and profitability composition are anchored to cited sources with explicit dates (for example, Vanguardâ€™s August 20, 2025 discussion through July 2025), and forward-looking forecasts are those reported by the cited institutions rather than produced internally here. ([corporate.vanguard.com](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)) The reportâ€™s treatment of Mayshar (1983) is limited by the inability to verify the exact 1983 reference within the retrieved sources; the broader attention/visibility mechanism is nevertheless supported by the validated literature cited above. ([academic.oup.com](https://academic.oup.com/qje/article-abstract/92/2/187/1853550))

---
Learn more:
1. [https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3](https://www.sciencedirect.com/journal/journal-of-financial-economics/vol/129/issue/3)
2. [https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855](https://www.sciencedirect.com/science/article/abs/pii/S1059056023001855)
3. [https://web.stanford.edu/~wfsharpe/art/art2.htm](https://web.stanford.edu/~wfsharpe/art/art2.htm)
4. [https://www.sciencedirect.com/science/article/pii/0304405X93900235](https://www.sciencedirect.com/science/article/pii/0304405X93900235)
5. [https://www.sciepub.com/reference/405143](https://www.sciepub.com/reference/405143)
6. [https://mgetit.lib.umich.edu/resolve?ctx\_enc=info%3Aofi%2Fenc%3AUTF-8&ctx\_id=10\_1&ctx\_tim=2025-05-15+06%3A29%3A29&ctx\_ver=Z39.88-2004&rfr\_id=info%3Asid%2Fprimo.exlibrisgroup.com-proquest\_cross&rft.atitle=A+five-factor+asset+pricing+model&rft.au=Fama%2C+Eugene+F.&rft.coden=JFECDT&rft.date=2015-04-01&rft.eissn=1879-2774&rft.epage=22&rft.genre=article&rft.issn=0304-405X&rft.issue=1&rft.jtitle=Journal+of+financial+economics&rft.pages=1-22&rft.place=Amsterdam&rft.pub=Elsevier+B.V&rft.spage=1&rft.volume=116&rft\_dat=%3Cproquest\_cross%3E2778405719%3C%2Fproquest\_cross%3E&rft\_id=info%3Adoi%2F10.1016%2Fj.jfineco.2014.10.010&rft\_pqid=1667757709&rft\_val\_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&svc\_dat=viewit&url\_ctx\_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Actx&url\_ver=Z39.88-2004](https://mgetit.lib.umich.edu/resolve?ctx_enc=info%3Aofi%2Fenc%3AUTF-8&ctx_id=10_1&ctx_tim=2025-05-15+06%3A29%3A29&ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fprimo.exlibrisgroup.com-proquest_cross&rft.atitle=A+five-factor+asset+pricing+model&rft.au=Fama%2C+Eugene+F.&rft.coden=JFECDT&rft.date=2015-04-01&rft.eissn=1879-2774&rft.epage=22&rft.genre=article&rft.issn=0304-405X&rft.issue=1&rft.jtitle=Journal+of+financial+economics&rft.pages=1-22&rft.place=Amsterdam&rft.pub=Elsevier+B.V&rft.spage=1&rft.volume=116&rft_dat=%3Cproquest_cross%3E2778405719%3C%2Fproquest_cross%3E&rft_id=info%3Adoi%2F10.1016%2Fj.jfineco.2014.10.010&rft_pqid=1667757709&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&svc_dat=viewit&url_ctx_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Actx&url_ver=Z39.88-2004)
7. [https://academic.oup.com/rfs/article-abstract/29/1/5/1843824](https://academic.oup.com/rfs/article-abstract/29/1/5/1843824)
8. [https://dialnet.unirioja.es/ejemplar/414389](https://dialnet.unirioja.es/ejemplar/414389)
9. [https://pubmed.ncbi.nlm.nih.gov/18633328/](https://pubmed.ncbi.nlm.nih.gov/18633328/)
10. [https://academic.oup.com/qje/article/108/4/905/1899978](https://academic.oup.com/qje/article/108/4/905/1899978)
11. [https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A56%3Ay%3A2001%3Ai%3A3%3Ap%3A877-919)
12. [https://www.sciencedirect.com/science/article/abs/pii/S0304405X08001839](https://www.sciencedirect.com/science/article/abs/pii/S0304405X08001839)
13. [https://www.bibsonomy.org/bibtex/1ee425f08bd1ee0680d3d2d06d12d9fb3/smicha](https://www.bibsonomy.org/bibtex/1ee425f08bd1ee0680d3d2d06d12d9fb3/smicha)
14. [https://ink.library.smu.edu.sg/lkcsb\_research/3031/](https://ink.library.smu.edu.sg/lkcsb_research/3031/)
15. [https://law-journals-books.vlex.com/vid/model-of-asset-trading-855652511](https://law-journals-books.vlex.com/vid/model-of-asset-trading-855652511)
16. [https://tesnewdev.econometricsociety.org/publications/econometrica/1973/01/01/subordinated-stochastic-process-model-finite-variance](https://tesnewdev.econometricsociety.org/publications/econometrica/1973/01/01/subordinated-stochastic-process-model-finite-variance)
17. [https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A32%3Ay%3A1977%3Ai%3A4%3Ap%3A1151-68](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A32%3Ay%3A1977%3Ai%3A4%3Ap%3A1151-68)
18. [https://academic.oup.com/qje/article-abstract/92/2/187/1853550](https://academic.oup.com/qje/article-abstract/92/2/187/1853550)
19. [https://www.sciencedirect.com/science/article/abs/pii/S0304405X11001954](https://www.sciencedirect.com/science/article/abs/pii/S0304405X11001954)
20. [https://www.sciencedirect.com/science/article/pii/S0304405X20302816](https://www.sciencedirect.com/science/article/pii/S0304405X20302816)
21. [https://www.sciencedirect.com/science/article/pii/0304405X81900180](https://www.sciencedirect.com/science/article/pii/0304405X81900180)
22. [https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A47%3Ay%3A1992%3Ai%3A2%3Ap%3A427-65](https://econpapers.repec.org/RePEc%3Abla%3Ajfinan%3Av%3A47%3Ay%3A1992%3Ai%3A2%3Ap%3A427-65)
23. [https://www.scholars.northwestern.edu/en/publications/the-disappearing-size-effect](https://www.scholars.northwestern.edu/en/publications/the-disappearing-size-effect)
24. [https://www.nber.org/papers/w9277](https://www.nber.org/papers/w9277)
25. [https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2](https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/taking-over-the-size-effect-asset-pricing-implications-of-merger-activity/A45B3D276635D8775FB828C259B592F2)
26. [https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html](https://corporate.vanguard.com/content/corporatesite/us/en/corp/vemo/fading-small-cap-premium-softer-us-labor-market.html)
27. [https://advisors.vanguard.com/insights/article/2025-economic-and-market-outlook](https://advisors.vanguard.com/insights/article/2025-economic-and-market-outlook)
28. [https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data\_library/f-f\_factors.html](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library/f-f_factors.html)
29. [https://www.pzena.com/americas/institutional-investors/insights/extreme-valuations-durable-fundamentals-the-case-for-small-caps-globally-3q-2025/](https://www.pzena.com/americas/institutional-investors/insights/extreme-valuations-durable-fundamentals-the-case-for-small-caps-globally-3q-2025/)
30. [https://www.rbcwealthmanagement.com/en-us/insights/is-2025-the-beginning-of-a-small-cap-renaissance](https://www.rbcwealthmanagement.com/en-us/insights/is-2025-the-beginning-of-a-small-cap-renaissance)
31. [https://www.marketwatch.com/story/small-stocks-have-crushed-big-ones-this-year-but-that-may-just-be-a-mirage-dcd84284](https://www.marketwatch.com/story/small-stocks-have-crushed-big-ones-this-year-but-that-may-just-be-a-mirage-dcd84284)
32. [https://bhwang.com/papers.html](https://bhwang.com/papers.html)
33. [https://www.nber.org/papers/w13762](https://www.nber.org/papers/w13762)
34. [https://econpapers.repec.org/RePEc%3Aeee%3Ajfinec%3Av%3A119%3Ay%3A2016%3Ai%3A3%3Ap%3A578-598](https://econpapers.repec.org/RePEc%3Aeee%3Ajfinec%3Av%3A119%3Ay%3A2016%3Ai%3A3%3Ap%3A578-598)
35. [https://www.sciencedirect.com/science/article/abs/pii/S1544612324009292](https://www.sciencedirect.com/science/article/abs/pii/S1544612324009292)
36. [https://www.fssaim.com/lu/en/professional-investor/insights/greater-china/fssaim-tapping-into-a-broader-range-of-investment-opportunities-as-china-market-opens-up.html](https://www.fssaim.com/lu/en/professional-investor/insights/greater-china/fssaim-tapping-into-a-broader-range-of-investment-opportunities-as-china-market-opens-up.html)
37. [https://listingcenter.nasdaqomx.com/rulebook/nasdaq/rules/Nasdaq%205600%20Series](https://listingcenter.nasdaqomx.com/rulebook/nasdaq/rules/Nasdaq%205600%20Series)
38. [https://resourcehub.bakermckenzie.com/en/resources/cross-border-listings-guide/north-america/new-york-stock-exchange/topics/principal-listing-and-maintenance-requirements-and-procedures](https://resourcehub.bakermckenzie.com/en/resources/cross-border-listings-guide/north-america/new-york-stock-exchange/topics/principal-listing-and-maintenance-requirements-and-procedures)
39. [https://academic.oup.com/rfs/article-abstract/7/1/149/1568491](https://academic.oup.com/rfs/article-abstract/7/1/149/1568491)
40. [https://www.sciencedirect.com/science/article/pii/S0895435605000235](https://www.sciencedirect.com/science/article/pii/S0895435605000235)

# ORDER BOOK ARBITRAGE: Detection of Institutional-Scale Orders via Publicly Available Data and Same-Side Execution Strategies â€” Methodology, Implementation, and Regulatory Compliance

Prepared for: \[Supervisor Name]  
Date: **February 19, 2026**  
Classification: **Internal â€” Research Division | COMPLIANCE-SENSITIVE**

âš ï¸ **COMPLIANCE NOTE:** This document is written for internal research and surveillance-aware strategy design. It does not constitute legal advice, an offer to trade, or a solicitation. All deployment decisions should be reviewed by Compliance and counsel in the relevant jurisdiction(s), and must adhere to market-data licensing terms, broker agreements, and venue rules.

---

## Abstract

This report formalizes a compliance-forward â€œsame-sideâ€ order-book strategy that seeks to infer the presence of institutional-scale resting liquidity using only publicly accessible or broker-provided market data, and then trades in the same direction as that inferred liquidity. The core hypothesis is microstructural: unusually large resting bid depth can create transient price support, and unusually large resting ask depth can create transient resistance, thereby yielding a short-horizon conditional edge when entries are aligned with that liquidity and exits are governed by an immediate â€œearly-stopâ€ rule that flattens exposure upon detected order removal. The methodology is explicitly designed to avoid manipulative conduct, particularly the placement of non-bona fide orders, and to avoid strategy patterns that resemble spoofing or layering under U.S. and EU frameworks. The report details order book foundations; multiple low-cost detection channels spanning broker Level 2 depth, Level 1 trade-and-quote inference, time-and-sales tape patterns, and crypto exchange WebSocket order books; an implementation blueprint expressed as a state machine with pseudocode; and a regulatory analysis grounded in the Commodity Exchange Act anti-spoofing provision, relevant CFTC interpretive guidance, EU Market Abuse Regulation provisions, and prominent enforcement matters including *United States v. Coscia* and the CFTCâ€™s 2020 JPMorgan spoofing order. ([law.justia.com](https://law.justia.com/cases/federal/appellate-courts/ca7/16-3017/16-3017-2017-08-07.html?utm_source=openai))

---

## 1. Order Book Microstructure Foundations

A modern electronic limit order market is organized around a limit order book (LOB), which records outstanding buy and sell limit orders across discrete price levels. The bid side consists of queued buy interest at various prices, and the ask side consists of queued sell interest. The best bid \(b_t\) is the highest priced buy limit order available at time \(t\), and the best ask \(a_t\) is the lowest priced sell limit order. The bidâ€“ask spread is \(s_t = a_t - b_t\), and a commonly used reference for the â€œcentralâ€ price is the mid-price \(m_t = \frac{a_t + b_t}{2}\). Market orders consume resting depth immediately (subject to available liquidity), whereas limit orders supply liquidity at a specified price and are executed only if marketable flow reaches them. The matching process typically obeys price priority (better prices execute first) and time priority (earlier orders at the same price execute first), although venue-specific rules and order types can modify these priorities.

Market data is usually partitioned into Level 1, Level 2, and Level 3 representations. Level 1 data provides the top of book, typically including \(b_t\), \(a_t\), last trade price, and sometimes top-level size. Level 2 data (market-by-price depth) provides aggregated visible quantities at a ladder of price levels on each side. Level 3 data (order-by-order) provides individual order updates including unique order identifiers, modifications, cancellations, and full depth reconstruction, and is the â€œgold standardâ€ for attribution of cancellations and queue dynamics. The present research problem is explicitly constrained to operate without exchange-co-located, proprietary Level 3 feeds, and therefore must rely on Level 2 depth where available, or on inference from Level 1 and public prints where depth is limited or aggregated.

The following schematic depicts an aggregated LOB with five levels on each side, highlighting what an anomalously large resting order (or aggregated resting depth) looks like relative to surrounding levels. In practice, because Level 2 is market-by-price, the â€œlarge orderâ€ is often observed as an unusually large quantity at one price level rather than as an identifiable single participantâ€™s order.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 AGGREGATED LIMIT ORDER BOOK (5Ã—5)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              ASKS             â”‚               BIDS               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Price (USD)  â”‚   Qty (sh)    â”‚  Price (USD)  â”‚    Qty (sh)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    100.05     â”‚     1,200     â”‚     99.95     â”‚      1,050       â”‚
â”‚    100.04     â”‚     1,450     â”‚     99.96     â”‚      1,180       â”‚
â”‚    100.03     â”‚     1,100     â”‚     99.97     â”‚  â–ˆâ–ˆâ–ˆ 12,500 â–ˆâ–ˆâ–ˆ  â”‚ â† large resting bid depth
â”‚    100.02     â”‚       980     â”‚     99.98     â”‚      1,210       â”‚
â”‚    100.01     â”‚     1,320     â”‚     99.99     â”‚      1,090       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Best Ask a_t = 100.01   Best Bid b_t = 99.99     â”‚
â”‚                 Spread s_t = 0.02       Mid m_t = 100.00         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Institutional-scale execution commonly manifests through order types and routing logic designed to manage information leakage and market impact. Iceberg (reserve) orders expose only a small displayed quantity while maintaining hidden reserve size, block trades may be arranged off-book or via specialized facilities, and algorithmic execution such as TWAP or VWAP slices a meta-order into child orders to reduce footprint. Dark pools and internalization can further obscure true supply and demand by shifting execution away from the lit book. Consequently, the â€œvery large pending orderâ€ targeted by this framework should be understood as â€œlarge *visible* resting depthâ€ when using Level 2, or â€œlarge *effective* passive liquidityâ€ when inferred from prints and price response.

The information content of large visible depth is microstructurally intuitive but empirically nuanced. A large bid resting at price \(p\) tends to increase the probability that downward price moves stall at or above \(p\), because aggressive sell flow must consume a greater-than-normal quantity to pass through that level; this is the mechanical support hypothesis. Symmetrically, a large ask can create mechanical resistance. However, because displayed liquidity can be transient and strategic, the presence of large depth is not by itself a guarantee of genuine intent to trade, and thus detection must incorporate persistence and fill-related diagnostics. The LOB literature emphasizes that order submission and cancellation processes encode information, but that converting LOB structure into robust out-of-sample trading profits is difficult and sensitive to transaction costs and regime changes, which motivates a conservative, early-stop design and strict compliance controls in the present approach. ([aimsciences.org](https://www.aimsciences.org/article/doi/10.3934/dcdsb.2019206?utm_source=openai))

---

## 2. Detection Methods â€” Without Expensive Private Data Feeds

The key deliverable is a suite of low-cost detection methods that can be implemented using commonly accessible data sources. Across methods, the objective is to estimate a latent binary variable \(L_t(p, \text{side})\) indicating whether a given price level \(p\) on a given side of the book is currently supported by unusually large passive liquidity. Because the strategyâ€™s P\&L is dominated by whether the â€œsupport/resistanceâ€ vanishes unexpectedly, the detection problem is not merely â€œdoes large depth exist,â€ but â€œis the observed depth *credible* and *stable* over a short horizon,â€ as credibility directly governs early-stop timing and slippage.

### 2A. Broker-Provided Level 2 Data (Low-Cost)

Many brokers provide Level 2 market depth through APIs as part of market data subscriptions. Interactive Brokersâ€™ TWS API, for example, exposes market depth through the `reqMktDepth` / `reqMarketDepth` request, delivering incremental updates via callbacks such as `updateMktDepth` and `updateMktDepthL2`, with the number of requested rows parameterizing how many depth entries the client attempts to receive. ([interactivebrokers.co.jp](https://www.interactivebrokers.co.jp/en/software/csharp/topics/market%20depth.htm?utm_source=openai)) The principal advantage of broker depth is that it provides an explicit, direct view of visible depth at multiple price levels, enabling a straightforward anomaly scan. The principal limitations are aggregation artifacts (market-by-price rather than order-by-order), possible depth truncation, venue coverage constraints, and latency that is typically â€œinternet-gradeâ€ rather than co-located.

A robust scanning approach treats depth as a stochastic process with strong intraday seasonality and price-distance structure. Let \(Q_{k,t}^{(s)}\) denote the displayed quantity at time \(t\) at level \(k\) ticks away from the best quote on side \(s \in \{\text{bid}, \text{ask}\}\). Because depth tends to increase with distance from the touch, the baseline should be conditioned on \(k\) (and often on time-of-day). A simple statistical detector defines a standardized anomaly score
$$
Z_{k,t}^{(s)} = \frac{Q_{k,t}^{(s)} - \mu_{k,\tau}^{(s)}}{\sigma_{k,\tau}^{(s)}},
$$
where \(\mu_{k,\tau}^{(s)}\) and \(\sigma_{k,\tau}^{(s)}\) are estimated from a rolling historical window and indexed by \(\tau\), a time-of-day bucket, to control for predictable liquidity rhythms. A level is flagged as â€œinstitutional-scale visible depthâ€ when \(Z_{k,t}^{(s)}\) exceeds a high threshold such as \(3\), possibly complemented by a robust ratio test \(Q_{k,t}^{(s)} / \text{median}(Q_{k,\cdot}^{(s)})\) to reduce sensitivity to heavy tails.

False positives arise when aggregated depth reflects multiple smaller participants, when the brokerâ€™s depth view is incomplete relative to the full consolidated book, or when liquidity is strategically posted and cancelled without intention to trade. Implementation difficulty is moderate, as it requires reliable depth stream handling, correct mapping of updates into a local depth ladder, and careful baseline calibration. Latency and reliability are typically sufficient for strategies operating on seconds-to-minutes horizons, but should be stress-tested under bursty update conditions, particularly around news and open/close auctions.

âš ï¸ **COMPLIANCE NOTE:** Even when depth is â€œavailableâ€ via a broker, its use is typically governed by exchange licensing terms and broker agreements that can restrict redistribution, derived-data publication, and sometimes automated trading usage. Compliance review should explicitly confirm permitted uses, storage durations, and audit obligations before deployment.

### 2B. Inference from Level 1 / Trade-and-Quote Data

When explicit depth is unavailable or unreliable, a large passive order can be inferred from the joint behavior of price changes, quote dynamics, and executed trades. The core microstructural signature is absorption: aggressive flow executes without producing proportional price movement, implying that passive liquidity is replenishing or sufficiently large at a level.

A practical formalization is an absorption ratio computed at or near a candidate level \(p\). Let \(V(p,\Delta t)\) be the total executed volume whose prints occur at price \(p\) (or within one tick of \(p\)) over a window \(\Delta t\). Let \(\Delta m(\Delta t)\) be the mid-price change over the same window. Then define
$$
\text{AR}(p,\Delta t) = \frac{V(p,\Delta t)}{|\Delta m(\Delta t)| + \epsilon},
$$
where \(\epsilon > 0\) is a small stabilizer. A large \(\text{AR}\) indicates â€œlarge traded volume for small price movement,â€ consistent with passive absorption. Because \(\text{AR}\) can blow up when \(|\Delta m|\) is near zero, operational implementations typically cap the ratio and supplement it with a â€œfailed breakâ€ count, defined as the number of times the best quote approaches \(p\) and then retreats without a print beyond \(p\). Quote-side diagnostics, such as spread compression as the market approaches \(p\) and repeated replenishment at the touch, further increase confidence.

This inference method is low cost because Level 1 trade-and-quote data is widely available, including via many retail/professional platforms, public crypto feeds, and vendor APIs. Latency depends on the feed; reliability depends on accurate trade classification and microstructure noise. False positives are common in range-bound markets, during volatility halts, and when hidden liquidity or internalization obscures the relationship between trades and lit quotes. Implementation difficulty is moderate to high because inference requires careful feature engineering and regime filters.

### 2C. Tape Reading / Time & Sales Analysis

Time-and-sales (T\&S) tape analysis focuses on prints rather than displayed depth. Institutional-scale passive participation often leaves tape signatures even when the book is partially hidden. Large prints reported at a single price, repeated execution at a stable price despite directional order flow, and â€œtrade-through that does not extendâ€ patterns can suggest a large resting order absorbing aggressive flow.

Iceberg orders, in particular, may be inferred when executions occur in repeated, nearly identical clip sizes at the same price, with short inter-arrival times and without the expected depletion of displayed size. In a simplified model, let trades at price \(p\) have sizes \(\{v_i\}\). An iceberg signature is suggested when the empirical distribution of \(v_i\) exhibits a strong mode at a clip size \(v^\*\), and when sequences of prints at \(p\) of size \(v^\*\) occur without mid-price movement. This method is attractive because it can be implemented using public prints, including many crypto and equities datasets, and does not require depth. However, it is vulnerable to false positives from order-splitting by aggressors, venue reporting conventions, and the presence of internalized trades that print without reflecting lit-book dynamics.

### 2D. Volume Profile and Order Flow Analysis

Volume-at-price (VAP) and order flow metrics provide a complementary â€œwhere did trading concentrateâ€ view. High traded volume clustered at a narrow price band can indicate that a large participant was active there, potentially using passive orders to accumulate or distribute. VAP is intrinsically retrospective at short horizons, but it can be made quasi-real-time by maintaining an intraday histogram of traded volume by price.

VWAP anchoring can provide an additional context for inference. Define intraday VWAP up to time \(t\) as
$$
\text{VWAP}_t = \frac{\sum_{i \le t} P_i V_i}{\sum_{i \le t} V_i},
$$
where \((P_i,V_i)\) are trade price and size. Persistent deviations \(D_t = P_t - \text{VWAP}_t\) that do not mean-revert despite directional flow can be consistent with a large passive participant defending a level. Similarly, cumulative delta approximations can detect divergence between net aggressor flow and price, a classic absorption signature.

A particularly well-studied microstructure measure is order flow imbalance (OFI). In the Contâ€“Kukanovâ€“Stoikov framework, short-horizon price changes are strongly related to imbalances in order book events at the best bid and ask, and the authors document a near-linear relationship between OFI and price changes with a slope inversely related to depth. ([academic.oup.com](https://academic.oup.com/jfec/article-abstract/12/1/47/816163?utm_source=openai)) Operationally, even when one cannot observe all order events, one can compute an OFI-like statistic from best-quote size changes and trades, and use it as an explanatory variable for whether a price level is being defended by passive liquidity.

The cost of these methods is low when based on publicly available trades and quotes; latency is typically acceptable for second-to-minute horizons; reliability is moderate; false positives arise in high-volatility regimes and when trade classification is noisy; and implementation difficulty ranges from moderate (VAP histograms) to high (robust OFI estimation with microstructure noise).

### 2E. Public Exchange APIs and Open-Source Tooling (Crypto as a Prototyping Laboratory)

Crypto venues often provide free, high-frequency order book updates via WebSocket market data streams. Binanceâ€™s spot API documentation, for example, publicly specifies market data WebSocket streams including diff-depth streams such as \(`<symbol>@depth@100ms`\) and partial-depth streams, and it maintains these specifications in its official documentation repository. ([github.com](https://github.com/binance/binance-spot-api-docs?utm_source=openai)) Coinbase similarly provides an Advanced Trade WebSocket feed with market data channels including a Level 2 channel intended to support order book maintenance, and documents production endpoints for market data and user order data. ([docs.cdp.coinbase.com](https://docs.cdp.coinbase.com/coinbase-app/advanced-trade-apis/websocket/websocket-overview?utm_source=openai))

For research and prototyping, such feeds offer an unusually accessible environment for building and validating detection algorithms end-to-end, including local order book maintenance, event-driven feature computation, and early-stop execution simulations. Libraries such as CCXT provide a unified interface to multiple exchanges (notwithstanding differences in semantics and rate limits), which can reduce engineering overhead in early phases. ([github.com](https://github.com/ccxt/ccxt/wiki/manual?utm_source=openai))

This channelâ€™s primary caveat is that crypto market microstructure differs materially from U.S. equities and regulated futures. The absence of a consolidated tape, heterogeneous matching rules, differing tick size regimes, and variable surveillance intensity can change both the prevalence of spoof-like behaviors and the stability of displayed depth. Therefore, crypto should be treated as an engineering testbed rather than a direct performance proxy for equities. Latency for WebSocket updates is typically in the sub-second regime and is usually adequate for minute-scale strategies, but burst loss, sequence gaps, and reconnection logic are non-trivial engineering problems that directly impact early-stop correctness.

âš ï¸ **COMPLIANCE NOTE:** Although crypto market data is often â€œpublic,â€ automated ingestion and storage may still be governed by exchange terms of service. A compliance review should verify permissible use, especially if data is stored, redistributed, or used to generate client-facing outputs.

### 2F. Indirect / Statistical Detection (No Real-Time Feed Required)

When real-time feeds are constrained, one can still perform end-of-day or delayed anomaly detection to identify â€œinstitutional levelsâ€ for subsequent monitoring. Unusually high volume concentration at specific prices in daily volume profiles can indicate the session-level presence of large passive participation. Options markets can sometimes provide complementary signals because large equity orders are often hedged, and unusual changes in options volume or open interest may co-occur with large stock flows. Short interest changes and borrowing cost shifts can also correlate with institutional positioning, although these signals are typically low frequency and confounded by reporting lags.

This class is inexpensive and easy to implement, but it is not a standalone trigger for an intraday early-stop strategy. Rather, it is best used to generate watchlists of candidate price levels and instruments, improving the efficiency of real-time monitoring by focusing attention on â€œlikely defendedâ€ zones.

---

### Comprehensive Comparison Matrix

The following matrix summarizes the practical trade-offs across detection methods. Cost is expressed qualitatively because actual fees vary by venue, instrument, and subscription.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Detection Method              â”‚ Data Source            â”‚ Cost      â”‚ Latency     â”‚ Reliability    â”‚ False Positive Rate   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ A. Broker Level 2 anomaly     â”‚ Broker API depth (MBP)â”‚ Lowâ€“Med   â”‚ 10â€“500 ms   â”‚ Med            â”‚ Med (aggregation)     â”‚
â”‚    scan (Z-score)             â”‚                       â”‚           â”‚             â”‚                â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ B. Level 1 absorption inferenceâ”‚ Trades + best quotes  â”‚ Low       â”‚ 10â€“500 ms   â”‚ Lowâ€“Med        â”‚ High (regime noise)   â”‚
â”‚    (AR, failed-break counts)  â”‚                       â”‚           â”‚             â”‚                â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ C. Tape reading / T&S         â”‚ Prints (time & sales)  â”‚ Low       â”‚ 10â€“500 ms   â”‚ Med            â”‚ Medâ€“High (splitting)  â”‚
â”‚    (iceberg signatures)       â”‚                       â”‚           â”‚             â”‚                â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ D. VAP + order flow metrics   â”‚ Trades + quotes        â”‚ Low       â”‚ 10â€“1000 ms  â”‚ Med            â”‚ Med (classification)  â”‚
â”‚    (VWAP, delta, OFI-like)    â”‚                       â”‚           â”‚             â”‚                â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ E. Public crypto L2 WebSocket â”‚ Exchange WebSockets    â”‚ Low       â”‚ 50â€“500 ms   â”‚ Medâ€“High       â”‚ Med (feed gaps)       â”‚
â”‚    (full/partial depth)       â”‚                       â”‚           â”‚             â”‚                â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ F. Indirect statistical        â”‚ EOD profiles, options, â”‚ Very Low â”‚ Minutesâ€“Daysâ”‚ Low (intraday) â”‚ Med (macro confounds) â”‚
â”‚    (watchlist generation)      â”‚ borrow data            â”‚          â”‚             â”‚                â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Same-Side Trading Strategy â€” Full Technical Specification

The strategyâ€™s defining constraint is that it trades in the same direction as the detected passive liquidity. If a large bid is detected, the strategyâ€™s position is long-biased; if a large ask is detected, the strategyâ€™s position is short-biased (subject to instrument constraints and borrow availability). The economic rationale is that the large resting order increases the conditional probability of short-horizon mean reversion away from the defended level or reduces the conditional probability of an adverse breach, creating a favorable skew for same-side entries. The risk is that the observed liquidity is cancelled, at which point the strategyâ€™s thesis collapses, motivating an immediate early-stop exit.

### Pipeline Schematic: Detection â†’ Evaluation â†’ Entry â†’ Monitoring â†’ Exit

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DETECTION    â”‚â†’â†’â”‚  EVALUATION     â”‚â†’â†’â”‚     ENTRY      â”‚â†’â†’â”‚    MONITORING     â”‚â†’â†’â”‚      EXIT      â”‚
â”‚ (multi-signal) â”‚   â”‚ (credibility)  â”‚   â”‚ (same-side)    â”‚   â”‚ (early-stop)     â”‚   â”‚ (flatten)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                    â”‚                    â”‚                    â”‚
        â”‚                   â”‚                    â”‚                    â”‚                    â”‚
        â–¼                   â–¼                    â–¼                    â–¼                    â–¼
  Depth Z-score        Persistence time     Execution tactic     Order pulled?       Market/limit
  Absorption ratio     Partial-fill signal  Size = f(risk)       Depth decay?        exit + logging
  Iceberg signature    Size stability       Join vs lift/hit     Adverse move?       Compliance flags
  OFI-like metrics     Cross-check signals  Slippage estimate    Time stop?          Post-trade review
```

### State Machine Definition

The strategy is most cleanly specified as a finite-state machine with explicit transitions and hard stops, ensuring that the early-stop rule is not subject to discretionary override. Let the states be \(\{\text{SCANNING}, \text{DETECTED}, \text{ENTERED}, \text{MONITORING}, \text{EXITED}\}\). The transition logic is governed by a credibility score \(C_t \in [0,1]\), an anomaly score \(Z_t\), and an order-presence indicator \(\mathbb{1}\{\text{depth at level}\}\).

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SAME-SIDE STRATEGY STATE MACHINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Z_t â‰¥ Z* & cross-signal ok   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  SCANNING    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚   DETECTED   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                                               â”‚                      â”‚
â”‚         â”‚ Z_t < Z*                                       â”‚ C_t â‰¥ C*             â”‚
â”‚         â”‚ (no signal)                                    â”‚ (credible)           â”‚
â”‚         â”‚                                               â–¼                      â”‚
â”‚         â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    ENTRY     â”‚              â”‚
â”‚                                                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                         â”‚ order ack + fill      â”‚
â”‚                                                         â–¼                      â”‚
â”‚                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                                                  â”‚ MONITORING   â”‚              â”‚
â”‚                                                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                         â”‚                      â”‚
â”‚                         early-stop: depth disappears     â”‚ profit/time/stop     â”‚
â”‚                         or decays below threshold        â”‚ reached              â”‚
â”‚                                                         â–¼                      â”‚
â”‚                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                                                  â”‚    EXIT      â”‚              â”‚
â”‚                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Credibility and â€œNot-a-Spoofâ€ Heuristics (Operational, Not Determinative)

Because the strategyâ€™s premise relies on third-party liquidity, a central engineering objective is to reduce exposure to large displayed orders that are likely to be cancelled rapidly or that oscillate in size, which are hallmarks often associated with deceptive depth. The CFTC has emphasized that spoofing under the CEA focuses on intent to cancel before execution, and has further clarified that legitimate, good-faith order modification and cancellation can occur without constituting spoofing, particularly when part of a bona fide attempt to trade. ([cftc.gov](https://www.cftc.gov/LawRegulation/FederalRegister/ProposedRules/2011-6398.html?utm_source=openai)) While the strategy does not place the large order and therefore does not control intent, it can reduce risk by requiring that the detected depth exhibit persistence and interaction with executed flow.

A practical credibility score can be constructed as a weighted combination of features such as persistence time \(T_{\text{persist}}\), partial fill evidence \(F_{\text{partial}}\), size stability \(S_{\text{stable}}\), and multi-signal agreement \(M_{\text{agree}}\). One convenient parameterization is
$$
C_t = \sigma\!\left(w_0 + w_1 \log(1+T_{\text{persist}}) + w_2 F_{\text{partial}} + w_3 S_{\text{stable}} + w_4 M_{\text{agree}}\right),
$$
where \(\sigma(\cdot)\) is a logistic function mapping to \([0,1]\). Here, \(F_{\text{partial}}\) can be proxied by repeated prints at the level with no depletion of the defended price, and \(S_{\text{stable}}\) can be defined as an inverse coefficient of variation for displayed size at the level over a short window.

âš ï¸ **COMPLIANCE NOTE:** Credibility heuristics are risk controls and should not be framed as â€œproofâ€ that another participant is spoofing. Surveillance conclusions about manipulation require broader context and are a compliance function, not a trading signal justification.

### Entry Logic and Position Sizing

Same-side entry can be implemented in a conservative manner by entering only when price is sufficiently close to the defended level, thereby making the early-stop meaningful and keeping the thesis localized. For a large bid at price \(p_b\), an entry condition might be \(m_t - p_b \le \delta\) for a small \(\delta\) measured in ticks, with a long position opened either via a passive limit buy at or slightly above \(p_b\) (seeking favorable fill) or via a small aggressive order if fill probability is low and the signal is time-sensitive. For a large ask at \(p_a\), analogous logic applies for short entry.

Position size should be explicitly risk-capped. Let the per-trade risk budget be \(R\) in dollars, and let the effective stop distance be \(d\), defined as the expected adverse move upon early-stop execution, including spread and slippage. A basic sizing rule is
$$
q = \left\lfloor \frac{R}{d} \right\rfloor,
$$
with additional caps based on participation rate (to avoid being the market), instrument volatility, and liquidity. The key is that the early-stop can entail gap-like slippage if the large order vanishes during fast conditions; thus \(d\) should be stress-tested and not set to optimistic values.

### Monitoring and Early-Stop Exit

Monitoring must treat disappearance or rapid decay of the detected depth as a first-class risk event. On Level 2 feeds, â€œpulledâ€ can be operationalized as (i) the levelâ€™s quantity dropping below a threshold \(Q_{\min}\), (ii) the level moving away from the best quote without trades that would explain depletion, or (iii) the entire depth ladder reshaping such that the anomalous level no longer exists within a price tolerance band. On Level 1 inference, early-stop can be triggered by a break of the defended level accompanied by spread widening and a regime shift in trade imbalance.

The exit mechanism should be execution-prioritized. If the early-stop triggers, the strategy should flatten with an immediately marketable order (or an aggressive limit with a tight cap) to reduce exposure to adverse continuation once support/resistance is removed. This is a deliberate trade-off: early-stop should prefer certainty of exit over minimizing fees or seeking price improvement.

### Pseudocode (Core Algorithm Skeleton)

```text
INPUTS:
  stream_depth (optional), stream_quotes, stream_trades
  params: Z*, C*, Q_min, delta_ticks, R, time_limit, profit_target, stop_loss

STATE â† SCANNING
position â† 0
signal â† None

LOOP on each market update event at time t:
  features â† compute_features(depth, quotes, trades, window=tâˆ’W..t)
  Z_t â† depth_anomaly_score(features)
  AR_t â† absorption_ratio(features)
  C_t â† credibility_score(features)

  IF STATE = SCANNING:
     IF Z_t â‰¥ Z* AND cross_signal_ok(AR_t, features):
        signal â† identify_level_and_side(features)
        STATE â† DETECTED

  ELSE IF STATE = DETECTED:
     IF signal_invalid(signal, features):
        STATE â† SCANNING
     ELSE IF C_t â‰¥ C* AND price_near_level(signal, quotes, delta_ticks):
        order_id â† send_same_side_entry(signal, size=compute_size(R, features))
        IF order_acknowledged(order_id):
           STATE â† ENTERED
           entry_time â† t

  ELSE IF STATE = ENTERED:
     position â† update_position_from_fills()
     STATE â† MONITORING

  ELSE IF STATE = MONITORING:
     early_stop â† (depth_at_signal_level(features) < Q_min) OR signal_disappeared(signal, features)
     risk_stop  â† adverse_move_exceeds(stop_loss, quotes, entry_price)
     time_stop  â† (t âˆ’ entry_time) > time_limit
     take_profit â† pnl_exceeds(profit_target, quotes, entry_price)

     IF early_stop OR risk_stop OR time_stop OR take_profit:
        send_flatten_order(position)
        log_event(signal, features, reason=exit_reason)
        STATE â† EXITED

  ELSE IF STATE = EXITED:
     position â† 0
     signal â† None
     STATE â† SCANNING
```

### Execution, Slippage, and Infrastructure Requirements

Slippage should be modeled explicitly as a function of spread, urgency, and volatility. A conservative expected execution cost per round trip can be approximated as
$$
\text{EC} \approx \alpha \, s_t + \beta \, \sigma_t \sqrt{\Delta t} + \gamma \, \text{impact}(q),
$$
where \(s_t\) is the spread, \(\sigma_t\) is short-horizon volatility, and \(\alpha,\beta,\gamma\) are calibration parameters. Because early-stop exits are executed under adverse information (the support vanished), realized slippage can be right-tailed; thus, the operational requirement is not â€œfastest possibleâ€ execution, but â€œconsistent and reliableâ€ connectivity plus a broker/venue path that does not introduce unpredictable delays.

---

## 4. Legal and Regulatory Framework â€” Critical Analysis

### 4A. Definition of Spoofing and Layering

Under U.S. commodities and futures regulation, the anti-spoofing provision is codified in the Commodity Exchange Act at 7 U.S.C. Â§ 6c(a)(5)(C), which characterizes spoofing as â€œbidding or offering with the intent to cancel the bid or offer before execution.â€ ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/7/6c?utm_source=openai)) The CFTCâ€™s interpretive guidance has emphasized that intent (scienter) is essential and that legitimate, good-faith cancellations or modifications, including cancellations of partially filled orders, are not necessarily spoofing, while also noting that partial fills do not automatically immunize conduct if the original intent was to cancel prior to execution. ([cftc.gov](https://www.cftc.gov/LawRegulation/FederalRegister/ProposedRules/2011-6398.html?utm_source=openai)) In the EU, the Market Abuse Regulation (Regulation (EU) No 596/2014) defines market manipulation to include the placing of orders that give or are likely to give false or misleading signals as to supply, demand, or price, and it provides non-exhaustive indicators, including patterns where orders alter the displayed order book and are removed before execution. ([eur-lex.europa.eu](https://eur-lex.europa.eu/eli/reg/2014/596/2016-07-03/eng?utm_source=openai))

The case law and enforcement record underscore that spoofing and layering are treated as serious forms of market manipulation. In *United States v. Coscia*, the Seventh Circuit affirmed a criminal spoofing conviction and discussed the statutory definition of spoofing under 7 U.S.C. Â§ 6c(a)(5)(C). ([law.justia.com](https://law.justia.com/cases/federal/appellate-courts/ca7/16-3017/16-3017-2017-08-07.html?utm_source=openai)) In the Navinder Sarao matter, the U.S. Department of Justice described Saraoâ€™s conduct as placing thousands of orders he did not intend to trade in order to create the appearance of false supply and demand, including on May 6, 2010 (the Flash Crash). ([justice.gov](https://www.justice.gov/archives/opa/pr/futures-trader-pleads-guilty-illegally-manipulating-futures-market-connection-2010-flash?utm_source=openai)) The CFTCâ€™s 2020 order against JPMorgan imposed record monetary relief for spoofing and manipulative conduct in precious metals and U.S. Treasury futures markets. ([cftc.gov](https://www.cftc.gov/PressRoom/PressReleases/8260-20?utm_source=openai))

âš ï¸ **COMPLIANCE NOTE:** This report discusses spoofing mechanics only to delineate prohibited conduct from legitimate same-side trading based on publicly observed data. It does not endorse, instruct, or facilitate manipulative practices.

### 4B. Why Opposite-Side Trading Is Legally Dangerous

The â€œopposite-sideâ€ archetype becomes legally perilous when it is coupled with any deceptive order placement intended to move price or to mislead others about supply and demand. Under U.S. commodities law, placing orders with intent to cancel before execution is explicitly captured by the anti-spoofing rule. ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/7/6c?utm_source=openai)) Under EU MAR, orders that give false or misleading signals about supply/demand or that secure an artificial price level can constitute market manipulation. ([fma.gv.at](https://www.fma.gv.at/en/capital-markets/market-abuse/market-manipulation/?utm_source=openai)) Even in U.S. securities markets where the CEA spoofing provision is not directly the governing statute, manipulative and deceptive trading schemes can implicate Exchange Act provisions such as Rule 10b-5 and Section 9(a)(2), which prohibit fraudulent schemes and series of transactions creating artificial price effects to induce others. ([law.cornell.edu](https://www.law.cornell.edu/cfr/text/17/240.10b-5?utm_source=openai))

The practical regulatory risk is that an opposite-side strategy can drift from â€œreacting to liquidityâ€ into â€œinducing price movesâ€ by placing or cancelling orders to create an impression of depth, particularly when the strategy systematically benefits from other participantsâ€™ reactions. In enforcement narratives, such as those described in the Sarao and JPM matters, the pattern of placing large visible orders to move the market and then executing genuine orders on the other side is a core alleged mechanism. ([justice.gov](https://www.justice.gov/archives/opa/pr/futures-trader-pleads-guilty-illegally-manipulating-futures-market-connection-2010-flash?utm_source=openai))

### 4C. Spoofing vs. Legitimate Same-Side Trading (Side-by-Side Mechanics)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LEFT: SPOOFING PATTERN          â”‚        RIGHT: LEGITIMATE SAME-SIDE TRADING   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1) Trader posts large visible order          â”‚ 1) Trader observes third-party large depth   â”‚
â”‚    (no genuine intent to fill).              â”‚    via permitted public/broker data.         â”‚
â”‚                                              â”‚                                              â”‚
â”‚ 2) Market participants infer support/        â”‚ 2) Trader forms hypothesis of transient      â”‚
â”‚    resistance and adjust behavior.           â”‚    support/resistance.                        â”‚
â”‚                                              â”‚                                              â”‚
â”‚ 3) Trader executes genuine orders on the     â”‚ 3) Trader enters on the same side as the     â”‚
â”‚    opposite side to profit from induced move.â”‚    observed support/resistance.              â”‚
â”‚                                              â”‚                                              â”‚
â”‚ 4) Trader cancels the deceptive large order  â”‚ 4) Trader does not place deceptive orders;   â”‚
â”‚    before execution (or as soon as risk rises).â”‚   orders are bona fide and execution-intended.â”‚
â”‚                                              â”‚                                              â”‚
â”‚ 5) Market depth â€œevaporates,â€ price snaps,   â”‚ 5) Trader monitors for disappearance of the  â”‚
â”‚    counterparties harmed.                    â”‚    third-party depth and exits immediately   â”‚
â”‚                                              â”‚    when the supporting depth is pulled.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The compliance-relevant distinction is agency and intent. Spoofing involves the traderâ€™s own order placement and the intent to cancel in order to mislead, whereas legitimate same-side trading is a reactive strategy based on observation of publicly displayed market conditions, without the trader placing deceptive orders. The legal exposure of the strategy described in this report primarily arises not from â€œtrading the same side,â€ but from any deviation into non-bona fide order placement, misuse of nonpublic information, or patterns that create artificial price effects.

### 4D. Why Same-Side Trading Is Generally Legal (with Material Caveats)

Same-side trading based on publicly observable information is generally aligned with the notion that market participants may incorporate public order book information into their trading decisions. The strategy does not require placing misleading orders; rather, it exploits a conditional price-response asymmetry associated with visible liquidity. It is therefore conceptually distinct from spoofing as defined under the CEA and from MAR forms that focus on false or misleading signals created by manipulative orders. ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/7/6c?utm_source=openai))

However, legality is contingent on facts. If the trader has material nonpublic information about who placed the large order or about an impending institutional execution (for example, as a broker or in any fiduciary capacity), then trading alongside it may become illegal front-running, misappropriation, or a breach of duty. Likewise, if the trader engages in a pattern of transactions intended to move price to induce others, Section 9(a)(2) and Rule 10b-5 considerations can become relevant in securities contexts. ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/15/78i?utm_source=openai))

âš ï¸ **COMPLIANCE NOTE:** The only defensible version of this strategy is one that is strictly limited to decisions derived from permitted, publicly observable market data and that uses only bona fide orders placed with genuine intent to execute. Any use of confidential client order information, internal broker flow information, or any attempt to â€œshapeâ€ the order book would materially change the compliance posture.

### 4E. Compliance Best Practices (Operational Controls)

A compliance-forward implementation should embed contemporaneous documentation and surveillance-friendly telemetry. The trading rationale should be logged at entry and exit with the observed anomaly metrics, persistence evidence, and early-stop triggers, so that post-trade review can distinguish reactive liquidity-based trading from manipulative intent. Order placement behavior should be constrained to avoid abnormal order-to-trade ratios and to ensure that cancellations are explainable as legitimate execution management rather than as signal generation. Pre-trade controls should include instrument eligibility rules (borrow availability, short-sale constraints, halts), volatility-based kill switches, maximum participation caps, and data-integrity checks that prevent trading when the feed is degraded. Audit trails should preserve raw feed snapshots or event hashes sufficient to reconstruct why the strategy acted, consistent with data licensing and retention policies.

### Legal Decision Tree: â€œIs My Order Book Strategy Legal?â€

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LEGALITY SCREEN (HIGH-LEVEL)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Q1: Is the information source PUBLIC or properly LICENSED?                 â”‚
â”‚     â”œâ”€ No â†’ STOP. Obtain proper licenses / approvals.                      â”‚
â”‚     â””â”€ Yes â†’ Q2                                                             â”‚
â”‚                                                                           â”‚
â”‚ Q2: Do you possess MNPI about the large order (identity, client intent)?   â”‚
â”‚     â”œâ”€ Yes â†’ STOP. Potential front-running / misuse of MNPI.               â”‚
â”‚     â””â”€ No â†’ Q3                                                             â”‚
â”‚                                                                           â”‚
â”‚ Q3: Are your own orders BONAFIDE with genuine intent to execute?           â”‚
â”‚     â”œâ”€ No â†’ STOP. High spoofing / manipulation risk.                       â”‚
â”‚     â””â”€ Yes â†’ Q4                                                             â”‚
â”‚                                                                           â”‚
â”‚ Q4: Does your strategy create FALSE/MISLEADING signals (e.g., fake depth)? â”‚
â”‚     â”œâ”€ Yes â†’ STOP. MAR 12 / CEA 4c(a)(5)(C) / 10b-5 / 9(a)(2) concerns.     â”‚
â”‚     â””â”€ No â†’ Q5                                                             â”‚
â”‚                                                                           â”‚
â”‚ Q5: Are exits governed by RISK controls (early-stop), not inducement?      â”‚
â”‚     â”œâ”€ No â†’ REVIEW. Risk of â€œprice-movingâ€ characterization.               â”‚
â”‚     â””â”€ Yes â†’ Proceed subject to jurisdictional review and monitoring.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Risk Management Framework

The strategyâ€™s dominant risk is sudden cancellation of the supporting depth, because the tradeâ€™s thesis depends on continued presence of the large passive liquidity. Early-stop is therefore not an optimization, but a safety constraint. The operational goal is to minimize â€œunsupported exposure time,â€ defined as the time between the depth disappearing and the position being flattened. This exposure time is the product of detection latency, decision latency, order routing latency, and execution time, and it should be measured and bounded.

A second risk is iceberg and hidden liquidity ambiguity. A visible â€œlargeâ€ level may be only the displayed clip of a larger reserve, in which case disappearance of the displayed portion might not imply removal of true support. Conversely, visible depth may be a decoy, and the true liquidity may not exist. These ambiguities imply that early-stop rules must be based on observable, repeatable criteria, even if they sometimes exit prematurely in the presence of hidden support, because the compliance and risk objective is to avoid betting on unverifiable hidden liquidity.

A third risk is adversarial behavior. A sophisticated algorithm may strategically post and pull liquidity to induce predictable responses from liquidity-following strategies, including same-side followers, thereby creating a trap where early-stop exits become predictable sources of liquidity for the adversary. This risk is heightened in less regulated venues and in instruments with thin books.

A fourth risk is misclassification of spoof-like liquidity as genuine. Recent machine learning literature has explored spoofing detection using deep learning architectures on high-frequency order book data, including a GRU-based detection model proposed by Tuccella, Nadler, and Åerban (arXiv:2110.03687, 2021) and probabilistic neural approaches to â€œspoofabilityâ€ proposed by Fabre and Challet (arXiv:2504.15908, 2025). ([arxiv.org](https://arxiv.org/abs/2110.03687?utm_source=openai)) While such models are generally trained on Level 3-style event data and may not be directly portable to low-cost Level 2 feeds, they motivate practical heuristics: spoof-like depth is often short-lived, rapidly modified, weakly correlated with prints (few or no partial fills), and appears at strategically chosen distances from the touch.

A fifth risk is technology failure, including connectivity loss during an open position, corrupted depth ladders due to missed updates, and broker API outages. Because the early-stop depends on timely detection of depth disappearance, a feed outage can convert a controlled risk into an uncontrolled exposure. A conservative implementation should treat feed degradation as a reason to reduce exposure or disallow new entries.

A sixth risk is latency mismatch. Even if the detection operates on minutes-to-hours horizons, early-stop is intrinsically event-driven and can become a race condition during fast markets. A WebSocket feed with hundreds of milliseconds of jitter, combined with a broker routing path of similar magnitude, can yield multi-second unsupported exposure under stress, which materially changes the risk distribution.

### Lifecycle of a Large Institutional Order (Observed via Public Data)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LARGE ORDER LIFE-CYCLE (OBSERVABLE PHENOTYPES)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Placement â†’ Persistence â†’ Partial fills / replenishment â†’ Modify â†’ Cancel    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Appears  â”‚â†’â†’â”‚  Stays visible  â”‚â†’â†’â”‚ Prints cluster at p â”‚â†’â†’â”‚  Resizes  â”‚  â”‚
â”‚  â”‚ Q(p) jumpsâ”‚   â”‚ for T seconds  â”‚   â”‚ price holds        â”‚   â”‚ or moves â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                         â”‚                     â”‚                 â”‚
â”‚          â”‚                         â”‚                     â”‚                 â”‚
â”‚          â–¼                         â–¼                     â–¼                 â–¼
â”‚   Genuine support            Potential spoof       Iceberg-like refill    Cancellation:
â”‚   often shows fills          if cancels quickly    if repeated clips      early-stop trigger
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Risk-Management State Machine (Risk Controls as Transitions)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RISK CONTROL STATE MACHINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  NORMAL â†’ DETECTED â†’ ENTERED â†’ MONITORING â†’ EXIT                            â”‚
â”‚                                                                            â”‚
â”‚  NORMAL â”€â”€signal quality okâ”€â”€â–º DETECTED â”€â”€credibility okâ”€â”€â–º ENTERED         â”‚
â”‚     â”‚                               â”‚                         â”‚            â”‚
â”‚     â”‚ feed degraded / volatility     â”‚ credibility fails        â”‚            â”‚
â”‚     â”‚ spike                          â–¼                         â–¼            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º EXIT (NO-TRADE)  NORMAL              MONITORING         â”‚
â”‚                                                    â”‚ early-stop / stop-loss â”‚
â”‚                                                    â–¼                        â”‚
â”‚                                                   EXIT                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ASCII Risk Register (Likelihood Ã— Severity Ã— Mitigation)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Risk                                          â”‚ Likelihood â”‚ Severity  â”‚ Mitigation                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Large depth cancels abruptly (support vanishes)â”‚ High       â”‚ High      â”‚ Hard early-stop; aggressive flatten; latency  â”‚
â”‚                                               â”‚            â”‚           â”‚ monitoring; max hold-time caps                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€œLargeâ€ level is aggregated noise (not one    â”‚ Medium     â”‚ Medium    â”‚ Z-score by distance/time-of-day; multi-signal â”‚
â”‚ institution)                                  â”‚            â”‚           â”‚ confirmation; require persistence             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Detected depth is spoof-like (not genuine)     â”‚ Medium     â”‚ High      â”‚ Persistence + fill evidence filters; ban fast â”‚
â”‚                                               â”‚            â”‚           â”‚ oscillators; reduce size in suspect regimes   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Adversarial â€œlure and pullâ€ behavior           â”‚ Lowâ€“Med    â”‚ High      â”‚ Randomize entry timing; cap participation;    â”‚
â”‚                                               â”‚            â”‚           â”‚ instrument selection; post-trade analytics    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hidden liquidity misleads early-stop (exit too â”‚ Medium     â”‚ Lowâ€“Med   â”‚ Accept false negatives; treat early-stop as   â”‚
â”‚ early or too late)                             â”‚            â”‚           â”‚ safety; calibrate with backtests              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Connectivity loss / feed gaps while in positionâ”‚ Lowâ€“Med    â”‚ High      â”‚ Heartbeats; failover feeds; broker redundancy;â”‚
â”‚                                               â”‚            â”‚           â”‚ auto-flatten on data loss                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Compliance breach (data licensing / MNPI)      â”‚ Low        â”‚ Very High â”‚ Legal review; data entitlement checks; access â”‚
â”‚                                               â”‚            â”‚           â”‚ controls; restricted lists; audit trails      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âš ï¸ **COMPLIANCE NOTE:** The final row is not â€œoperational riskâ€ but existential firm risk. The strategyâ€™s strongest technical performance is irrelevant if data licensing is violated or if the approach is deployed in a context where the trader has MNPI exposure.

---

## 6. Implementation Roadmap and Recommendations

A phased implementation is recommended to separate engineering validation from regulatory exposure, and to ensure that early-stop correctness is verified under high event rates before any material capital deployment.

In Phase 1, the research team should paper-trade and replay historical data using free or low-cost crypto exchange order book streams, focusing on end-to-end pipeline correctness: local book maintenance, anomaly detection calibration, credibility filtering, and early-stop execution simulation. Binanceâ€™s market data stream specifications and Coinbaseâ€™s Level 2 WebSocket channels provide suitable real-time-like environments for this purpose. ([github.com](https://github.com/binance/binance-spot-api-docs?utm_source=openai)) The performance objective of this phase is not profit, but reduction of false early-stops, avoidance of book-desynchronization, and measurement of unsupported exposure time under feed stress.

In Phase 2, the strategy can be deployed with small live risk in crypto markets, still treating the effort as an engineering and monitoring exercise rather than a performance program. The key deliverable is a stable â€œproduction loopâ€ with dashboards that display detected levels, confidence metrics, position state, feed health, and compliance logs. The strategy should be configured with tight kill switches and conservative sizing, because cryptoâ€™s regime shifts and event-driven volatility can overwhelm early-stop assumptions.

In Phase 3, migration to regulated equity markets can be pursued through broker APIs that provide market depth and order management, such as the Interactive Brokers TWS APIâ€™s market depth requests. ([interactivebrokers.github.io](https://interactivebrokers.github.io/tws-api/market_depth.html?utm_source=openai)) This phase must include explicit market data entitlement verification, exchange fee budgeting, and jurisdiction-specific compliance sign-off. Particular attention should be given to short-selling constraints, locate/borrow mechanics, and market structure differences such as auctions, halts, and consolidated vs. fragmented liquidity.

Expected return characteristics should be estimated empirically rather than asserted. Opportunity frequency depends on the instrumentâ€™s depth distribution and on the thresholding of â€œlargeâ€ levels; average hold time depends on the persistence of detected depth; win rate is heavily influenced by transaction costs and by the early-stop slippage distribution. The appropriate internal target is not a single Sharpe estimate, but a plausible range conditioned on conservative assumptions. If the strategy is run with strict same-side alignment, small per-trade risk, and aggressive early-stops, the return distribution is likely to be characterized by many small outcomes and occasional large adverse slippage events when support vanishes during fast markets. This suggests that risk-adjusted performance is primarily a function of engineering quality (exit reliability) and filtering (avoiding spoof-like depth), rather than raw signal discovery.

âš ï¸ **COMPLIANCE NOTE:** Any discussion of expected returns is inherently speculative and must not be used as marketing or client-facing material. Internal evaluation should use controlled backtests with explicit cost models and robust out-of-sample validation.

---

## Appendix A: Glossary of Microstructure Terms

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Term                          â”‚ Definition                                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Limit Order Book (LOB)        â”‚ The set of resting buy/sell limit orders organized by price and priority.    â”‚
â”‚ Best Bid / Best Ask           â”‚ Highest buy price / lowest sell price currently available.                   â”‚
â”‚ Spread                        â”‚ \(s_t = a_t - b_t\), the difference between best ask and best bid.            â”‚
â”‚ Mid-price                     â”‚ \(m_t = (a_t + b_t)/2\), a reference â€œcenterâ€ price.                         â”‚
â”‚ Depth                         â”‚ Visible quantity available at each price level on bid/ask sides.             â”‚
â”‚ Level 1 data                  â”‚ Top-of-book quotes and last trade (best bid/ask and related fields).         â”‚
â”‚ Level 2 data                  â”‚ Market-by-price ladder of aggregated depth across multiple price levels.     â”‚
â”‚ Level 3 data                  â”‚ Order-by-order updates with IDs, modifications, cancellations, queue detail. â”‚
â”‚ Iceberg (reserve) order       â”‚ Order with a displayed portion and a hidden reserve replenishing the display.â”‚
â”‚ TWAP / VWAP                   â”‚ Algorithms slicing execution over time or relative to volume.                â”‚
â”‚ Absorption                    â”‚ Large executed volume occurs with limited price movement, implying passive    â”‚
â”‚                               â”‚ liquidity is absorbing aggressor flow.                                       â”‚
â”‚ Order Flow Imbalance (OFI)    â”‚ Imbalance in order book events at best quotes that predicts short-horizon     â”‚
â”‚                               â”‚ price changes in empirical studies.                                          â”‚
â”‚ Spoofing                      â”‚ Under CEA, bidding/offering with intent to cancel before execution.          â”‚
â”‚ Layering                      â”‚ A spoofing variant placing multiple deceptive orders at different levels.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Appendix B: Regulatory Reference Table (U.S., EU, UK, HK, CN)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Jurisdiction â”‚ Primary Market Manipulation Framework         â”‚ Notes Relevant to Order-Book Strategies                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ United Statesâ”‚ CEA 7 U.S.C. Â§ 6c(a)(5)(C) (spoofing);         â”‚ Spoofing defined by intent to cancel before execution; CFTC      â”‚
â”‚ (Futures)    â”‚ CFTC interpretive guidance                    â”‚ guidance emphasizes scienter and context.                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ United Statesâ”‚ Exchange Act Rule 10b-5; 15 U.S.C. Â§ 78i       â”‚ Manipulative/deceptive schemes and series of transactions         â”‚
â”‚ (Securities) â”‚ (Section 9(a)(2))                             â”‚ creating artificial price effects can be actionable.             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ European Unionâ”‚ MAR Regulation (EU) No 596/2014, Article 12   â”‚ Orders giving false/misleading signals or securing artificial     â”‚
â”‚              â”‚ + indicators in annexes/delegated acts        â”‚ prices; includes patterns of orders removed before execution.     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ United Kingdomâ”‚ UK MAR (onshored MAR) + FCA enforcement       â”‚ Substantively aligned with MAR concepts; enforcement guidance      â”‚
â”‚              â”‚                                               â”‚ and market practice context remain critical.                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hong Kong     â”‚ SFO market misconduct provisions             â”‚ False/misleading appearance of active trading and price rigging    â”‚
â”‚              â”‚                                               â”‚ concepts can apply; local counsel review recommended.             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mainland Chinaâ”‚ CSRC/Exchange rules on manipulation          â”‚ Venue-specific rules and enforcement practices; local expertise    â”‚
â”‚              â”‚                                               â”‚ required prior to deployment.                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âš ï¸ **COMPLIANCE NOTE:** The table is a high-level map and is not a substitute for jurisdiction-specific legal advice. Strategy deployment must be reviewed against the relevant venue rulebooks, local statutes, and current enforcement posture.

---

## References

Gould, M. D., Porter, M. A., Williams, S., McDonald, M., Fenn, D. J., & Howison, S. D. (2013). â€œLimit order books.â€ *Quantitative Finance*, 13(11), 1709â€“1742. DOI: 10.1080/14697688.2013.803148. ([aimsciences.org](https://www.aimsciences.org/article/doi/10.3934/dcdsb.2019206?utm_source=openai))

Cont, R., Kukanov, A., & Stoikov, S. (2014). â€œThe Price Impact of Order Book Events.â€ *Journal of Financial Econometrics*, 12(1), 47â€“88. ([academic.oup.com](https://academic.oup.com/jfec/article-abstract/12/1/47/816163?utm_source=openai))

Tuccella, J.-N., Nadler, P., & Åerban, O. (2021). â€œProtecting Retail Investors from Order Book Spoofing using a GRU-based Detection Model.â€ arXiv:2110.03687. ([arxiv.org](https://arxiv.org/abs/2110.03687?utm_source=openai))

Fabre, T., & Challet, D. (2025). â€œLearning the Spoofability of Limit Order Books With Interpretable Probabilistic Neural Networks.â€ arXiv:2504.15908. ([arxiv.org](https://arxiv.org/abs/2504.15908?utm_source=openai))

Commodity Exchange Act, 7 U.S.C. Â§ 6c(a)(5)(C) (spoofing definition) and related provisions. ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/7/6c?utm_source=openai))

CFTC (2011). Proposed interpretive guidance on disruptive trading practices, including spoofing scienter and good-faith cancellations. ([cftc.gov](https://www.cftc.gov/LawRegulation/FederalRegister/ProposedRules/2011-6398.html?utm_source=openai))

*United States v. Coscia*, No. 16-3017 (7th Cir. 2017) (discussion of spoofing definition and conviction). ([law.justia.com](https://law.justia.com/cases/federal/appellate-courts/ca7/16-3017/16-3017-2017-08-07.html?utm_source=openai))

U.S. Department of Justice (2016; updated 2025). â€œFutures Trader Pleads Guilty to Illegally Manipulating the Futures Market in Connection With 2010 â€˜Flash Crashâ€™.â€ ([justice.gov](https://www.justice.gov/archives/opa/pr/futures-trader-pleads-guilty-illegally-manipulating-futures-market-connection-2010-flash?utm_source=openai))

CFTC Press Release 8260-20 (September 29, 2020). â€œCFTC Orders JPMorgan to Pay Record $920 Million for Spoofing and Manipulation.â€ ([cftc.gov](https://www.cftc.gov/PressRoom/PressReleases/8260-20?utm_source=openai))

Regulation (EU) No 596/2014 (Market Abuse Regulation), Article 12 and related indicators. ([eur-lex.europa.eu](https://eur-lex.europa.eu/eli/reg/2014/596/2016-07-03/eng?utm_source=openai))

Interactive Brokers. TWS API Market Depth documentation (`reqMktDepth` / `reqMarketDepth`) and limitations. ([interactivebrokers.github.io](https://interactivebrokers.github.io/tws-api/market_depth.html?utm_source=openai))

Binance. Official Spot API and WebSocket market data stream documentation repository. ([github.com](https://github.com/binance/binance-spot-api-docs?utm_source=openai))

Coinbase. Advanced Trade WebSocket feed documentation and Level 2 channel overview. ([docs.cdp.coinbase.com](https://docs.cdp.coinbase.com/coinbase-app/advanced-trade-apis/websocket/websocket-overview?utm_source=openai))

U.S. Securities Exchange Act Rule 10b-5 (17 CFR Â§ 240.10b-5). ([law.cornell.edu](https://www.law.cornell.edu/cfr/text/17/240.10b-5?utm_source=openai))

15 U.S.C. Â§ 78i (Exchange Act Section 9: manipulation of security prices, including Section 9(a)(2)). ([law.cornell.edu](https://www.law.cornell.edu/uscode/text/15/78i?utm_source=openai))

---
Learn more:
1. [United States v. Coscia, No. 16-3017 (7th Cir. 2017) :: Justia](https://law.justia.com/cases/federal/appellate-courts/ca7/16-3017/16-3017-2017-08-07.html?utm_source=openai)
2. [A dynamic model of the limit order book](https://www.aimsciences.org/article/doi/10.3934/dcdsb.2019206?utm_source=openai)
3. [Market Depth - reqMktDepth](https://www.interactivebrokers.co.jp/en/software/csharp/topics/market%20depth.htm?utm_source=openai)
4. [Price Impact of Order Book Events | Journal of Financial Econometrics | Oxford Academic](https://academic.oup.com/jfec/article-abstract/12/1/47/816163?utm_source=openai)
5. [GitHub - binance/binance-spot-api-docs: Official Documentation for the Binance Spot APIs and Streams](https://github.com/binance/binance-spot-api-docs?utm_source=openai)
6. [Advanced Trade WebSocket Overview - Coinbase Developer Documentation](https://docs.cdp.coinbase.com/coinbase-app/advanced-trade-apis/websocket/websocket-overview?utm_source=openai)
7. [Manual Â· ccxt/ccxt Wiki Â· GitHub](https://github.com/ccxt/ccxt/wiki/manual?utm_source=openai)
8. [2011-6398 | CFTC](https://www.cftc.gov/LawRegulation/FederalRegister/ProposedRules/2011-6398.html?utm_source=openai)
9. [7 U.S. Code Â§ 6c - Prohibited transactions | U.S. Code | US Law | LII / Legal Information Institute](https://www.law.cornell.edu/uscode/text/7/6c?utm_source=openai)
10. [EUR-Lex - 02014R0596-20160703 - EN - EUR-Lex](https://eur-lex.europa.eu/eli/reg/2014/596/2016-07-03/eng?utm_source=openai)
11. [Office of Public Affairs | Futures Trader Pleads Guilty to Illegally Manipulating the Futures Market in Connection With 2010 â€œFlash Crashâ€ | United States Department of Justice](https://www.justice.gov/archives/opa/pr/futures-trader-pleads-guilty-illegally-manipulating-futures-market-connection-2010-flash?utm_source=openai)
12. [CFTC Orders JPMorgan to Pay Record $920 Million for Spoofing and Manipulation | CFTC](https://www.cftc.gov/PressRoom/PressReleases/8260-20?utm_source=openai)
13. [Market manipulation - FMA Ã–sterreich](https://www.fma.gv.at/en/capital-markets/market-abuse/market-manipulation/?utm_source=openai)
14. [17 CFR Â§ 240.10b-5 - Employment of manipulative and deceptive devices. | Electronic Code of Federal Regulations (e-CFR) | US Law | LII / Legal Information Institute](https://www.law.cornell.edu/cfr/text/17/240.10b-5?utm_source=openai)
15. [15 U.S. Code Â§ 78i - Manipulation of security prices | U.S. Code | US Law | LII / Legal Information Institute](https://www.law.cornell.edu/uscode/text/15/78i?utm_source=openai)
16. [Protecting Retail Investors from Order Book Spoofing using a GRU-based Detection Model](https://arxiv.org/abs/2110.03687?utm_source=openai)
17. [TWS API v9.72+: Market Depth (Level II)](https://interactivebrokers.github.io/tws-api/market_depth.html?utm_source=openai)
18. [Learning the Spoofability of Limit Order Books With Interpretable Probabilistic Neural Networks](https://arxiv.org/abs/2504.15908?utm_source=openai)