<?xml version="1.0" encoding="UTF-8"?>
<documents>
  <document index="1">
    <source>all.hpp</source>
    <document_content><![CDATA[/* This file is automatically generated; do not edit.     */
/* Add the files to be included into Makefile.am instead. */

#include <ql/experimental/credit/basecorrelationlossmodel.hpp>
#include <ql/experimental/credit/basecorrelationstructure.hpp>
#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/binomiallossmodel.hpp>
#include <ql/experimental/credit/blackcdsoptionengine.hpp>
#include <ql/experimental/credit/cdo.hpp>
#include <ql/experimental/credit/cdsoption.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/correlationstructure.hpp>
#include <ql/experimental/credit/defaultevent.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <ql/experimental/credit/defaultprobabilitylatentmodel.hpp>
#include <ql/experimental/credit/defaulttype.hpp>
#include <ql/experimental/credit/distribution.hpp>
#include <ql/experimental/credit/factorspreadedhazardratecurve.hpp>
#include <ql/experimental/credit/gaussianlhplossmodel.hpp>
#include <ql/experimental/credit/homogeneouspooldef.hpp>
#include <ql/experimental/credit/inhomogeneouspooldef.hpp>
#include <ql/experimental/credit/integralcdoengine.hpp>
#include <ql/experimental/credit/integralntdengine.hpp>
#include <ql/experimental/credit/interpolatedaffinehazardratecurve.hpp>
#include <ql/experimental/credit/issuer.hpp>
#include <ql/experimental/credit/loss.hpp>
#include <ql/experimental/credit/lossdistribution.hpp>
#include <ql/experimental/credit/midpointcdoengine.hpp>
#include <ql/experimental/credit/nthtodefault.hpp>
#include <ql/experimental/credit/onefactoraffinesurvival.hpp>
#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/experimental/credit/onefactorgaussiancopula.hpp>
#include <ql/experimental/credit/onefactorstudentcopula.hpp>
#include <ql/experimental/credit/pool.hpp>
#include <ql/experimental/credit/randomdefaultlatentmodel.hpp>
#include <ql/experimental/credit/randomdefaultmodel.hpp>
#include <ql/experimental/credit/randomlosslatentmodel.hpp>
#include <ql/experimental/credit/recoveryratemodel.hpp>
#include <ql/experimental/credit/recoveryratequote.hpp>
#include <ql/experimental/credit/recursivelossmodel.hpp>
#include <ql/experimental/credit/riskyassetswap.hpp>
#include <ql/experimental/credit/riskyassetswapoption.hpp>
#include <ql/experimental/credit/riskybond.hpp>
#include <ql/experimental/credit/saddlepointlossmodel.hpp>
#include <ql/experimental/credit/spotlosslatentmodel.hpp>
#include <ql/experimental/credit/spreadedhazardratecurve.hpp>
#include <ql/experimental/credit/syntheticcdo.hpp>

]]></document_content>
  </document>
  <document index="2">
    <source>basecorrelationlossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_base_correl_lossmodel_hpp
#define quantlib_base_correl_lossmodel_hpp


#include <ql/quote.hpp>
#include <ql/quotes/simplequote.hpp>

#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/experimental/credit/basecorrelationstructure.hpp>

// move these to the CPP (and the template spezs)
#include <ql/experimental/credit/binomiallossmodel.hpp>
#include <ql/experimental/credit/gaussianlhplossmodel.hpp>
#include <ql/experimental/credit/inhomogeneouspooldef.hpp>
#include <utility>

namespace QuantLib {

    /*! Base Correlation loss model; interpolation is performed by portfolio 
    (live) amount percentage.\par
    Though the literature on this model is inmense, see for a more than 
    introductory level (precrisis) chapters 19, 20 and 21 of <b>Modelling single
    name and multi-name credit derivatives.</b> Dominic O'Kane, Wiley Finance, 
    2008\par
    For freely available documentation see:\par
    Credit Correlation: A Guide; JP Morgan Credit Derivatives Strategy; 
        12 March 2004 \par
    Introducing Base Correlations; JP Morgan Credit Derivatives Strategy; 
        22 March 2004 \par
    A Relative Value Framework for Credit Correlation; JP Morgan Credit 
        Derivatives Strategy; 27 April 2004 \par
    Valuing and Hedging Synthetic CDO Tranches Using Base Correlations; Bear 
        Stearns; May 17, 2004 \par
    Correlation Primer; Nomura Fixed Income Research, August 6, 2004 \par
    Base Correlation Explained; Lehman Brothers Fixed Income Quantitative 
        Credit Research; 15 November 2004 \par
    'Pricing CDOs with a smile' in Societe Generale Credit Research; 
        February 2005 \par
    For bespoke base correlation see: \par
    Base Correlation Mapping in Lehman Brothers' Quantitative Credit Research 
        Quarterly; Volume 2007-Q1 \par
    You can explore typical postcrisis data by perusing some of the JPMorgan 
    Global Correlation Daily Analytics \par
    Here the crisis model problems of ability to price stressed portfolios 
    or tranches over the maximum loss are the responsibility of the base models.
    Users should select their models according to this; choosing the copula or
    a random loss given default base model (or more exotic ones). \par
    Notice this is different to a bespoke base correlation loss (bespoke here 
    refering to basket composition, not just attachment levels) ; where 
    loss interpolation is on the expected loss value to match the two baskets. 
    Therefore the correlation surface should refer to the same basket intended
    to be priced. But this is left to the user and is not implemented in the 
    correlation surface (yet...)

    \todo Bespoke portfolios BC models are yet to be implemented.

    BaseModel_T must have a constructor with a single quote value
    */
    /* Criticism:
    This model is not as generic as it could be. In principle a default loss 
    model dependent on a single factor correlation parameter is the only 
    restriction on the base loss model(s) type. This class however is tied to a 
    LatentModel single factor. But there is no need for the 
    underlying model to be of a latent type. This link is due to the copula 
    initialization traits which have to be present for non trivial copula 
    policies initialization (e.g. Student-T base correl models)

    Maybe a possibility is to pass copiable instances of the model and relinking
    to the correlation in two internal copies.
    */
    template <class BaseModel_T, class Corr2DInt_T>
    class BaseCorrelationLossModel : public DefaultLossModel, 
        public virtual Observer {
    private:
        typedef typename BaseModel_T::copulaType::initTraits initTraits;
    public:
      BaseCorrelationLossModel(const Handle<BaseCorrelationTermStructure<Corr2DInt_T> >& correlTS,
                               std::vector<Real> recoveries,
                               const initTraits& traits = initTraits())
      : localCorrelationAttach_(ext::make_shared<SimpleQuote>(0.)),
        localCorrelationDetach_(ext::make_shared<SimpleQuote>(0.)),
        recoveries_(std::move(recoveries)), correlTS_(correlTS), copulaTraits_(traits) {
          registerWith(correlTS);
          registerWith(Settings::instance().evaluationDate());
      }

    private:
        // react to base correl surface notifications (quotes or reference date)
      void update() override {
          setupModels();
          // tell basket to notify instruments, etc, we are invalid
          if (!basket_.empty())
              basket_->notifyObservers();
      }

        /* Update model caches after basket assignement. */
      void resetModel() override {
          remainingNotional_ = basket_->remainingNotional();
          attachRatio_ = basket_->remainingAttachmentAmount() / remainingNotional_;
          detachRatio_ = basket_->remainingDetachmentAmount() / remainingNotional_;

          basketAttach_ = ext::make_shared<Basket>(basket_->refDate(), basket_->remainingNames(),
                                                   basket_->remainingNotionals(), basket_->pool(),
                                                   0.0, attachRatio_, basket_->claim());
          basketDetach_ = ext::make_shared<Basket>(basket_->refDate(), basket_->remainingNames(),
                                                   basket_->remainingNotionals(), basket_->pool(),
                                                   0.0, detachRatio_, basket_->claim());
          setupModels();
      }
        /* Most of the statistics are not implemented, not impossible but
        the model is intended for pricing rather than ptfolio risk management.
        */
      Real expectedTrancheLoss(const Date& d) const override;

    protected:
        /*! Sets up attach/detach models. Gets called on basket update. 
        To be specialized on the spacific model type.
        */
        void setupModels() const;
    private:
        mutable Real attachRatio_, detachRatio_;
        mutable Real remainingNotional_;

        //! Correlation buffer to pick up values from the surface and 
        //  trigger calculation.
        ext::shared_ptr<SimpleQuote> localCorrelationAttach_, 
            localCorrelationDetach_;
        mutable ext::shared_ptr<Basket> basketAttach_,
            basketDetach_;
        // just cached for the update method
        mutable std::vector<Real> recoveries_;
        Handle<BaseCorrelationTermStructure<Corr2DInt_T> > correlTS_;
        // Initialization parameters for models copula
        mutable typename BaseModel_T::copulaType::initTraits copulaTraits_;
        // Models of equity baskets.
        mutable ext::shared_ptr<BaseModel_T> scalarCorrelModelAttach_;
        mutable ext::shared_ptr<BaseModel_T> scalarCorrelModelDetach_;
    };


    // Remember ETL returns the EL on the live part of the basket. 
    template<class LM, class I>
    Real BaseCorrelationLossModel<LM, I>::expectedTrancheLoss(
        const Date& d) const 
    {
        Real correlK1 = correlTS_->correlation(d, attachRatio_);
        Real correlK2 = correlTS_->correlation(d, detachRatio_);

        /* reset correl and call base models which have the different baskets 
        associated.*/
        localCorrelationAttach_->setValue(correlK1);
        Real expLossK1 = 
            basketAttach_->expectedTrancheLoss(d);
        localCorrelationDetach_->setValue(correlK2);
        Real expLossK2 = 
            basketDetach_->expectedTrancheLoss(d);
        return expLossK2 - expLossK1;
    }


    // ----------------------------------------------------------------------


    /* Concrete specializations submodels construction. With the dummy template 
    parameter trick partial specializations leaving the interpolation open 
    would be possible.
    */

    #ifndef QL_PATCH_SOLARIS

    template<>
    inline void BaseCorrelationLossModel<GaussianLHPLossModel, 
        BilinearInterpolation>::setupModels() const 
    {
        // on this assignment any previous registration with the attach and 
        //   detach baskets should be removed
        scalarCorrelModelAttach_ = ext::make_shared<GaussianLHPLossModel>(
            Handle<Quote>(localCorrelationAttach_), recoveries_);
        scalarCorrelModelDetach_ = ext::make_shared<GaussianLHPLossModel>(
            Handle<Quote>(localCorrelationDetach_), recoveries_);

        basketAttach_->setLossModel(scalarCorrelModelAttach_);
        basketDetach_->setLossModel(scalarCorrelModelDetach_);
    }

    template<>
    inline void BaseCorrelationLossModel<GaussianBinomialLossModel, 
        BilinearInterpolation>::setupModels() const 
    {
        ext::shared_ptr<GaussianConstantLossLM> lmA = 
            ext::make_shared<GaussianConstantLossLM>(
                Handle<Quote>(localCorrelationAttach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);
        ext::shared_ptr<GaussianConstantLossLM> lmD = 
            ext::make_shared<GaussianConstantLossLM>(
                Handle<Quote>(localCorrelationDetach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);
        scalarCorrelModelAttach_ = 
            ext::make_shared<GaussianBinomialLossModel>(lmA);
        scalarCorrelModelDetach_ = 
            ext::make_shared<GaussianBinomialLossModel>(lmD);
            
        basketAttach_->setLossModel(scalarCorrelModelAttach_);
        basketDetach_->setLossModel(scalarCorrelModelDetach_);

    }

    template<>
    inline void BaseCorrelationLossModel<TBinomialLossModel, 
        BilinearInterpolation>::setupModels() const 
    {
        ext::shared_ptr<TConstantLossLM> lmA = 
            ext::make_shared<TConstantLossLM>(
                Handle<Quote>(localCorrelationAttach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);
        ext::shared_ptr<TConstantLossLM> lmD = 
            ext::make_shared<TConstantLossLM>(
                Handle<Quote>(localCorrelationDetach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);

        scalarCorrelModelAttach_ = 
            ext::make_shared<TBinomialLossModel>(lmA);
        scalarCorrelModelDetach_ = 
            ext::make_shared<TBinomialLossModel>(lmD);
            
        basketAttach_->setLossModel(scalarCorrelModelAttach_);
        basketDetach_->setLossModel(scalarCorrelModelDetach_);
    }

    /* \todo Fix this model, is failing for equity tranches at least, the
    base model works all right, its the link here.
    */
    template<>
    inline void BaseCorrelationLossModel<IHGaussPoolLossModel, 
        BilinearInterpolation>::setupModels() const 
    {
        ext::shared_ptr<GaussianConstantLossLM> lmA = 
            ext::make_shared<GaussianConstantLossLM>(
                Handle<Quote>(localCorrelationAttach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);
        ext::shared_ptr<GaussianConstantLossLM> lmD = 
            ext::make_shared<GaussianConstantLossLM>(
                Handle<Quote>(localCorrelationDetach_), recoveries_, 
                LatentModelIntegrationType::GaussianQuadrature, 
                recoveries_.size(), copulaTraits_);

        // \todo Allow the sending specific model params, as the number of 
        //   buckets here.
        scalarCorrelModelAttach_ = 
            ext::make_shared<IHGaussPoolLossModel>(lmA, 500);
        scalarCorrelModelDetach_ = 
            ext::make_shared<IHGaussPoolLossModel>(lmD, 500);
            
        basketAttach_->setLossModel(scalarCorrelModelAttach_);
        basketDetach_->setLossModel(scalarCorrelModelDetach_);
    }

    #endif


    // Vanilla BC model
    #ifndef QL_PATCH_SOLARIS
    typedef BaseCorrelationLossModel<GaussianLHPLossModel, 
                BilinearInterpolation> GaussianLHPFlatBCLM;
    #endif

}

#endif
]]></document_content>
  </document>
  <document index="3">
    <source>basecorrelationstructure.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_base_correl_structure_hpp
#define quantlib_base_correl_structure_hpp

#include <ql/quote.hpp>
#include <ql/utilities/dataformatters.hpp>
#include <ql/math/interpolations/bilinearinterpolation.hpp>
#include <ql/math/interpolations/bicubicsplineinterpolation.hpp>

#include <ql/experimental/credit/correlationstructure.hpp>

namespace QuantLib {


    /*! Matrix based Base Correlation Term Structure\par
    Loss level versus time interpolated scalar copula type parametric 
    correlation term structure. Represents the correlation for the credit loss 
    level of a given portfolio at a given loss level and time.

    \todo The relation to a given basket is to be made explicit for bespoke 
    models to be implemented.
    \todo Consider moving to a matrix data structure. A matrix might make some
    computations heavy, template specialization on the dimension might be an
    alternative to having two classes, one for scalars and another for matrices.
    \todo Rethink all the data structure with a basket where current losses are 
    not zero.
    \todo In principle the 2D interpolator is left optional since there are 
    arbitrage issues on the interpolator type to be used. However one has to be
    careful when using non local interpolators like CubicSplines which have an
    effect on the past (calibrated) coupons of previous tenors.
    */
    template<class Interpolator2D_T>
    class BaseCorrelationTermStructure : public CorrelationTermStructure {
    public:
        /*
        @param correls Corresponds to: correls[iYear][iLoss]

        The Settlement date should in an ideal world coincide with the 
        (implicit) basket inception date and its default term structures 
        settlement dates.
        */
        BaseCorrelationTermStructure(
            Natural settlementDays,
            const Calendar& cal,
            BusinessDayConvention bdc,
            const std::vector<Period>& tenors,// sorted
            const std::vector<Real>& lossLevel,//sorted
            const std::vector<std::vector<Handle<Quote> > >& correls,
            const DayCounter& dc = DayCounter()
            )
        : CorrelationTermStructure(settlementDays, cal, bdc, dc),
          correlHandles_(correls),
          correlations_(correls.size(), correls.front().size()),
          nTrancheTenors_(tenors.size()),
          nLosses_(lossLevel.size()),
          tenors_(tenors),
          lossLevel_(lossLevel),
          trancheTimes_(tenors.size(), 0.) {
              checkTrancheTenors();

              for (auto& tenor : tenors_)
                  trancheDates_.push_back(
                      calendar().advance(referenceDate(), tenor, businessDayConvention()));

              initializeTrancheTimes();
              checkInputs(correlations_.rows(), correlations_.columns());
                updateMatrix();
              registerWithMarketData();
              // call factory
              setupInterpolation();
        }
    private:
        virtual void setupInterpolation() ;
    public:
      Size correlationSize() const override { return 1; }
      //! Implicit correlation for the given loss interval.
      Real ImplicitCorrelation(Real, Real);

      void checkTrancheTenors() const;
      void checkLosses() const;
      void initializeTrancheTimes() const;
      void checkInputs(Size volRows, Size volsColumns) const;
      void registerWithMarketData();

      void update() override;
      void updateMatrix() const;

      // TermStructure interface
      Date maxDate() const override { return trancheDates_.back(); }
      Real correlation(const Date& d, Real lossLevel, bool extrapolate = false) const {
          return correlation(timeFromReference(d), lossLevel, extrapolate);
        }
        Real correlation(Time t, Real lossLevel, 
            bool extrapolate = false) const 
        {
            return interpolation_(t, lossLevel, true);
        }
    private:
        std::vector<std::vector<Handle<Quote> > > correlHandles_;
        mutable Matrix correlations_;
        Interpolation2D interpolation_;
        Size nTrancheTenors_,
            nLosses_;
        std::vector<Period> tenors_;
        mutable std::vector<Real> lossLevel_;
        mutable std::vector<Date> trancheDates_;
        mutable std::vector<Time> trancheTimes_;
    };

    // ----------------------------------------------------------------------

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::checkTrancheTenors() const {
        QL_REQUIRE(tenors_[0]>0*Days,
                   "first tranche tenor is negative (" <<
                   tenors_[0] << ")");
        for (Size i=1; i<nTrancheTenors_; ++i)
            QL_REQUIRE(tenors_[i]>tenors_[i-1],
                       "non increasing tranche tenor: " << io::ordinal(i) <<
                       " is " << tenors_[i-1] << ", " << io::ordinal(i+1) <<
                       " is " << tenors_[i]);
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::checkLosses() const {
        QL_REQUIRE(lossLevel_[0]>0.,
                   "first loss level is negative (" <<
                   lossLevel_[0] << ")");
        QL_REQUIRE(lossLevel_[0] <= 1.,
            "First loss level larger than 100% (" << lossLevel_[0] <<")");
        for (Size i=1; i<nLosses_; ++i) {
            QL_REQUIRE(lossLevel_[i]>lossLevel_[i-1],
                       "non increasing losses: " << io::ordinal(i) <<
                       " is " << lossLevel_[i-1] << ", " << io::ordinal(i+1) <<
                       " is " << lossLevel_[i]);
        QL_REQUIRE(lossLevel_[i] <= 1.,
            "Loss level " << i << " larger than 100% (" << lossLevel_[i] <<")");
        }
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::initializeTrancheTimes() const {
        for (Size i=0; i<nTrancheTenors_; ++i)
            trancheTimes_[i] = timeFromReference(trancheDates_[i]);
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::checkInputs(Size volRows,
                                               Size volsColumns) const {
        QL_REQUIRE(nLosses_==volRows,
                   "mismatch between number of loss levels (" <<
                   nLosses_ << ") and number of rows (" << volRows <<
                   ") in the correl matrix");
        QL_REQUIRE(nTrancheTenors_==volsColumns,
                   "mismatch between number of tranche tenors (" <<
                   nTrancheTenors_ << ") and number of columns (" << 
                   volsColumns << ") in the correl matrix");
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::registerWithMarketData()
    {
        for (Size i=0; i<correlHandles_.size(); ++i)
            for (Size j=0; j<correlHandles_.front().size(); ++j)
                registerWith(correlHandles_[i][j]);
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::update() {
        updateMatrix();
        TermStructure::update();
    }

    template <class I2D_T>
    void BaseCorrelationTermStructure<I2D_T>::updateMatrix() const {
        for (Size i=0; i<correlHandles_.size(); ++i)
            for (Size j=0; j<correlHandles_.front().size(); ++j)
                correlations_[i][j] = correlHandles_[i][j]->value();

    }

}

#endif
]]></document_content>
  </document>
  <document index="4">
    <source>basket.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file basket.hpp
    \brief basket of issuers and related notionals
*/

#ifndef quantlib_basket_hpp
#define quantlib_basket_hpp

#include <ql/instruments/claim.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <ql/patterns/lazyobject.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <ql/experimental/credit/issuer.hpp>
#include <ql/experimental/credit/recoveryratemodel.hpp>
#include <ql/experimental/credit/pool.hpp>
#include <ql/experimental/credit/loss.hpp>
#include <ql/utilities/disposable.hpp>

namespace QuantLib {

    class DefaultLossModel;

    /*! Credit Basket.\par
        A basket is a collection of credit names, represented by a
        unique identifier (a text string), associated notional
        amounts, a pool and tranche information. The pool is a map of
        "names" to issuers.  The Basket structure is motivated by CDO
        squared instruments containing various underlying inner CDOs
        which can be represented by respective baskets including their
        tranche structure.  The role of the Pool is providing a unique
        list of relevant issuers while names may appear multiple times
        across different baskets (overlap).
     */
    class Basket : public LazyObject {
      public:
        Basket() = default;
        /*! Constructs a basket of simple collection of constant notional 
          positions subject to default risk only.
          
          The refDate parameter is the basket inception date, that is,
          the date at which defaultable events are relevant. (There
          are no constraints on forward baskets but models assigned
          should be consistent.)
        */
        Basket(const Date& refDate,
               const std::vector<std::string>& names,
               std::vector<Real> notionals,
               ext::shared_ptr<Pool> pool,
               Real attachmentRatio = 0.0,
               Real detachmentRatio = 1.0,
               ext::shared_ptr<Claim> claim = ext::shared_ptr<Claim>(new FaceValueClaim()));
        void update() override {
            computeBasket();
            LazyObject::update();
        }
        void computeBasket() const {
            Date today = Settings::instance().evaluationDate();
            /* update cache values at the calculation date (work as arguments 
              to the Loss Models)
            \to do: IMPORTANT: notice that defaults added to Issuers dont get
            notify as the codes stnds today. Issuers need to be observables.
            */
            //this one must remain on top since there are dependencies
            evalDateLiveKeys_      = remainingDefaultKeys(today);
            evalDateSettledLoss_   = settledLoss(today);
            evalDateRemainingNot_  = remainingNotional(today);
            evalDateLiveNotionals_ = remainingNotionals(today);
            evalDateLiveNames_     = remainingNames(today);
            evalDateAttachAmount_  = remainingAttachmentAmount(today);
            evalDateDetachAmmount_ = 
                remainingDetachmentAmount(today);
            evalDateLiveList_ = liveList(today);
        }
        //! Basket inception number of counterparties.
        Size size() const;
        //! Basket counterparties names at inception.
        const std::vector<std::string>& names() const {return pool_->names();}
        //! Basket counterparties notionals at inception.
        const std::vector<Real>& notionals() const;
        //! Basket total notional at inception.
        Real notional() const;
        //! Returns the total expected exposures for that name.
        Real exposure(const std::string& name, const Date& = Date()) const;
        //! Underlying pool
        const ext::shared_ptr<Pool>& pool() const;
        //! The keys each counterparty enters the basket with (sensitive to)
        Disposable<std::vector<DefaultProbKey> > defaultKeys() const;
        /*! Loss Given Default for all issuers/notionals based on
            expected recovery rates for the respective issuers.
        */
        //! Basket inception date.
        const Date& refDate() const {return refDate_;}
        /*! Attachment point expressed as a fraction of the total inception 
          notional.
        */
        Real attachmentRatio() const {return attachmentRatio_;}
        //! Detachment point expressed as a fraction of the total pool notional
        Real detachmentRatio() const {return detachmentRatio_;}
        //! Original basket notional ignoring any losses.
        Real basketNotional() const {return basketNotional_;}
        //! Original tranche notional ignoring any realized losses.
        Real trancheNotional() const {return trancheNotional_;}
        //! Attachment amount = attachmentRatio() * basketNotional()
        Real attachmentAmount() const {return attachmentAmount_;}
        //! Detachment amount = detachmentRatio() * basketNotional()
        Real detachmentAmount() const {return detachmentAmount_;}
        //! default claim, same for all positions and counterparties
        ext::shared_ptr<Claim> claim() const {return claim_;}
        /*! Vector of cumulative default probability to date d for all
            issuers in the basket.
        */
        Disposable<std::vector<Probability> > 
            probabilities(const Date& d) const;
        /*! Realized basket losses between the reference date and the 
            calculation date, taking the actual recovery rates of loss events 
            into account. 
            Only default events that have settled (have a realized RR) are 
            accounted for. For contingent losses after a default you need
            to compute the losses through a DefaultLossModel

            Optionally one can pass a date in the future and that will collect 
            events stored in the issuers list. This shows the effect of 
            'programmed' (after today's) events on top of past ones. The 
            intention is to be used in risk analysis (jump to default, etc).
        */
        Real settledLoss() const;
        Real settledLoss(const Date&) const;
        /*! Actual basket losses between the reference date and the calculation
            date, taking the actual recovery rates of loss events into account.
            If the event has not settled yet a model driven recovery is used.

            Returns the realized losses in this portfolio since the portfolio
            default reference date.
            This method relies on an implementation of the loss given default 
            since the events have not necessarily settled.
        */
        Real cumulatedLoss() const;
        Real cumulatedLoss(const Date&) const;
        /*! Remaining full basket (untranched) notional after settled losses 
          between the reference date and the given date.  The full notional 
          for defaulted names is subracted, recovery ignored.
        */
        Real remainingNotional() const;
        Real remainingNotional(const Date&) const;
        /*! Vector of surviving notionals after settled losses between the 
          reference date and the given date, recovery ignored.
        */
        const std::vector<Real>& remainingNotionals() const;
        Disposable<std::vector<Real> > remainingNotionals(const Date&) const;
        /*! Vector of surviving issuers after defaults between the reference 
          basket date and the given (or evaluation) date.
        */
        const std::vector<std::string>& remainingNames() const;
        Disposable<std::vector<std::string> > 
            remainingNames(const Date&) const;
        /*! Default keys of non defaulted counterparties
        */
        const std::vector<DefaultProbKey>& remainingDefaultKeys() const;
        Disposable<std::vector<DefaultProbKey> > remainingDefaultKeys(
            const Date&) const;
        //! Number of counterparties alive on the requested date.
        Size remainingSize() const;
        Size remainingSize(const Date&) const;
        /*! Vector of cumulative default probability to date d for all
            issuers still (at the evaluation date) alive in the basket.
        */
        Disposable<std::vector<Probability> > 
            remainingProbabilities(const Date& d) const;
        /*!
          Attachment amount of the equivalent (after defaults) remaining basket
          The remaining attachment amount is
          RAA = max (0, attachmentAmount - cumulatedLoss())

          The remaining attachment ratio is then
          RAR = RAA / remainingNotional()
        */
        Real remainingAttachmentAmount() const;
        Real remainingAttachmentAmount(const Date& endDate) const;

        /*!
          Detachment amount of the equivalent remaining basket.
          The remaining detachment amount is
          RDA = max (0, detachmentAmount - cumulatedLoss())

          The remaining detachment ratio is then
          RDR = RDA / remainingNotional()
        */
        Real remainingDetachmentAmount() const;
        Real remainingDetachmentAmount(const Date& endDate) const;

        //! Remaining basket tranched notional on calculation date
        Real remainingTrancheNotional() const {
            calculate();
            return evalDateDetachAmmount_ - evalDateAttachAmount_;
        }
        /*! Expected basket tranched notional on the requested date
            according to the basket model. Model should have been assigned.
        */
        Real remainingTrancheNotional(const Date& endDate) const {
            calculate();
            return remainingDetachmentAmount(endDate) - 
                remainingAttachmentAmount(endDate);
        }
        //!Indexes of remaining names. Notice these are names and not positions.
        const std::vector<Size>& liveList() const;
        Disposable<std::vector<Size> > liveList(const Date&) const;//?? keep?
        //! Assigns the default loss model to this basket. Resets calculations.
        void setLossModel(
            const ext::shared_ptr<DefaultLossModel>& lossModel);
        /*! \name Basket Loss Statistics
            Methods providing statistical metrics on the loss or value 
            distribution of the basket. Most calculations rely on the pressence
            of a model assigned to the basket.
        */
        //@{
        Real expectedTrancheLoss(const Date& d) const;
        /*! The lossFraction is the fraction of losses expressed in 
            inception (no losses) tranche units (e.g. 'attach level'=0%, 
            'detach level'=100%)
        */
        Probability probOverLoss(const Date& d, Real lossFraction) const;
        /*! 
        */
        Real percentile(const Date& d, Probability prob) const;
        /*! ESF 
        */
        Real expectedShortfall(const Date& d, Probability prob) const;
        /* Split a portfolio loss along counterparties. Typically loss 
        corresponds to some percentile.*/
        Disposable<std::vector<Real> > 
            splitVaRLevel(const Date& date, Real loss) const;
        /*! Full loss distribution
        */
        Disposable<std::map<Real, Probability> > lossDistribution(
            const Date&) const;
        Real densityTrancheLoss(const Date& d, Real lossFraction) const;
        Real defaultCorrelation(const Date& d, Size iName, Size jName) const;
        /*! Probability vector that each of the remaining live names (at eval
          date) is the n-th default by date d.

          The n parameter is the internal index to the name; it should
          be alive at the evaluation date.

        ---------TO DO: Implement with a string passed----------------------
        ---------TO DO: Perform check the name is alive---------------------
        */
        std::vector<Probability> probsBeingNthEvent(
            Size n, const Date& d) const;
        /*! Returns the probaility of having a given or larger number of 
        defaults in the basket portfolio at a given time.
        */
        Probability probAtLeastNEvents(Size n, const Date& d) const;
        /*! Expected recovery rate of the underlying position as a fraction of 
          its exposure value at date d _given_ it has defaulted _on_ that date.
          NOTICE THE ARG IS THE CTPTY....SHOULDNT IT BE THE POSITION/INSTRUMENT?????<<<<<<<<<<<<<<<<<<<<<<<
        */
        Real recoveryRate(const Date& d, Size iName) const;
        //@}
      private:
        // LazyObject interface
        void performCalculations() const override;

        std::vector<Real> notionals_;
        ext::shared_ptr<Pool> pool_;
        //! The claim is the same for all names
        const ext::shared_ptr<Claim> claim_;

        Real attachmentRatio_;
        Real detachmentRatio_;
        Real basketNotional_;
        //! basket tranched inception attachment amount:
        mutable Real attachmentAmount_;
        //! basket tranched inception detachment amount:
        mutable Real detachmentAmount_;
        //! basket tranched notional amount:
        mutable Real trancheNotional_;
        /* Caches. Most of the times one wants statistics on the distribution of
        futures losses at arbitrary dates but some problems (e.g. derivatives 
        pricing) work with todays (evalDate) magnitudes which do not require a 
        loss model and would be too expensive to recompute on every call.
        */
        mutable Real evalDateSettledLoss_,
            evalDateRemainingNot_,
            evalDateAttachAmount_,
            evalDateDetachAmmount_;
        mutable std::vector<Size> evalDateLiveList_;
        mutable std::vector<Real> evalDateLiveNotionals_;
        mutable std::vector<std::string> evalDateLiveNames_;
        mutable std::vector<DefaultProbKey> evalDateLiveKeys_;
        //! Basket inception date.
        const Date refDate_;
        /* It is the basket responsibility to ensure that the model assigned it 
          is properly initialized to the basket current data. 
          This might not be the case for various reasons: the basket data might
          have been updated, the evaluation date has changed or the model has 
          received another request from another basket pointing to it. For
          this last reason we can never be sure between calls that this is the 
          case (and that is true in a single thread environment only).
        */
        ext::shared_ptr<DefaultLossModel> lossModel_;
    };

    // ------------ Inlines -------------------------------------------------

    inline Size Basket::size() const {
        return pool_->size();
    }

    inline const std::vector<Real>& Basket::notionals() const {
        return notionals_;
    }

    inline Disposable<std::vector<DefaultProbKey> > 
        Basket::defaultKeys() const {
        return pool_->defaultKeys();
    }

    inline const ext::shared_ptr<Pool>& Basket::pool() const {
        return pool_;
    }

    inline const std::vector<Size>& Basket::liveList() const {
        return evalDateLiveList_;
    }

    inline Real Basket::remainingDetachmentAmount() const {
        return evalDateDetachAmmount_;
    }

    inline Real Basket::remainingAttachmentAmount() const {
        return evalDateAttachAmount_;
    }

    inline const std::vector<std::string>& Basket::remainingNames() const {
        return evalDateLiveNames_;
    }

    inline const std::vector<Real>& Basket::remainingNotionals() const {
        return evalDateLiveNotionals_;
    }

    inline Real Basket::cumulatedLoss() const {
        return this->evalDateSettledLoss_;
    }
    
    inline Real Basket::settledLoss() const {
        return evalDateSettledLoss_;
    }

    inline const std::vector<DefaultProbKey>& 
        Basket::remainingDefaultKeys() const 
    {
        return evalDateLiveKeys_;
    }

}


#endif
]]></document_content>
  </document>
  <document index="5">
    <source>binomiallossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_binomial_loss_model_hpp
#define quantlib_binomial_loss_model_hpp

#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/functional.hpp>
#include <ql/handle.hpp>
#include <algorithm>
#include <numeric>
#include <utility>

namespace QuantLib {

    /*! Binomial Defaultable Basket Loss Model\par
    Models the portfolio loss distribution by approximatting it to an adjusted 
    binomial. Fits the two moments of the loss distribution through an adapted 
    binomial approximation. This simple model allows for portfolio inhomogeneity
    with no excesive cost over the LHP.\par
    See:\par
    <b>Approximating Independent Loss Distributions with an Adjusted Binomial 
    Distribution</b> , Dominic O'Kane, 2007 EDHEC RISK AND ASSET MANAGEMENT 
    RESEARCH CENTRE \par
    <b>Modelling single name and multi-name credit derivatives</b> Chapter 
    18.5.2, Dominic O'Kane, Wiley Finance, 2008 \par
    The version presented here is adaptated to the multifactorial case
    by computing a conditional binomial approximation; notice that the Binomial
    is stable. This way the model can be used also in risk management models
    rather than only in pricing. The copula is also left 
    undefined/arbitrary. \par
    LLM: Loss Latent Model template parameter able to model default and 
    loss.\par
    The model is allowed and arbitrary copula, although initially designed for
    a Gaussian setup. If these exotic versions were not allowed the template 
    parameter can then be dropped but the use of random recoveries should be
    added in some other way.

    \todo untested/wip for the random recovery models.
    \todo integrate with the previously computed probability inversions of
    the cumulative functions.
    */
    template<class LLM>
    class BinomialLossModel : public DefaultLossModel {
    public:
        typedef typename LLM::copulaType copulaType;
        explicit BinomialLossModel(ext::shared_ptr<LLM> copula) : copula_(std::move(copula)) {}

      private:
        void resetModel() override {
            /* say there are defaults and these havent settled... and this is
            the engine to compute them.... is this the wrong place?:*/
            attachAmount_ = basket_->remainingAttachmentAmount();
            detachAmount_ = basket_->remainingDetachmentAmount();

            copula_->resetBasket(basket_.currentLink()); // forces interface
      }

    protected:
        /*! Returns the probability of the default loss values given by the 
            method lossPoints.
        */
        Disposable<std::vector<Real> > 
            expectedDistribution(const Date& date) const {
            // precal date conditional magnitudes:
            std::vector<Real> notionals = basket_->remainingNotionals(date);
            std::vector<Probability> invProbs = 
                basket_->remainingProbabilities(date);
            for(Size iName=0; iName<invProbs.size(); iName++)
                invProbs[iName] = 
                    copula_->inverseCumulativeY(invProbs[iName], iName);

            return copula_->integratedExpectedValueV(
                [&](const std::vector<Real>& v1) {
                    return lossProbability(date, notionals, invProbs, v1);
                });
        }
        //! attainable loss points this model provides
        Disposable<std::vector<Real> > lossPoints(const Date&) const;
        //! Returns the cumulative full loss distribution
        Disposable<std::map<Real, Probability> > lossDistribution(const Date& d) const override;
        //! Loss level for this percentile
        Real percentile(const Date& d, Real percentile) const override;
        Real expectedShortfall(const Date& d, Real percentile) const override;
        Real expectedTrancheLoss(const Date& d) const override;

        // Model internal workings ----------------
        //! Average loss per credit.
        Real averageLoss(const Date&, const std::vector<Real>& reminingNots, 
            const std::vector<Real>&) const;
        Real condTrancheLoss(const Date&, const std::vector<Real>& lossVals, 
            const std::vector<Real>& bsktNots,
            const std::vector<Probability>& uncondDefProbs, 
            const std::vector<Real>&) const;
        // expected as in time-value, not average, see literature
        Disposable<std::vector<Real> >
            expConditionalLgd(const Date& d,
                               const std::vector<Real>& mktFactors) const
        {
            std::vector<Real> condLgds;
            const std::vector<Size>& evalDateLives = basket_->liveList();
            condLgds.reserve(evalDateLives.size());
            for (unsigned long evalDateLive : evalDateLives)
                condLgds.push_back(1. - copula_->conditionalRecovery(d, evalDateLive, mktFactors));
            return condLgds;
        }

        //! Loss probability density conditional on the market factor value.
        // Heres where the burden of the algorithm setup lies.
        Disposable<std::vector<Real> > 
            lossProbability(      
                const Date& date,
                // expected exposures at the passed date, no wrong way means
                //  no dependence of the exposure with the mkt factor 
                const std::vector<Real>& bsktNots,
                const std::vector<Real>& uncondDefProbInv, 
                            const std::vector<Real>&  mktFactor) const;

        const ext::shared_ptr<LLM> copula_;

        // cached arguments:
        // remaining basket magnitudes:
        mutable Real attachAmount_, detachAmount_;
    };

    //-------------------------------------------------------------------------

    /* The algorithm to compute the prob. of n defaults in the basket is 
        recursive. For this reason theres no sense in returning the prob 
        distribution of a given number of defaults.
    */
    template< class LLM>
    Disposable<std::vector<Real> > BinomialLossModel<LLM>::lossProbability(
        const Date& date, 
        const std::vector<Real>& bsktNots,
        const std::vector<Real>& uncondDefProbInv, 
        const std::vector<Real>& mktFactors) const 
    {   // the model as it is does not model the exposures conditional to the 
        //   mkt factr, otherwise this needs revision
        /// model does not take the unconditional rr
        Size bsktSize = basket_->remainingSize();
        /* The conditional loss per unit notional of each name at time 'date'
            The spot recovery model is returning for all i's:
            \frac{\int_0^t  [1-rr_i(\tau; \xi)] P_{def-i}(0, \tau; \xi) d\tau}
                 {P_{def-i}(0,t;\xi)}
            and the constant recovery model is simply returning: 
            1-RR_i
        */
        // conditional fractional LGD expected as given by the recovery model 
        //   for the ramaining(live) names at the current eval date.
        std::vector<Real> fractionalEL = expConditionalLgd(date, mktFactors);
        std::vector<Real> lgdsLeft;
        std::transform(fractionalEL.begin(), fractionalEL.end(), 
            bsktNots.begin(), std::back_inserter(lgdsLeft), 
            std::multiplies<Real>());
        Real avgLgd = 
            std::accumulate(lgdsLeft.begin(), lgdsLeft.end(), Real(0.)) /
                bsktSize;

        std::vector<Probability> condDefProb(bsktSize, 0.);
        for(Size j=0; j<bsktSize; j++)//transform
            condDefProb[j] = 
                copula_->conditionalDefaultProbabilityInvP(uncondDefProbInv[j],
                    j, mktFactors);
        // of full portfolio:
        Real avgProb = avgLgd <= QL_EPSILON ? 0. : // only if all are 0
                std::inner_product(condDefProb.begin(), 
                    condDefProb.end(), lgdsLeft.begin(), 0.)
                / (avgLgd * bsktSize);
        // model parameters:
        Real m = avgProb * bsktSize;
        Real floorAveProb = std::min(Real(bsktSize-1), std::floor(Real(m)));
        Real ceilAveProb = floorAveProb + 1.;
        // nu_A
        Real varianceBinom = avgProb * (1. - avgProb)/bsktSize;
        // nu_E
        std::vector<Probability> oneMinusDefProb;//: 1.-condDefProb[j]
        std::transform(condDefProb.begin(), condDefProb.end(), 
                       std::back_inserter(oneMinusDefProb), 
                       subtract_from<Real>(1.0));

        //breaks condDefProb and lgdsLeft to spare memory
        std::transform(condDefProb.begin(), condDefProb.end(), 
            oneMinusDefProb.begin(), condDefProb.begin(), 
            std::multiplies<Real>());
        std::transform(lgdsLeft.begin(), lgdsLeft.end(), 
            lgdsLeft.begin(), lgdsLeft.begin(), std::multiplies<Real>());
        Real variance = std::inner_product(condDefProb.begin(), 
            condDefProb.end(), lgdsLeft.begin(), 0.);

        variance = avgLgd <= QL_EPSILON ? 0. : 
            variance / (bsktSize * bsktSize * avgLgd * avgLgd );
        Real sumAves = -std::pow(ceilAveProb-m, 2) 
            - (std::pow(floorAveProb-m, 2) - std::pow(ceilAveProb,2.)) 
                * (ceilAveProb-m);
        Real alpha = (variance * bsktSize + sumAves) 
            / (varianceBinom * bsktSize + sumAves);
        // Full distribution: 
        // ....DO SOMETHING CHEAPER at least go up to the loss tranche limit.
        std::vector<Probability> lossProbDensity(bsktSize+1, 0.); 
        if(avgProb >= 1.-QL_EPSILON) {
           lossProbDensity[bsktSize] = 1.;
        }else if(avgProb <= QL_EPSILON) {
           lossProbDensity[0] = 1.;
        }else{
            /* FIX ME: With high default probabilities one only gets tiny values
            at the end and the sum of probabilities in the 
            conditional distribution does not add up to one. It might be due to 
            the fact that recursion should be done in the other direction as 
            pointed out in the book. This is numerical.
            */
            Probability probsRatio = avgProb/(1.-avgProb);
            lossProbDensity[0] = std::pow(1.-avgProb, 
                static_cast<Real>(bsktSize));
            for(Size i=1; i<bsktSize+1; i++) // recursive to avoid factorial
                lossProbDensity[i] = lossProbDensity[i-1] * probsRatio 
                    * (bsktSize-i+1.)/i;
            // redistribute probability:
            for(Size i=0; i<bsktSize+1; i++)
                lossProbDensity[i] *= alpha;
            // adjust average
            Real epsilon = (1.-alpha)*(ceilAveProb-m);
            Real epsilonPlus = 1.-alpha-epsilon;
            lossProbDensity[static_cast<Size>(floorAveProb)] += epsilon;
            lossProbDensity[static_cast<Size>(ceilAveProb)]  += epsilonPlus;
        }
        return lossProbDensity;
    }

    //-------------------------------------------------------------------------

    template< class LLM>
    Real BinomialLossModel<LLM>::averageLoss(
        const Date& d, 
        const std::vector<Real>& reminingNots,
        const std::vector<Real>& mktFctrs) const 
    {
        Size bsktSize = basket_->remainingSize();
        /* The conditional loss per unit notional of each name at time 'date'
            The spot recovery model is returning for all i's:
            \frac{\int_0^t  [1-rr_i(\tau; \xi)] P_{def-i}(0, \tau; \xi) d\tau}
                 {P_{def-i}(0,t;\xi)}
            and the constant recovery model is simply returning: 
            1-RR_i
        */
        std::vector<Real> fractionalEL = expConditionalLgd(d, mktFctrs);
        Real notBskt = std::accumulate(reminingNots.begin(), 
                                       reminingNots.end(), Real(0.));
        std::vector<Real> lgdsLeft;
        std::transform(fractionalEL.begin(), fractionalEL.end(), 
                       reminingNots.begin(), std::back_inserter(lgdsLeft),
                       std::multiplies<Real>());
        return std::accumulate(lgdsLeft.begin(), lgdsLeft.end(), Real(0.)) 
            / (bsktSize*notBskt);
    }

    template< class LLM>
    Disposable<std::vector<Real> >
        BinomialLossModel<LLM>::lossPoints(const Date& d) const 
    {
        std::vector<Real> notionals = basket_->remainingNotionals(d);

        Real aveLossFrct = copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return averageLoss(d, notionals, v1);
            });

        std::vector<Real> data;
        Size dataSize = basket_->remainingSize() + 1;
        data.reserve(dataSize);
        // use std::algorithm
        Real outsNot = basket_->remainingNotional(d);
        for(Size i=0; i<dataSize; i++)
            data.push_back(i * aveLossFrct * outsNot);
        return data;
    }

    template< class LLM>
    Real BinomialLossModel<LLM>::condTrancheLoss(
        const Date& d, 
        const std::vector<Real>& lossVals, 
        const std::vector<Real>& bsktNots,
        const std::vector<Real>& uncondDefProbsInv,
        const std::vector<Real>& mkf) const {

        std::vector<Real> condLProb = 
            lossProbability(d, bsktNots, uncondDefProbsInv, mkf);
        // \to do: move to a do-while over attach to detach
        Real suma = 0.;
        for(Size i=0; i<lossVals.size(); i++) { 
            suma += condLProb[i] * 
                std::min(std::max(lossVals[i]
                 - attachAmount_, 0.), detachAmount_ - attachAmount_);
        }
        return suma;
    }

    template< class LLM>
    Real BinomialLossModel<LLM>::expectedTrancheLoss(const Date& d) const {
        std::vector<Real> lossVals  = lossPoints(d);
        std::vector<Real> notionals = basket_->remainingNotionals(d);
        std::vector<Probability> invProbs = 
            basket_->remainingProbabilities(d);
        for(Size iName=0; iName<invProbs.size(); iName++)
            invProbs[iName] = 
                copula_->inverseCumulativeY(invProbs[iName], iName);
            
        return copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return condTrancheLoss(d, lossVals, notionals, invProbs, v1);
            });
    }


    template< class LLM>
    Disposable<std::map<Real, Probability> > 
        BinomialLossModel<LLM>::lossDistribution(const Date& d) const 
    {
        std::map<Real, Probability> distrib;
        std::vector<Real> lossPts = lossPoints(d);
        std::vector<Real> values  = expectedDistribution(d);
        Real sum = 0.;
        for(Size i=0; i<lossPts.size(); i++) {
            distrib.insert(std::make_pair(lossPts[i], 
                //capped, some situations giving a very small probability over 1
                std::min(sum+values[i],1.)
                ));
            sum+= values[i];
        }
        return distrib;
    }

    template< class LLM>
    Real BinomialLossModel<LLM>::percentile(const Date& d, Real perc) const {
        std::map<Real, Probability> dist = lossDistribution(d);
        // \todo: Use some of the library interpolators instead
        if(// included in test below-> (dist.begin()->second >=1.) ||
            (dist.begin()->second >= perc))return dist.begin()->first;

        // deterministic case (e.g. date requested is todays date)
        if(dist.size() == 1) return dist.begin()->first;

        if(perc == 1.) return dist.rbegin()->first;
        if(perc == 0.) return dist.begin()->first;
        std::map<Real, Probability>::const_iterator itdist = dist.begin();
        while (itdist->second <= perc) ++itdist;
        Real valPlus = itdist->second;
        Real xPlus   = itdist->first;
        --itdist; //we're never 1st or last, because of tests above
        Real valMin  = itdist->second;
        Real xMin    = itdist->first;

        Real portfLoss = xPlus-(xPlus-xMin)*(valPlus-perc)/(valPlus-valMin);

        return 
            std::min(std::max(portfLoss - attachAmount_, 0.), 
                detachAmount_ - attachAmount_);
    }

    template< class LLM>
    Real BinomialLossModel<LLM>::expectedShortfall(const Date&d, 
        Real perctl) const 
    {
        //taken from recursive since we have the distribution in both cases.
        if(d == Settings::instance().evaluationDate()) return 0.;
            std::map<Real, Probability> distrib = lossDistribution(d);

            std::map<Real, Probability>::iterator 
                itNxt, itDist = distrib.begin();
            for(; itDist != distrib.end(); ++itDist)
                if(itDist->second >= perctl) break;
            itNxt = itDist;
            --itDist;

            // \todo: I could linearly triangulate the exact point and get 
            //    extra precission on the first(broken) period.
            if(itNxt != distrib.end()) { 
                Real lossNxt = std::min(std::max(itNxt->first - attachAmount_, 
                    0.), detachAmount_ - attachAmount_);
                Real lossHere = std::min(std::max(itDist->first - attachAmount_,
                    0.), detachAmount_ - attachAmount_);

                Real val =  lossNxt - (itNxt->second - perctl) * 
                    (lossNxt - lossHere) / (itNxt->second - itDist->second); 
                Real suma = (itNxt->second - perctl) * (lossNxt + val) * .5;
                ++itDist; ++itNxt;
                do{
                    lossNxt = std::min(std::max(itNxt->first - attachAmount_, 
                        0.), detachAmount_ - attachAmount_);
                    lossHere = std::min(std::max(itDist->first - attachAmount_, 
                        0.), detachAmount_ - attachAmount_);
                    suma += .5 * (lossHere + lossNxt) 
                        * (itNxt->second - itDist->second);
                    ++itDist; ++itNxt;
                }while(itNxt != distrib.end());
                return suma / (1.-perctl);
            }
            QL_FAIL("Binomial model fails to calculate ESF.");
    }

    // The standard use:
    typedef BinomialLossModel<GaussianConstantLossLM> GaussianBinomialLossModel;
    typedef BinomialLossModel<TConstantLossLM> TBinomialLossModel;

}

#endif
]]></document_content>
  </document>
  <document index="6">
    <source>blackcdsoptionengine.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Stamm
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file blackcdsoptionengine.hpp
    \brief Black credit default swap option engine
*/

#ifndef quantlib_black_cds_option_engine_hpp
#define quantlib_black_cds_option_engine_hpp

#include <ql/experimental/credit/cdsoption.hpp>

namespace QuantLib {

    //! Black-formula CDS-option engine
    /*! \warning The engine assumes that the exercise date equals the
                 start date of the passed CDS.
    */
    class BlackCdsOptionEngine : public CdsOption::engine {
      public:
        BlackCdsOptionEngine(Handle<DefaultProbabilityTermStructure>,
                             Real recoveryRate,
                             Handle<YieldTermStructure> termStructure,
                             Handle<Quote> vol);
        void calculate() const override;
        Handle<YieldTermStructure> termStructure();
        Handle<Quote> volatility();
      private:
        Handle<DefaultProbabilityTermStructure> probability_;
        Real recoveryRate_;
        Handle<YieldTermStructure> termStructure_;
        Handle<Quote> volatility_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="7">
    <source>cdo.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file cdo.hpp
    \brief collateralized debt obligation
*/

#ifndef quantlib_cdo_hpp
#define quantlib_cdo_hpp

#include <ql/instrument.hpp>
#include <ql/termstructures/yieldtermstructure.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <ql/experimental/credit/lossdistribution.hpp>
#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/time/schedule.hpp>

namespace QuantLib {

    //! collateralized debt obligation
    /*! The instrument prices a mezzanine CDO tranche with loss given
        default between attachment point \f$ D_1\f$ and detachment
        point \f$ D_2 > D_1 \f$.

        For purchased protection, the instrument value is given by the
        difference of the protection value \f$ V_1 \f$ and premium
        value \f$ V_2 \f$,

        \f[ V = V_1 - V_2. \f]

        The protection leg is priced as follows:

        - Build the probability distribution for volume of defaults
          \f$ L \f$ (before recovery) or Loss Given Default \f$ LGD =
          (1-r)\,L \f$ at times/dates \f$ t_i, i=1, ..., N\f$ (premium
          schedule times with intermediate steps)
        - Determine the expected value
          \f$ E_i = E_{t_i}\,\left[Pay(LGD)\right] \f$
          of the protection payoff \f$ Pay(LGD) \f$ at each time
          \f$ t_i\f$ where
         \f[
         Pay(L) = min (D_1, LGD) - min (D_2, LGD) = \left\{
         \begin{array}{lcl}
         \displaystyle 0 &;& LGD < D_1 \\
         \displaystyle LGD - D_1 &;& D_1 \leq LGD \leq D_2 \\
         \displaystyle D_2 - D_1 &;& LGD > D_2
         \end{array}
         \right.
         \f]
        - The protection value is then calculated as
          \f[ V_1 \:=\: \sum_{i=1}^N (E_i - E_{i-1}) \cdot  d_i \f]
          where \f$ d_i\f$ is the discount factor at time/date \f$ t_i \f$

        The premium is paid on the protected notional amount,
        initially \f$ D_2 - D_1. \f$ This notional amount is reduced
        by the expected protection payments \f$ E_i \f$ at times
        \f$ t_i, \f$ so that the premium value is calculated as

        \f[
        V_2 = m \, \cdot \sum_{i=1}^N \,(D_2 - D_1 - E_i)
                   \cdot \Delta_{i-1,i}\,d_i
        \f]

        where \f$ m \f$ is the premium rate, \f$ \Delta_{i-1, i}\f$ is
        the day count fraction between date/time \f$ t_{i-1}\f$ and
        \f$ t_i.\f$

        The construction of the portfolio loss distribution \f$ E_i
        \f$ is based on the probability bucketing algorithm described
        in

        <strong>
        John Hull and Alan White, "Valuation of a CDO and nth to default CDS
        without Monte Carlo simulation", Journal of Derivatives 12, 2, 2004
        </strong>

        The pricing algorithm allows for varying notional amounts and
        default termstructures of the underlyings.

        \todo Investigate and fix cases \f$ E_{i+1} < E_i. \f$
    */
    class CDO : public Instrument {
      public:
        /*! \param attachment  fraction of the LGD where protection starts
            \param detachment  fraction of the LGD where protection ends
            \param nominals    vector of basket nominal amounts
            \param basket      default basket represented by a vector of
                               default term structures that allow
                               computing single name default
                               probabilities depending on time
            \param copula      one-factor copula
            \param protectionSeller   sold protection if set to true, purchased
                                      otherwise
            \param premiumSchedule    schedule for premium payments
            \param premiumRate        annual premium rate, e.g. 0.05 for 5% p.a.
            \param dayCounter         day count convention for the premium rate
            \param recoveryRate       recovery rate as a fraction
            \param upfrontPremiumRate premium as a tranche notional fraction
            \param yieldTS            yield term structure handle
            \param nBuckets           number of distribution buckets
            \param integrationStep    time step for integrating over one
                                      premium period; if larger than premium
                                      period length, a single step is taken
        */
        CDO(Real attachment,
            Real detachment,
            std::vector<Real> nominals,
            const std::vector<Handle<DefaultProbabilityTermStructure> >& basket,
            Handle<OneFactorCopula> copula,
            bool protectionSeller,
            Schedule premiumSchedule,
            Rate premiumRate,
            DayCounter dayCounter,
            Rate recoveryRate,
            Rate upfrontPremiumRate,
            Handle<YieldTermStructure> yieldTS,
            Size nBuckets,
            const Period& integrationStep = Period(10, Years));

        Real nominal() const { return nominal_; }
        Real lgd() const { return lgd_; }
        Real attachment() const { return attachment_; }
        Real detachment() const { return detachment_; }
        std::vector<Real> nominals() { return nominals_; }
        Size size() { return basket_.size(); }

        bool isExpired() const override;
        Rate fairPremium() const;
        Rate premiumValue () const;
        Rate protectionValue () const;
        Size error () const;

      private:
        void setupExpired() const override;
        void performCalculations() const override;
        Real expectedTrancheLoss (Date d) const;

        Real attachment_;
        Real detachment_;
        std::vector<Real> nominals_;
        std::vector<Handle<DefaultProbabilityTermStructure> > basket_;
        Handle<OneFactorCopula> copula_;
        bool protectionSeller_;

        Schedule premiumSchedule_;
        Rate premiumRate_;
        DayCounter dayCounter_;
        Rate recoveryRate_;
        Rate upfrontPremiumRate_;
        Handle<YieldTermStructure> yieldTS_;
        Size nBuckets_; // number of buckets up to detachment point
        Period integrationStep_;

        std::vector<Real> lgds_;

        Real nominal_;  // total basket volume (sum of nominals_)
        Real lgd_;      // maximum loss given default (sum of lgds_)
        Real xMax_;     // tranche detachment point (tranche_ * nominal_)
        Real xMin_;     // tranche attachment point (tranche_ * nominal_)

        mutable Size error_;

        mutable Real premiumValue_;
        mutable Real protectionValue_;
        mutable Real upfrontPremiumValue_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="8">
    <source>cdsoption.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Stamm
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file cdsoption.hpp
    \brief CDS option
*/

#ifndef quantlib_cds_option_hpp
#define quantlib_cds_option_hpp

#include <ql/option.hpp>
#include <ql/instruments/creditdefaultswap.hpp>

namespace QuantLib {

    class Quote;
    class YieldTermStructure;

    //! CDS option
    /*! The side of the swaption is set by choosing the side of the CDS.
        A receiver CDS option is a right to buy an underlying CDS
        selling protection and receiving a coupon. A payer CDS option
        is a right to buy an underlying CDS buying protection and
        paying coupon.
    */
    class CdsOption : public Option {
      public:
        class arguments;
        class results;
        class engine;
        CdsOption(const ext::shared_ptr<CreditDefaultSwap>& swap,
                  const ext::shared_ptr<Exercise>& exercise,
                  bool knocksOut = true);

        //! \name Instrument interface
        //@{
        bool isExpired() const override;
        void setupArguments(PricingEngine::arguments*) const override;
        //@}
        //! \name Inspectors
        //@{
        const ext::shared_ptr<CreditDefaultSwap>& underlyingSwap() const {
            return swap_;
        }
        //@}
        //! \name Calculations
        //@{
        Rate atmRate() const;
        Real riskyAnnuity() const;
        Volatility impliedVolatility(
                              Real price,
                              const Handle<YieldTermStructure>& termStructure,
                              const Handle<DefaultProbabilityTermStructure>&,
                              Real recoveryRate,
                              Real accuracy = 1.e-4,
                              Size maxEvaluations = 100,
                              Volatility minVol = 1.0e-7,
                              Volatility maxVol = 4.0) const;
        //@}

    private:
        ext::shared_ptr<CreditDefaultSwap> swap_;
        bool knocksOut_;

        mutable Real riskyAnnuity_;
        void setupExpired() const override;
        void fetchResults(const PricingEngine::results*) const override;
    };


    //! %Arguments for CDS-option calculation
    class CdsOption::arguments : public CreditDefaultSwap::arguments,
                                 public Option::arguments {
      public:
        arguments() = default;

        ext::shared_ptr<CreditDefaultSwap> swap;
        bool knocksOut;
        void validate() const override;
    };

    //! %Results from CDS-option calculation
    class CdsOption::results : public Option::results {
      public:
        Real riskyAnnuity;
        void reset() override;
    };

    //! base class for swaption engines
    class CdsOption::engine
        : public GenericEngine<CdsOption::arguments, CdsOption::results> {};

}

#endif
]]></document_content>
  </document>
  <document index="9">
    <source>constantlosslatentmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/


#ifndef quantlib_constantloss_latentmodel_hpp
#define quantlib_constantloss_latentmodel_hpp

#include <ql/experimental/credit/defaultprobabilitylatentmodel.hpp>
// take the loss model to a different file and avoid this inclusion
#include <ql/experimental/credit/defaultlossmodel.hpp>

namespace QuantLib {

    /*! Constant deterministic loss amount default latent model. Integrable 
        implementation.
    */
    /* \todo: 
        Several options: Refer to a set of individual RR models, have quotes to
        RRs (registered)
    */
    template <class copulaPolicy>
    class ConstantLossLatentmodel : public DefaultLatentModel<copulaPolicy> {
    private:
        const std::vector<Real> recoveries_;
        typedef typename copulaPolicy::initTraits initTraits;
    public:
        ConstantLossLatentmodel(
            const std::vector<std::vector<Real> >& factorWeights,
            const std::vector<Real>& recoveries,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            const initTraits& ini = initTraits()            
            ) 
        : DefaultLatentModel<copulaPolicy>(factorWeights, integralType, ini),
          recoveries_(recoveries) {

              QL_REQUIRE(recoveries.size() == factorWeights.size(), 
                "Incompatible factors and recovery sizes.");
        }

        ConstantLossLatentmodel(
            const Handle<Quote>& mktCorrel,
            const std::vector<Real>& recoveries,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            Size nVariables,
            const initTraits& ini = initTraits()            
            ) 
        : DefaultLatentModel<copulaPolicy>(mktCorrel, nVariables,
                                           integralType, ini),
          recoveries_(recoveries) {
            // actually one could define the other and get rid of the variable 
            // here and in other similar models
            QL_REQUIRE(recoveries.size() == nVariables, 
                "Incompatible model and recovery sizes.");
        }

        Real conditionalRecovery(const Date& d, Size iName, 
                                 const std::vector<Real>& mktFactors) const {
            return recoveries_[iName];
        }

        Real conditionalRecovery(Probability uncondDefP, Size iName, 
                                 const std::vector<Real>& mktFactors) const {
            return recoveries_[iName];
        }

        Real conditionalRecoveryInvP(Real invUncondDefP, Size iName, 
                                 const std::vector<Real>& mktFactors) const {
            return recoveries_[iName];
        }

        Real conditionalRecovery(Real latentVarSample, 
            Size iName, const Date& d) const {
            return recoveries_[iName];
        }

        const std::vector<Real>& recoveries() const {
            return recoveries_;
        }

        // this is really an interface to rr models even if not imposed. Default
        // loss models do have an interface for this one. Enforced only through
        // duck typing.
        Real expectedRecovery(const Date& d, Size iName, 
            const DefaultProbKey& defKeys) const {
            return recoveries_[iName];
        }
    };

    typedef ConstantLossLatentmodel<GaussianCopulaPolicy> 
        GaussianConstantLossLM;
    typedef ConstantLossLatentmodel<TCopulaPolicy> TConstantLossLM;


    /*! ConstantLossLatentModel interface for loss models. 
    While it does not provide distribution type losses (e.g. expected tranche 
    losses) because it lacks an integration algorithm it serves to allow 
    pricing of digital type products like NTDs.

    Alternatively fuse with the aboves class.
    */
    template <class copulaPolicy>
    class ConstantLossModel : 
        public virtual ConstantLossLatentmodel<copulaPolicy>, 
        public virtual DefaultLossModel 
    {
    public:
        ConstantLossModel(
            const std::vector<std::vector<Real> >& factorWeights,
            const std::vector<Real>& recoveries,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            const typename copulaPolicy::initTraits& ini = 
                copulaPolicy::initTraits()) 
        : ConstantLossLatentmodel<copulaPolicy>(factorWeights, recoveries, 
            integralType, ini) {}

        ConstantLossModel(
            const Handle<Quote>& mktCorrel,
            const std::vector<Real>& recoveries,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            Size nVariables,
            const typename copulaPolicy::initTraits& ini = 
                copulaPolicy::initTraits()) 
        : ConstantLossLatentmodel<copulaPolicy>(mktCorrel, recoveries, 
            integralType, nVariables,ini) {}

    protected:
        //Disposable<std::vector<Probability> > probsBeingNthEvent(
        //    Size n, const Date& d) const {
        //    return 
        //      ConstantLossLatentmodel<copulaPolicy>::probsBeingNthEvent(n, d);
        //}
      Real defaultCorrelation(const Date& d, Size iName, Size jName) const override {
          return ConstantLossLatentmodel<copulaPolicy>::defaultCorrelation(d, iName, jName);
      }
      Probability probAtLeastNEvents(Size n, const Date& d) const override {
          return ConstantLossLatentmodel<copulaPolicy>::probAtLeastNEvents(n, d);
      }
      Real expectedRecovery(const Date& d, Size iName, const DefaultProbKey& k) const override {
          return ConstantLossLatentmodel<copulaPolicy>::expectedRecovery(d, iName, k);
      }

    private:
      void resetModel() override {
          // update the default latent model we derive from
          DefaultLatentModel<copulaPolicy>::resetBasket(
              DefaultLossModel::basket_.currentLink()); // forces interface
      }
    };

}

#endif
]]></document_content>
  </document>
  <document index="10">
    <source>correlationstructure.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_correl_term_structure_hpp
#define quantlib_correl_term_structure_hpp

#include <ql/termstructure.hpp>

namespace QuantLib {

    // pretty much like the volatility TS, here the correlation range is 
    // obviously known in advance and theres no reference to a strike.

    /*! Abstract interface, derived correlations TS might have elements with 
    arbitrary dimensions.\par
    In principle there might be several extrapolation dimensions, at this 
    level we do not know how many or the nature of those dimensions (time, 
    strike...) 
    Equally we ignore at this level if the correlation is a number, 
    matrix. Rather than including an arbitrary size matrix this data
    structure is deferred in the hierarchy to enable potential optimizations
    on the data nature.
    */
    class CorrelationTermStructure : public TermStructure {
    public:
        /*! \name Constructors
            See the TermStructure documentation for issues regarding
            constructors.
        */
        //@{
        //! default constructor
        /*! \warning term structures initialized by means of this
                     constructor must manage their own reference date
                     by overriding the referenceDate() method.
        */
        CorrelationTermStructure(const Calendar& cal,
                                 BusinessDayConvention bdc,
                                 const DayCounter& dc = DayCounter());
        //! initialize with a fixed reference date
        CorrelationTermStructure(const Date& referenceDate,
                                 const Calendar& cal,
                                 BusinessDayConvention bdc,
                                 const DayCounter& dc = DayCounter());
        //! calculate the reference date based on the global evaluation date
        CorrelationTermStructure(Natural settlementDays,
                                 const Calendar& cal,
                                 BusinessDayConvention bdc,
                                 const DayCounter& dc = DayCounter());
        //@}
        BusinessDayConvention businessDayConvention() const;
        //! period/date conversion
        Date dateFromTenor(const Period&) const;
        //! The size of the squared correlation.
        virtual Size correlationSize() const = 0;
    private:
        BusinessDayConvention bdc_;
    };

    // inline definitions
    inline BusinessDayConvention
    CorrelationTermStructure::businessDayConvention() const {
        return bdc_;
    }

    inline Date
    CorrelationTermStructure::dateFromTenor(const Period& p) const {
        // swaption style, still holds here.
        return calendar().advance(referenceDate(),
                                  p,
                                  businessDayConvention());
    }
}

#endif
]]></document_content>
  </document>
  <document index="11">
    <source>defaultevent.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 StatPro Italia srl
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file defaultevent.hpp
    \brief Classes for default-event description.
*/

#ifndef quantlib_default_event_hpp
#define quantlib_default_event_hpp

#include <ql/event.hpp>
#include <ql/currency.hpp>
#include <ql/math/comparison.hpp>
#include <ql/experimental/credit/defaulttype.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <map>

namespace QuantLib {

    /**
    @class DefaultEvent
    @brief Credit event on a bond of a certain seniority(ies)/currency

      Represents a credit event affecting all bonds with a given \
      seniority and currency. It assumes that all such bonds suffer \
      the event simultaneously.
      Some events affect all seniorities and this has to be encoded
      through a different set of events of the same event type.
      The event is an actual realization, not a contractual reference,
      as such it contains only an atomic type.
    */
    class DefaultEvent : public Event {
      public:
        class DefaultSettlement : public Event {
          public:
            friend class DefaultEvent;
          protected:
            /*! Default settlement events encode the settlement date
                and the recovery rates for the affected
                seniorities. Specific events might require different
                sets of recoveries to be present. The way these
                objects are constructed is a prerogative of the
                particular event class.
            */
            DefaultSettlement(const Date& date,
                              const std::map<Seniority, Real>& recoveryRates);
            /*! When NoSeniority is passed all seniorities are assumed
                to have settled to the recovery passed.
            */
            DefaultSettlement(const Date& date = Date(),
                              Seniority seniority = NoSeniority,
                              Real recoveryRate = 0.4);
          public:
            Date date() const override;
            /*! Returns the recovery rate of a default event which has already
                settled.
            */
            Real recoveryRate(Seniority sen) const;
            void accept(AcyclicVisitor&) override;

          private:
            Date settlementDate_;
            //! Realized recovery rates
            std::map<Seniority, Real> recoveryRates_;
        };
      private:
        // for some reason, gcc chokes on the default parameter below
        // unless we use the typedef
        typedef std::map<Seniority, Real> rate_map;
      public:
        /*! Credit event with optional settlement
            information. Represents a credit event that has taken
            place. Realized events are of an atomic type.  If the
            settlement information is given seniorities present are
            the seniorities/bonds affected by the event.
        */
        DefaultEvent(const Date& creditEventDate,
                     const DefaultType& atomicEvType,
                     Currency curr,
                     Seniority bondsSen,
                     // Settlement information:
                     const Date& settleDate = Null<Date>(),
                     const std::map<Seniority, Real>& recoveryRates = rate_map());
        /*! Use NoSeniority to settle to all seniorities with that
            recovery. In that case the event is assumed to have
            affected all seniorities.
        */
        DefaultEvent(const Date& creditEventDate,
                     const DefaultType& atomicEvType,
                     Currency curr,
                     Seniority bondsSen,
                     // Settlement information:
                     const Date& settleDate = Null<Date>(),
                     Real recoveryRate = 0.4);

        Date date() const override;
        bool isRestructuring() const { return eventType_.isRestructuring(); }
        bool isDefault() const { return !isRestructuring();}
        bool hasSettled() const {
            return defSettlement_.date() != Null<Date>();
        }
        const DefaultSettlement& settlement() const {
            return defSettlement_;
        }
        const DefaultType& defaultType() const {
            return eventType_;
        }
        //! returns the currency of the bond this event refers to.
        const Currency& currency() const {
            return bondsCurrency_;
        }
        //! returns the seniority of the bond that triggered the event.
        Seniority eventSeniority() const {
            return bondsSeniority_;
        }
        /*! returns a value if the event lead to a settlement for the
            requested seniority.  Specializations on the default
            atomics and recoveries could change the default policy.
        */
        virtual Real recoveryRate(Seniority seniority) const {
            if(hasSettled()) {
                return defSettlement_.recoveryRate(seniority);
            }
            return Null<Real>();
        }

        /*! matches the event if this event would trigger a contract
            related to the requested event type.  Notice the
            contractual event types are not neccesarily atomic.
            Notice it does not check seniority or currency only event
            type.  typically used from Issuer
        */
        virtual bool matchesEventType(
                 const ext::shared_ptr<DefaultType>& contractEvType) const {
            // remember we are made of an atomic type.
            // behaviour by default...
            return
                contractEvType->containsRestructuringType(
                    eventType_.restructuringType()) &&
                contractEvType->containsDefaultType(
                    eventType_.defaultType());
        }
        /*! Returns true if this event would trigger a contract with
            the arguments characteristics.
        */
        virtual bool matchesDefaultKey(const DefaultProbKey& contractKey) const;

        void accept(AcyclicVisitor&) override;

      protected:
        Currency bondsCurrency_;
        Date defaultDate_;
        DefaultType eventType_;
        Seniority bondsSeniority_;
        DefaultSettlement defSettlement_;
    };

    /*! Two credit events are the same independently of their
        settlement member data. This has the side effect of
        overwritting different settlements from the same credit event
        when, say, inserting in a map. But on the other hand one given
        event can only have one settlement. This means we can not have
        two restructuring events on a bond on the same date.
    */
    bool operator==(const DefaultEvent& lhs, const DefaultEvent& rhs);

    inline bool operator!=(const DefaultEvent& lhs, const DefaultEvent& rhs) {
        return !(lhs == rhs);
    }

    template<>
    struct earlier_than<DefaultEvent> {
        bool operator()(const DefaultEvent& e1,
                        const DefaultEvent& e2) const {
            return e1.date() < e2.date();
        }
    };


    // ------------------------------------------------------------------------

    class FailureToPayEvent : public DefaultEvent {
      public:
        FailureToPayEvent(const Date& creditEventDate,
                          const Currency& curr,
                          Seniority bondsSen,
                          Real defaultedAmount,
                          // Settlement information:
                          const Date& settleDate,
                          const std::map<Seniority, Real>& recoveryRates);
        FailureToPayEvent(const Date& creditEventDate,
                          const Currency& curr,
                          Seniority bondsSen,
                          Real defaultedAmount,
                          // Settlement information:
                          const Date& settleDate,
                          Real recoveryRates);
        Real amountDefaulted() const {return defaultedAmount_;}
        bool matchesEventType(const ext::shared_ptr<DefaultType>& contractEvType) const override;

      private:
        Real defaultedAmount_;
    };


    // ------------------------------------------------------------------------

    class BankruptcyEvent : public DefaultEvent {
      public:
        BankruptcyEvent(const Date& creditEventDate,
                        const Currency& curr,
                        Seniority bondsSen,
                        // Settlement information:
                        const Date& settleDate,
                        const std::map<Seniority, Real>& recoveryRates);
        BankruptcyEvent(const Date& creditEventDate,
                        const Currency& curr,
                        Seniority bondsSen,
                        // Settlement information:
                        const Date& settleDate,
                        // means same for all
                        Real recoveryRates);
        //! This is a stronger than all event and will trigger all of them.
        bool matchesEventType(const ext::shared_ptr<DefaultType>&) const override { return true; }
    };

}

#endif
]]></document_content>
  </document>
  <document index="12">
    <source>defaultlossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_defaultlossmodel_hpp
#define quantlib_defaultlossmodel_hpp

#include <ql/instruments/claim.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <ql/utilities/disposable.hpp>
#include <ql/experimental/credit/basket.hpp>

#include <ql/utilities/null_deleter.hpp>

/* Intended to replace LossDistribution in 
    ql/experimental/credit/lossdistribution, not sure its covering all the 
    functionality (see mthod below)
*/

namespace QuantLib {

    /*! Default loss model interface definition.
    Allows communication between the basket and specific algorithms. Intended to
    hold any kind of portfolio joint loss, latent models, top-down,....

    An inconvenience of this design as opposed to the full arguments/results
    is that when pricing several derivatives instruments on the same basket
    not all the pricing engines would point to the same loss model; thus when
    pricing a set of such instruments there might be some switching on the 
    basket loss models, which might require recalculations (of the basket) or 
    not depending on the pricing order.
    */
    class DefaultLossModel : public Observable {// joint-? basket?-defaultLoss
     /* Protection together with frienship to avoid the need of checking the 
     basket-argument pointer integrity. It is the responsibility of the basket 
     now; our only caller.
     */
        friend class Basket;
    protected:
        // argument basket:
        mutable RelinkableHandle<Basket> basket_;

        DefaultLossModel() = default;
        //! \name Statistics
        //@{
        /* Non mandatory implementations, fails if client is not providing what 
        requested. */

        /* Default implementation using the expectedLoss(Date) method. 
          Typically this method is called repeatedly with the same 
          date parameter which makes it innefficient. */
        virtual Real expectedTrancheLoss(const Date& d) const {
            QL_FAIL("expectedTrancheLoss Not implemented for this model.");
        }
        /*! Probability of the tranche losing the same or more than the 
            fractional amount given.

            The passed lossFraction is a fraction of losses over the
            tranche notional (not the portfolio).
        */
        virtual Probability probOverLoss(
            const Date& d, Real lossFraction) const {
            QL_FAIL("probOverLoss Not implemented for this model.");   
        }
        //! Value at Risk given a default loss percentile.
        virtual Real percentile(const Date& d, Real percentile) const {
            QL_FAIL("percentile Not implemented for this model.");   
        }
        //! Expected shortfall given a default loss percentile.
        virtual Real expectedShortfall(const Date& d, Real percentile) const {
            QL_FAIL("eSF Not implemented for this model.");   
        }
        //! Associated VaR fraction to each counterparty.
        virtual Disposable<std::vector<Real> >
            splitVaRLevel(const Date& d, Real loss) const {
            QL_FAIL("splitVaRLevel Not implemented for this model.");   
        }
        //! Associated ESF fraction to each counterparty.
        virtual Disposable<std::vector<Real> >
            splitESFLevel(const Date& d, Real loss) const {
            QL_FAIL("splitESFLevel Not implemented for this model.");   
        }

        // \todo Add splits by instrument position.

        //! Full loss distribution.
        virtual Disposable<std::map<Real, Probability> > 
            lossDistribution(const Date&) const {
            QL_FAIL("lossDistribution Not implemented for this model.");   
        }
        //! Probability density of a given loss fraction of the basket notional.
        virtual Real densityTrancheLoss(
            const Date& d, Real lossFraction) const {
            QL_FAIL("densityTrancheLoss Not implemented for this model.");
        }
        /*! Probabilities for each of the (remaining) basket elements in the 
        pool to have defaulted by time d and at the same time be the Nth 
        defaulting name to default in the basket. This method is oriented to 
        default order dependent portfolio pricing (e.g. NTDs)
            The the probabilities ordering in the vector coincides with the 
            pool order.
        */
        virtual Disposable<std::vector<Probability> > probsBeingNthEvent(
            Size n, const Date& d) const {
            QL_FAIL("probsBeingNthEvent Not implemented for this model.");
        }
        //! Pearsons' default probability correlation. 
        virtual Real defaultCorrelation(const Date& d, Size iName, 
            Size jName) const {
            QL_FAIL("defaultCorrelation Not implemented for this model.");
        }
        /*! Returns the probaility of having a given or larger number of 
        defaults in the basket portfolio at a given time.
        */
        virtual Probability probAtLeastNEvents(Size n, const Date& d) const {
            QL_FAIL("probAtLeastNEvents Not implemented for this model.");
        }
        /*! Expected RR for name conditinal to default by that date.
        */
        virtual Real expectedRecovery(const Date&, Size iName, 
            const DefaultProbKey&) const {
            QL_FAIL("expected recovery Not implemented for this model.");
        }
        //@}

        /*! Send a reference to the basket to allow the model to read the 
        problem arguments (contained in the basket)
        */
    private: //can only be called from Basket
        void setBasket(Basket* bskt) {
            /* After this; if the model modifies its internal status/caches (if 
            any) it should notify the  prior basket to recognise that basket is 
            not in a calculated=true state. Since we dont know at this level if 
            the model keeps caches it is the children responsibility. Typically 
            this is done at the first call to calculate to the loss model, there
            it notifies the basket. The old basket is still registered with us 
            until the basket takes in a new model....
            ..alternatively both old basket and model could be forced reset here
            */
            basket_.linkTo(ext::shared_ptr<Basket>(bskt, null_deleter()),
                           false);
            resetModel();// or rename to setBasketImpl(...)
        }
        // the call order matters, which is the reason for the parent to be the 
        //   sole caller.
        //! Concrete models do now any updates/inits they need on basket reset
        virtual void resetModel() = 0;
    };

}

#endif
]]></document_content>
  </document>
  <document index="13">
    <source>defaultprobabilitykey.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 StatPro Italia srl
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file defaultprobabilitykey.hpp
    \brief Classes for default-event description.
*/

#ifndef quantlib_default_probability_key_hpp
#define quantlib_default_probability_key_hpp

#include <ql/experimental/credit/defaulttype.hpp>
#include <ql/currency.hpp>
#include <vector>

namespace QuantLib {

    /*! Used to index market implied credit curve probabilities. It is
        a proxy to the defaultable bond or class of bonds which
        determines the credit contract conditions.  It aggregates the
        atomic default types in a group defining the contract
        conditions and which serves to index the probability curves
        calibrated to the market.
    */
    class DefaultProbKey {
      protected:
        //! aggregation of event types for which the contract is sensitive.
        std::vector<ext::shared_ptr<DefaultType> > eventTypes_;
        //! Currency of the bond and protection leg payment.
        Currency obligationCurrency_;
        //! Reference bonds seniority.
        Seniority seniority_ = NoSeniority;

      public:
        DefaultProbKey();

        DefaultProbKey(std::vector<ext::shared_ptr<DefaultType> > eventTypes,
                       Currency cur,
                       Seniority sen);

        const Currency& currency() const {return obligationCurrency_;}
        Seniority seniority() const {return seniority_;}
        const std::vector<ext::shared_ptr<DefaultType> >&
            eventTypes() const {
                return eventTypes_;
        }
        Size size() const {return eventTypes_.size();}
    };

    bool operator==(const DefaultProbKey& lhs, const DefaultProbKey& rhs);


    //! ISDA standard default contractual key for corporate US debt.
    //    Restructuring here can be set to NoRestructuring.
    class NorthAmericaCorpDefaultKey : public DefaultProbKey {
      public:
        // with only one restructuring type
        NorthAmericaCorpDefaultKey(const Currency& currency,
                                   Seniority sen,
                                   Period graceFailureToPay =
                                       Period(30, Days),
                                   Real amountFailure = 1.e6,
                                   Restructuring::Type resType =
                                                           Restructuring::CR);
    };

}

#endif
]]></document_content>
  </document>
  <document index="14">
    <source>defaultprobabilitylatentmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_default_latent_model_hpp
#define quantlib_default_latent_model_hpp

#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/math/latentmodel.hpp>
#include <ql/experimental/math/gaussiancopulapolicy.hpp>
#include <boost/dynamic_bitset.hpp>

namespace QuantLib {

    /*! \brief Default event Latent Model.

     This is a model for joint default events based on a generic Latent 
      Model. It models solely the default events in a portfolio, not making any 
      reference to severities, exposures, etc...
     An implicit correspondence is stablished between the variables modelled and
     the names in the basket given by the basket and model variable access 
     indices.
     The class is parametric on the Latent Model copula.

     \todo Consider QL_REQUIRE(basket_, "No portfolio basket set.") test in 
     debug model only for performance reasons.
    */
    template<class copulaPolicy>
    class DefaultLatentModel : public LatentModel<copulaPolicy> {
        // import template members
    protected:
        using LatentModel<copulaPolicy>::factorWeights_;
        using LatentModel<copulaPolicy>::idiosyncFctrs_;
        using LatentModel<copulaPolicy>::copula_;
    public:
        using LatentModel<copulaPolicy>::inverseCumulativeY;
        using LatentModel<copulaPolicy>::cumulativeZ;
        using LatentModel<copulaPolicy>::integratedExpectedValue;// which one?
    protected:
        // not a handle, the model doesnt keep any cached magnitudes, no need 
        //  for notifications, still...
        mutable ext::shared_ptr<Basket> basket_;
        ext::shared_ptr<LMIntegration> integration_;
    private:
        typedef typename copulaPolicy::initTraits initTraits;
    public:
        /*!
        @param factorWeights Latent model independent factors weights for each 
            variable.
        @param integralType Integration type.
        @param ini Copula initialization if any.

        \warning Baskets with realized defaults not tested/WIP.
        */
        DefaultLatentModel(
            const std::vector<std::vector<Real> >& factorWeights,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            const initTraits& ini = initTraits()
            ) 
        : LatentModel<copulaPolicy>(factorWeights, ini),
          integration_(LatentModel<copulaPolicy>::IntegrationFactory::
            createLMIntegration(factorWeights[0].size(), integralType))
        { }
        DefaultLatentModel(
            const Handle<Quote>& mktCorrel,
            Size nVariables,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            const initTraits& ini = initTraits()
            )
        : LatentModel<copulaPolicy>(mktCorrel, nVariables, ini),
          integration_(LatentModel<copulaPolicy>::IntegrationFactory::
            createLMIntegration(1, integralType))
        { }
        /* \todo
            Add other constructors as in LatentModel for ease of use. (less 
            dimensions, factors, etcc...)
        */

        /* To interface with loss models. It is possible to change the basket 
        since there are no cached magnitudes.
        */
        void resetBasket(const ext::shared_ptr<Basket>& basket) const {
            basket_ = basket;
            // in the future change 'size' to 'liveSize'
            QL_REQUIRE(basket_->size() == factorWeights_.size(), 
                "Incompatible new basket and model sizes.");
        }

        /*! Returns the probability of default of a given name conditional on
        the realization of a given set of values of the model independent
        factors. The date at which the probability is given is implicit in the
        probability since theres not other time dependence in this model.
        @param prob Unconditional probability of default.
        @param iName desired name.
        @param mktFactors Value of LM independent factors.
        \warning Most often it is preferred to use the method below avoiding the
        cumulative inversion.
        */
        Probability conditionalDefaultProbability(Probability prob, Size iName,
            const std::vector<Real>& mktFactors) const 
        {
            // we can be called from the outside (from an integrable loss model)
            //   but we are called often at integration points. This or
            //   consider a list of friends.
        #if defined(QL_EXTRA_SAFETY_CHECKS)
            QL_REQUIRE(basket_, "No portfolio basket set.");
        #endif
            /*Avoid redundant call to minimum value inversion (might be \infty),
            and this independently of the copula function.
            */
            if (prob < 1.e-10) return 0.;// use library macro...
            return conditionalDefaultProbabilityInvP(
                inverseCumulativeY(prob, iName), iName, mktFactors);
        }
    protected:
      void update() override {
          if (basket_ != nullptr)
              basket_->notifyObservers();
          LatentModel<copulaPolicy>::update();
      }

    public:// open since users access it for performance on joint integrations.

        /*! Returns the probability of default of a given name conditional on
        the realization of a given set of values of the model independent
        factors. The date at which the probability is given is implicit in the
        probability since theres not other time dependent in this model.
        Same intention as above but provides a performance opportunity, if the
        integration is along the market factors (as usually is) avoids computing
        the inverse of the probability on each call.
        @param invCumYProb Inverse cumul of the unconditional probability of 
          default, has to follow the same copula law for results to be coherent
        @param iName desired name.
        @param m Value of LM independent factors.
        */
        Probability conditionalDefaultProbabilityInvP(Real invCumYProb, 
            Size iName, 
            const std::vector<Real>& m) const {
            Real sumMs = 
                std::inner_product(factorWeights_[iName].begin(), 
                    factorWeights_[iName].end(), m.begin(), 0.);
            Real res = cumulativeZ((invCumYProb - sumMs) / 
                    idiosyncFctrs_[iName] );
            #if defined(QL_EXTRA_SAFETY_CHECKS)
            QL_REQUIRE (res >= 0. && res <= 1.,
                        "conditional probability " << res << "out of range");
            #endif
        
            return res;
        }
    protected:
        /*! Returns the probability of default of a given name conditional on
        the realization of a given set of values of the model independent
        factors.
        @param date The date for the probability of default.
        @param iName desired name.
        @param mktFactors Value of LM independent factors.

        Same intention as the above methods. Usage of this one is typically more
        expensive because most often the date we call this method with
        repeats itself and with this one the probability can not be cached
        outside the call.
        */
        Probability conditionalDefaultProbability(const Date& date, Size iName,
            const std::vector<Real>& mktFactors) const 
        {
            const ext::shared_ptr<Pool>& pool = basket_->pool();
            Probability pDefUncond =
                pool->get(pool->names()[iName]).
                defaultProbability(basket_->defaultKeys()[iName])
                  ->defaultProbability(date);
            return conditionalDefaultProbability(pDefUncond, iName, mktFactors);
        }
        /*! Conditional default probability product, intermediate step in the 
            correlation calculation.*/
        Probability condProbProduct(Real invCumYProb1, Real invCumYProb2, 
            Size iName1, Size iName2, 
            const std::vector<Real>& mktFactors) const {
            return 
                conditionalDefaultProbabilityInvP(invCumYProb1, iName1, 
                    mktFactors) *
                conditionalDefaultProbabilityInvP(invCumYProb2, iName2, 
                    mktFactors);
        }
        //! Conditional probability of n default events or more.
        // \todo: check the issuer has not defaulted.
        Real conditionalProbAtLeastNEvents(Size n, const Date& date,
            const std::vector<Real>& mktFactors) const;
        //! access to integration:
        const ext::shared_ptr<LMIntegration>& integration() const override { return integration_; }

      public:
        /*! Computes the unconditional probability of default of a given name. 
        Trivial method for testing
        */
        Probability probOfDefault(Size iName, const Date& d) const {
            QL_REQUIRE(basket_, "No portfolio basket set.");
            const ext::shared_ptr<Pool>& pool = basket_->pool();
            // avoid repeating this in the integration:
            Probability pUncond = pool->get(pool->names()[iName]).
                defaultProbability(basket_->defaultKeys()[iName])
                ->defaultProbability(d);
            if (pUncond < 1.e-10) return 0.;

            return integratedExpectedValue(
                [&](const std::vector<Real>& v1) {
                    return conditionalDefaultProbabilityInvP(
                        inverseCumulativeY(pUncond, iName), iName, v1);
                });
        }
        /*! Pearsons' default probability correlation. 
            Users should consider specialization on the copula type for specific
            distributions since that might simplify the integrations, most 
            importantly if this is to be used in calibration of observations for
            factor coefficients as it is expensive to integrate directly.
        */
        Real defaultCorrelation(const Date& d, Size iNamei, Size iNamej) const;

        /*! Returns the probaility of having a given or larger number of 
        defaults in the basket portfolio at a given time.
        */
        Probability probAtLeastNEvents(Size n, const Date& date) const {
            return integratedExpectedValue(
                [&](const std::vector<Real>& v1) {
                    return conditionalProbAtLeastNEvents(n, date, v1);
                });
        }
    };


    //---- Defines -----------------------------------------------------------

    template<class CP>
    Real DefaultLatentModel<CP>::defaultCorrelation(const Date& d, 
        Size iNamei, Size iNamej) const 
    {
        QL_REQUIRE(basket_, "No portfolio basket set.");

        const ext::shared_ptr<Pool>& pool = basket_->pool();
        // unconditionals:
        Probability pi = pool->get(pool->names()[iNamei]).
            defaultProbability(basket_->defaultKeys()[iNamei])
            ->defaultProbability(d);
        Probability pj = pool->get(pool->names()[iNamej]).
            defaultProbability(basket_->defaultKeys()[iNamej])
            ->defaultProbability(d);
        Real pipj = pi * pj;
        Real invPi = inverseCumulativeY(pi, iNamei);
        Real invPj = inverseCumulativeY(pj, iNamej);
        // avoid repetitive calls when i=j?
        Real E1i1j; // joint default covariance term
        if(iNamei !=iNamej) {
            E1i1j = integratedExpectedValue(
                [&](const std::vector<Real>& v1) {
                    return condProbProduct(invPi, invPj, iNamei, iNamej, v1); });
        }else{
            E1i1j = pi;
        }
        return (E1i1j - pipj )/std::sqrt(pipj*(1.-pi)*(1.-pj));
    }


    template<class CP>
    Real DefaultLatentModel<CP>::conditionalProbAtLeastNEvents(Size n, 
        const Date& date,
        const std::vector<Real>& mktFactors) const {
            QL_REQUIRE(basket_, "No portfolio basket set.");

            /* \todo 
            This algorithm traverses all permutations starting form the
            lowest one. This is inneficient, there shouldnt be any need to 
            go through the invalid ones. Use combinations of n elements.

            See integration in O'Kane for homogeneous ntds.
            */
            // first position with as many defaults as desired:
            Size poolSize = basket_->size();//move to 'livesize'
            const ext::shared_ptr<Pool>& pool = basket_->pool();

            auto limit = static_cast<BigNatural>(std::pow(2., (int)(poolSize)));

            // Precalc conditional probabilities
            std::vector<Probability> pDefCond;
            for(Size i=0; i<poolSize; i++)
                pDefCond.push_back(conditionalDefaultProbability(
                    pool->get(pool->names()[i]).
                    defaultProbability(basket_->defaultKeys()[i])->
                    defaultProbability(date), i, mktFactors));

            Probability probNEventsOrMore = 0.;
            for (auto mask = static_cast<BigNatural>(std::pow(2., (int)(n)) - 1); mask < limit;
                 mask++) {
                // cheap permutations
                boost::dynamic_bitset<> bsetMask(poolSize, mask);
                if(bsetMask.count() >= n) {
                    Probability pConfig = 1;
                    for(Size i=0; i<bsetMask.size(); i++)
                        pConfig *= 
                          (bsetMask[i] ? pDefCond[i] : (1.- pDefCond[i]));
                    probNEventsOrMore += pConfig;
                }
            }
            return probNEventsOrMore;
        }


    // often used:
    typedef DefaultLatentModel<GaussianCopulaPolicy> GaussianDefProbLM;
    typedef DefaultLatentModel<TCopulaPolicy> TDefProbLM;
}

#endif
]]></document_content>
  </document>
  <document index="15">
    <source>defaulttype.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 StatPro Italia srl
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file defaulttype.hpp
    \brief Classes for default-event description.
*/


#ifndef quantlib_default_type_hpp
#define quantlib_default_type_hpp

#include <ql/time/period.hpp>

namespace QuantLib {

    //! Seniority of a bond.
    /*! They are also ISDA tier/seniorities used for CDS conventional
        spreads.
    */
    enum Seniority {
        SecDom = 0,
        SnrFor,
        SubLT2,
        JrSubT2,
        PrefT1,
        // Unassigned value, allows for default RR quote
        NoSeniority,
        // markit parlance
        SeniorSec     = SecDom,
        SeniorUnSec   = SnrFor,
        SubTier1      = PrefT1,
        SubUpperTier2 = JrSubT2,
        SubLoweTier2  = SubLT2
    };


    //! Atomic (single contractual event) default events.
    /*! Default types defined as enum to allow easy aggregation of
        types. Theres an event algebra logic by default provided by
        DefaultType. If your new type requires more sofisticated test
        you need to derive from it as in FailureToPay
    */
    struct AtomicDefault {
        enum Type {
            // Includes one of the restructuring cases
            Restructuring = 0,
            Bankruptcy,
            FailureToPay,
            RepudiationMoratorium,
            Acceleration,
            Default,
            // synonyms
            ObligationAcceleration = Acceleration,
            ObligationDefault = Default,
            CrossDefault = Default,
            // Other non-isda
            Downgrade,   // Non-ISDA, not in FpML
            MergerEvent  // Non-ISDA, not in FpML
        };
    };


    // these could be merged with the ones above if not because
    //   restructuring types can not be combined together.

    //! Restructuring type
    struct Restructuring {
        enum Type {
            NoRestructuring = 0,
            ModifiedRestructuring,
            ModifiedModifiedRestructuring,
            FullRestructuring,
            AnyRestructuring,
            // Markit notation:
            XR = NoRestructuring,
            MR = ModifiedRestructuring,
            MM = ModifiedModifiedRestructuring,
            CR = FullRestructuring
        };
    };


    //! Atomic credit-event type.
    /*! This class encapsulates the ISDA default contractual types and
        their combinations. Non-atomicity works only at the atomic
        type level, obviating the specific event characteristics which
        it is accounted for only in derived classes.
    */
    class DefaultType {
      public:
        explicit DefaultType(AtomicDefault::Type defType =
                                                    AtomicDefault::Bankruptcy,
                             Restructuring::Type restType = Restructuring::XR);

        virtual ~DefaultType() = default;

        AtomicDefault::Type defaultType() const {
            return defTypes_;
        }
        Restructuring::Type restructuringType() const {return restrType_;}
        bool isRestructuring() const {
            return restrType_ != Restructuring::NoRestructuring;
        }

        // bool isAtomic() const { return defTypes_.size() == 1;}

        /*! Returns true if one or a set of event types is within this
            one and as such will be recognised as a trigger. Not the
            same as equality.

            Notice that these methods do not include any event logical
            hierarchy. The match is in a strict sense. If event B is
            contained in (implied by) event A this would not send a
            match. This policies should be implemented at the
            CreditEvent class, which is polymorphic.
        */
        bool containsDefaultType(AtomicDefault::Type defType) const {
            return defTypes_ ==  defType;
        }

        bool containsRestructuringType(Restructuring::Type resType) const {
            return (restrType_ == resType) ||
                (Restructuring::AnyRestructuring == resType);
        }
    protected:
        //std::set<AtomicDefault::Type> defTypes_;
        AtomicDefault::Type defTypes_;
        Restructuring::Type restrType_;
    };


    /*! Equality is the criteria for indexing the curves. This depends
        only on the atomic types and not on idiosincracies of derived
        type as mentioned in the functional documentation (specific
        event characteristics are relevant to credit event matching
        but not to the probability meaning).  operator== is also used
        to remove duplicates in some containers. This ensures we do
        not have two equal events (despite having different
        characteristics) in those containers. This makes sense, theres
        no logic in having two FailureToPay in a contract even if they
        have different characteristics.
    */
    bool operator==(const DefaultType& lhs, const DefaultType& rhs);



    //! Failure to Pay atomic event type.
    class FailureToPay : public DefaultType {
      public:
        // Only atomic construction.
        // Amount contract by default is in dollars as per ISDA doc and not
        //   the contract curr. Theres an issue here...... FIX ME
        explicit FailureToPay(const Period& grace,
                              Real amount = 1.e+6)
        : DefaultType(AtomicDefault::FailureToPay, Restructuring::XR),
          gracePeriod_(grace), amountRequired_(amount) {}

        Real amountRequired() const {return amountRequired_;}
        const Period& gracePeriod() const {return gracePeriod_;}
      private:
        // Grace period to consider the event. If payment occurs during
        // the period the event should be removed from its container.
        Period gracePeriod_;
        // Minimum default amount triggering the event
        Real amountRequired_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="16">
    <source>distribution.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file distribution.hpp
    \brief Discretized probability density and cumulative probability
*/

#ifndef quantlib_probability_distribution_hpp
#define quantlib_probability_distribution_hpp

#include <ql/types.hpp>
#include <vector>

namespace QuantLib {

    //! Discretized probability density and cumulative probability
    /*! Discretized probability density and cumulative probability
      \ingroup probability
    */
    class ManipulateDistribution;
    class Distribution {
    public:
        friend class ManipulateDistribution;
        Distribution (int nBuckets, Real xmin, Real xmax);
        Distribution() = default;
        ;

        void add (Real value);
        void addDensity (int bucket, Real value);
        void addAverage (int bucket, Real value);
        void normalize ();

        Size size () const { return size_; }
        Real x (Size k) { return x_.at(k); }
        std::vector<Real>& x () { return x_; }
        Real dx (Size k) { return dx_.at(k); }
        std::vector<Real>& dx () { return dx_; }
        Real dx (Real x);

        Real density (Size k) {
            normalize();
            return density_.at(k);
        }
        Real cumulative (Size k) {
            normalize();
            return cumulativeDensity_.at(k);
        }
        Real excess (Size k) {
            normalize();
            return excessProbability_.at(k);
        }
        Real cumulativeExcess (Size k) {
            normalize();
            return cumulativeExcessProbability_.at(k);
        }
        Real average (Size k) { return average_.at(k); }

        Real confidenceLevel (Real quantil);
        Real cumulativeDensity (Real x);
        Real cumulativeExcessProbability (Real a, Real b);
        Real expectedValue ();
        Real trancheExpectedValue (Real a, Real d);

        template <class F>
        Real expectedValue (F& f) {
            normalize();
            Real expected = 0;
            for (int i = 0; i < size_; i++) {
                Real x = x_[i] + dx_[i]/2;
                expected += f (x) * dx_[i] * density_[i];
            }
            return expected;
        }

        /*!
          Transform the loss distribution into the tranche loss distribution
          for losses L_T = min(L,D) - min(L,A).
          The effects are:
          1) shift the distribution to the left by A, then
          2) cut off at D-A, Pr(L_T > D-A) = 0
          3) ensure Pr(L_T >= 0) = 1, i.e. a density spike at L_T = 0
         */
        void tranche (Real attachmentPoint, Real detachmentPoint);

        /*
          index of the grid point to the left of x
        */
        int locate (Real x);

        /* Returns the average value conditional on values above
        the passed percentile probability */
        Real expectedShortfall (Real percValue);
    private:
        int size_;
        Real xmin_, xmax_;
        std::vector<int> count_;
        // x: coordinate of left hand cell bundary
        // dx: cell width
        std::vector<Real> x_, dx_;
        // density: probability density, densitx*dx = prob. of loss in cell i
        // cumulatedDensity: cumulated (integrated) from x = 0
        // excessProbability: cumulated from x_i to infinity
        // cumulativeExcessProbability: integrated excessProbability from x = 0
        std::vector<Real> density_, cumulativeDensity_;
        std::vector<Real> excessProbability_, cumulativeExcessProbability_;
        // average loss in cell i
        std::vector<Real> average_;

        int overFlow_, underFlow_;
        bool isNormalized_;
    };

    class ManipulateDistribution {
    public:
        static Distribution convolve (const Distribution& d1,
                                      const Distribution& d2);
    };

}

#endif
]]></document_content>
  </document>
  <document index="17">
    <source>factorspreadedhazardratecurve.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file factorspreadedhazardratecurve.hpp
    \brief Default-probability structure with a multiplicative spread on hazard rates
*/

#ifndef quantlib_factor_spreaded_hazard_rate_curve_hpp
#define quantlib_factor_spreaded_hazard_rate_curve_hpp

#include <ql/quote.hpp>
#include <ql/termstructures/credit/hazardratestructure.hpp>
#include <utility>

namespace QuantLib {

    //! Default-probability structure with a multiplicative spread on hazard rates
    /*! \note This term structure will remain linked to the original
              structure, i.e., any changes in the latter will be
              reflected in this structure as well.

        \ingroup termstructures
    */
    class FactorSpreadedHazardRateCurve : public HazardRateStructure {
      public:
        FactorSpreadedHazardRateCurve(Handle<DefaultProbabilityTermStructure> originalCurve,
                                      Handle<Quote> spread);
        //! \name DefaultTermStructure interface
        //@{
        DayCounter dayCounter() const override;
        Calendar calendar() const override;
        const Date& referenceDate() const override;
        Date maxDate() const override;
        Time maxTime() const override;
        //@}
      protected:
        Real hazardRateImpl(Time t) const override;

      private:
        Handle<DefaultProbabilityTermStructure> originalCurve_;
        Handle<Quote> spread_;
    };


    // inline definitions

    inline FactorSpreadedHazardRateCurve::FactorSpreadedHazardRateCurve(
        Handle<DefaultProbabilityTermStructure> h, Handle<Quote> spread)
    : originalCurve_(std::move(h)), spread_(std::move(spread)) {
        registerWith(originalCurve_);
        registerWith(spread_);
    }

    inline DayCounter FactorSpreadedHazardRateCurve::dayCounter() const {
        return originalCurve_->dayCounter();
    }

    inline Calendar FactorSpreadedHazardRateCurve::calendar() const {
        return originalCurve_->calendar();
    }

    inline const Date& FactorSpreadedHazardRateCurve::referenceDate() const {
        return originalCurve_->referenceDate();
    }

    inline Date FactorSpreadedHazardRateCurve::maxDate() const {
        return originalCurve_->maxDate();
    }

    inline Time FactorSpreadedHazardRateCurve::maxTime() const {
        return originalCurve_->maxTime();
    }

    inline Real FactorSpreadedHazardRateCurve::hazardRateImpl(Time t) const {
        return originalCurve_->hazardRate(t, true) * (1.0 + spread_->value());
    }

}

#endif
]]></document_content>
  </document>
  <document index="18">
    <source>gaussianlhplossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_gaussian_lhp_lossmodel_hpp
#define quantlib_gaussian_lhp_lossmodel_hpp

#include <ql/qldefines.hpp>

#ifndef QL_PATCH_SOLARIS

#include <ql/math/distributions/bivariatenormaldistribution.hpp>
#include <ql/experimental/credit/recoveryratequote.hpp>
#include <ql/quotes/simplequote.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/math/latentmodel.hpp>
#include <ql/functional.hpp>
#include <numeric>

/* Intended to replace GaussianLHPCDOEngine in 
    ql/experimental/credit/syntheticcdoengines.hpp
   Moved from an engine to a loss model, CDO engines might refer to it.
*/

namespace QuantLib {

    /*!
      Portfolio loss model with analytical expected tranche loss for a large 
      homogeneous pool with Gaussian one-factor copula. See for example
      "The Normal Inverse Gaussian Distribution for Synthetic CDO pricing.",
      Anna Kalemanova, Bernd Schmid, Ralf Werner,
      Journal of Derivatives, Vol. 14, No. 3, (Spring 2007), pp. 80-93.
      http://www.defaultrisk.com/pp_crdrv_91.htm

      It can be used to price a credit derivative or to provide risk metrics of 
      a portfolio.

      \todo It should be checking that basket exposures are deterministic (fixed
      or programmed amortizing) otherwise the model is not fit for the basket.

      \todo Bugging on tranched baskets with upper limit over maximum 
        attainable loss?
     */
    class GaussianLHPLossModel : public DefaultLossModel, 
        public LatentModel<GaussianCopulaPolicy> {
    public:
        typedef GaussianCopulaPolicy copulaType;

        GaussianLHPLossModel(
            const Handle<Quote>& correlQuote,
            const std::vector<Handle<RecoveryRateQuote> >& quotes);

        GaussianLHPLossModel(
            Real correlation,
            const std::vector<Real>& recoveries);

        GaussianLHPLossModel(
            const Handle<Quote>& correlQuote,
            const std::vector<Real>& recoveries);

        void update() override {
            sqrt1minuscorrel_ = std::sqrt(1.-correl_->value());
            beta_ = std::sqrt(correl_->value());
            biphi_ = BivariateCumulativeNormalDistribution(
                -beta_);
            // tell basket to notify instruments, etc, we are invalid
            if(!basket_.empty()) basket_->notifyObservers();
        }

    private:
      void resetModel() override {}
      /*! @param attachLimit as a fraction of the underlying live portfolio
      notional
      */
      Real expectedTrancheLossImpl(Real remainingNot, // << at the given date 'd'
                                   Real prob,         // << at the given date 'd'
                                   Real averageRR,    // << at the given date 'd'
                                   Real attachLimit,
                                   Real detachLimit) const;
    public:
      Real expectedTrancheLoss(const Date& d) const override {
          // can calls to Basket::remainingNotional(d) be cached?<<<<<<<<<<<<<
          const Real remainingfullNot = basket_->remainingNotional(d);
          Real averageRR = averageRecovery(d);
          Probability prob = averageProb(d);
          Real remainingAttachAmount = basket_->remainingAttachmentAmount();
          Real remainingDetachAmount = basket_->remainingDetachmentAmount();


          // const Real attach = std::min(remainingAttachAmount
          //    / remainingfullNot, 1.);
          // const Real detach = std::min(remainingDetachAmount
          //    / remainingfullNot, 1.);
          const Real attach = remainingAttachAmount / remainingfullNot;
          const Real detach = remainingDetachAmount / remainingfullNot;

          return expectedTrancheLossImpl(remainingfullNot, prob, averageRR, attach, detach);
      }

        /*! The passed remainingLossFraction is in live tranche units,
            not portfolio as a fraction of the remaining(live) tranche
            (i.e. a_remaining=0% and det_remaining=100%)
        */
      Real probOverLoss(const Date& d, Real remainingLossFraction) const override;

      //! Returns the ESF as an absolute amount (rather than a fraction)
      /* The way it is implemented here is a transformation from ETL to ESF
      is a generic algorithm, not specific to this model so it should be moved
      to the Basket/DefaultLossModel class.
      TO DO: Implement the inverse transformation
      */
      Real expectedShortfall(const Date& d, Probability perctl) const override;

    protected:
        // This is wrong, it is not accounting for the current defaults ....
        // returns the loss value in actual loss units, returns the loss value 
        // for the underlying portfolio, untranched
        Real percentilePortfolioLossFraction(const Date& d, Real perctl) const;
        Real expectedRecovery(const Date& d, Size iName, const DefaultProbKey& ik) const override {
            return rrQuotes_[iName].currentLink()->value();
        }

    public:
        // same as percentilePortfolio but tranched
      Real percentile(const Date& d, Real perctl) const override {
          const Real remainingNot = basket_->remainingNotional(d);
          Real remainingAttachAmount = basket_->remainingAttachmentAmount();
          Real remainingDetachAmount = basket_->remainingDetachmentAmount();
          const Real attach = std::min(remainingAttachAmount / remainingNot, 1.);
          const Real detach = std::min(remainingDetachAmount / remainingNot, 1.);
          return remainingNot *
                 std::min(std::max(percentilePortfolioLossFraction(d, perctl) - attach, 0.),
                          detach - attach);
      }

        Probability averageProb(const Date& d) const {// not an overload of Deflossmodel ???<<<<<???
            // weighted average by programmed exposure.
            const std::vector<Probability> probs = 
                basket_->remainingProbabilities(d);//use remaining basket
            const std::vector<Real> remainingNots = 
                basket_->remainingNotionals(d);
            return std::inner_product(probs.begin(), probs.end(), 
                remainingNots.begin(), 0.) / basket_->remainingNotional(d);
        }

        /* One could define the average recovery without the probability
        factor, weighting only by notional instead, but that way the expected 
        loss of the average/aggregated and the original portfolio would not 
        coincide. This introduces however a time dependence in the recovery 
        value.
        Weighting by notional implies time dependent weighting since the basket 
        might amortize.
        */
        Real averageRecovery(
            const Date& d) const //no explicit time dependence in this model
        {
            const std::vector<Probability> probs = 
                basket_->remainingProbabilities(d);
            std::vector<Real> recoveries;
            for(Size i=0; i<basket_->remainingSize(); i++)
                recoveries.push_back(rrQuotes_[i]->value());
            std::vector<Real> notionals = basket_->remainingNotionals(d);
            Real denominator = std::inner_product(notionals.begin(), 
                notionals.end(), probs.begin(), 0.);
            if(denominator == 0.) return 0.;

            std::transform(notionals.begin(), notionals.end(), probs.begin(),
                notionals.begin(), std::multiplies<Real>());

            return std::inner_product(recoveries.begin(), recoveries.end(), 
                notionals.begin(), 0.) / denominator;
        }

    private:
        // cached
        mutable Real sqrt1minuscorrel_;

        Handle<Quote> correl_;
        std::vector<Handle<RecoveryRateQuote> > rrQuotes_;
        // calculation buffers

        /* The problem with defining a fixed average recovery on a portfolio 
        with uneven exposures is that it does not preserve portfolio
        moments like the expected loss. To achieve it one should define the 
        averarage recovery with a time dependence: 
        $\hat{R}(t) = \frac{\sum_i R_i N_i P_i(t)}{\sum_i N_i P_i(t)}$
        But the date dependence increases significantly the calculations cost.
        Notice that this problem dissapears if the recoveries are all equal.
        */
        
        Real beta_;
        BivariateCumulativeNormalDistribution biphi_;
        static CumulativeNormalDistribution const phi_;
    };

}

#endif

#endif
]]></document_content>
  </document>
  <document index="19">
    <source>homogeneouspooldef.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_homogenous_pool_default_model_hpp
#define quantlib_homogenous_pool_default_model_hpp

#include <ql/experimental/credit/lossdistribution.hpp>
#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/math/functional.hpp>

// Intended to replace HomogeneousPoolCDOEngine in syntheticcdoengines.hpp

namespace QuantLib {

    //-------------------------------------------------------------------------
    //! Default loss distribution convolution for finite homogeneous pool
    /* A note on the number of buckets: As it is now the code goes splitting
    losses into buckets from loses equal to zero to losses up to the value of
    the underlying basket. This is in view of a stochastic loss given default
    but in a constant LGD situation this is a waste and it is more efficient to
    go up to the attainable losses.
    \todo Extend to the multifactor case for a generic LM
    */
    template<class copulaPolicy>
    class HomogeneousPoolLossModel : public DefaultLossModel {
    private:
      void resetModel() override;

    public:
        HomogeneousPoolLossModel(
            const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> >& 
                copula,
            Size nBuckets,
            Real max = 5.,
            Real min = -5.,
            Real nSteps = 50)
        : copula_(copula), 
          nBuckets_(nBuckets), 
          max_(max), min_(min), nSteps_(nSteps), delta_((max - min)/nSteps)
        { 
            QL_REQUIRE(copula->numFactors() == 1, 
                "Inhomogeneous model not implemented for multifactor");
        }
    protected:
        Distribution lossDistrib(const Date& d) const;
    public:
      Real expectedTrancheLoss(const Date& d) const override {
          return lossDistrib(d).cumulativeExcessProbability(attachAmount_, detachAmount_);
          // This one if the distribution is over the whole loss structure:
          // but it becomes very expensive
          /*
          return lossDistrib(d).trancheExpectedValue(attach_ * notional_,
              detach_ * notional_);
          */
      }
      Real percentile(const Date& d, Real percentile) const override {
          Real portfLoss = lossDistrib(d).confidenceLevel(percentile);
          return std::min(std::max(portfLoss - attachAmount_, 0.), detachAmount_ - attachAmount_);
      }
      Real expectedShortfall(const Date& d, Probability percentile) const override {
          Distribution dist = lossDistrib(d);
          dist.tranche(attachAmount_, detachAmount_);
          return dist.expectedShortfall(percentile);
      }

    protected:
        const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> > copula_;
        Size nBuckets_;
        mutable Real attach_, detach_, notional_, attachAmount_, detachAmount_;
        mutable std::vector<Real> notionals_;
    private:
        // integration:
        //  \todo move integration to latent model types when moving to a 
        //  multifactor version
        const Real max_;// redundant?
        const Real min_;
        const Real nSteps_;
        const Real delta_; 
    };
    // \todo Add other loss distribution statistics
    typedef HomogeneousPoolLossModel<GaussianCopulaPolicy> 
        HomogGaussPoolLossModel;
    typedef HomogeneousPoolLossModel<TCopulaPolicy> HomogTPoolLossModel;

    //-----------------------------------------------------------------------

    template<class CP>
    void HomogeneousPoolLossModel<CP>::resetModel()
    {
        // need to be capped now since the limit amounts might be over the 
        //  remaining notional (think amortizing)
        attach_ = std::min(basket_->remainingAttachmentAmount() / 
            basket_->remainingNotional(), 1.);
        detach_ = std::min(basket_->remainingDetachmentAmount() / 
            basket_->remainingNotional(), 1.);
        notional_ = basket_->remainingNotional();
        notionals_ = basket_->remainingNotionals();
        attachAmount_ = basket_->remainingAttachmentAmount();
        detachAmount_ = basket_->remainingDetachmentAmount();

        copula_->resetBasket(basket_.currentLink());
    }

    template<class CP>
    Distribution HomogeneousPoolLossModel<CP>::lossDistrib(
        const Date& d) const 
    {
        LossDistHomogeneous bucktLDistBuff(nBuckets_, detachAmount_);

        std::vector<Real> lgd;// switch to a mutable cache member
        std::vector<Real> recoveries = copula_->recoveries();
        std::transform(recoveries.begin(), recoveries.end(), 
                       std::back_inserter(lgd),
                       subtract_from<Real>(1.0));
        std::transform(lgd.begin(), lgd.end(), notionals_.begin(), 
            lgd.begin(), std::multiplies<Real>());
        std::vector<Real> prob = basket_->remainingProbabilities(d);
        for(Size iName=0; iName<prob.size(); iName++)
            prob[iName] = copula_->inverseCumulativeY(prob[iName], iName);

        // integrate locally (1 factor). 
        // use explicitly a 1D latent model object? 
        Distribution dist(nBuckets_, 0.0, 
            detachAmount_);
            //notional_);
        std::vector<Real> mkft(1, min_ + delta_ /2.);
        for (Size i = 0; i < nSteps_; i++) {
            std::vector<Real> conditionalProbs;
            for(Size iName=0; iName<notionals_.size(); iName++)
                conditionalProbs.push_back(
                copula_->conditionalDefaultProbabilityInvP(prob[iName], iName, 
                    mkft));
            Distribution bld = bucktLDistBuff(lgd, conditionalProbs);
            Real densitydm = delta_ * copula_->density(mkft);
            // also, instead of calling the static method it could be wrapped 
            // through an inlined call in the latent model
            for (Size j = 0; j < nBuckets_; j++)
                dist.addDensity(j, bld.density(j) * densitydm);
            mkft[0] += delta_;
        }
        return dist;
    }


}

#endif
]]></document_content>
  </document>
  <document index="20">
    <source>inhomogeneouspooldef.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_inhomogenous_pool_default_model_hpp
#define quantlib_inhomogenous_pool_default_model_hpp

#include <ql/experimental/credit/lossdistribution.hpp>
#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/math/functional.hpp>

// Intended to replace InhomogeneousPoolCDOEngine in syntheticcdoengines.hpp

namespace QuantLib {

    //-------------------------------------------------------------------------
    //! Default loss distribution convolution for finite non homogeneous pool
    /* A note on the number of buckets: As it is now the code goes splitting
    losses into buckets from loses equal to zero to losses up to the value of
    the underlying basket. This is in view of a stochastic loss given default
    but in a constant LGD situation this is a waste and it is more efficient to
    go up to the attainable losses.
    \todo Extend to the multifactor case for a generic LM
    \todo Many common code with the homogeneous version, both classes perform
    the same work on different loss distribution types, merge and send the 
    distribution object?
    */
    template<class copulaPolicy>
    class InhomogeneousPoolLossModel : public DefaultLossModel {
    private:
      void resetModel() override;

    public:
        // allow base correlations:
        typedef copulaPolicy copulaType;

        InhomogeneousPoolLossModel(
        // restricted to non random recoveries, but it could be possible.
            const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> >& 
                copula,
            Size nBuckets,
            Real max = 5.,
            Real min = -5.,
            Real nSteps = 50)
        : copula_(copula), 
          nBuckets_(nBuckets), 
          max_(max), min_(min), nSteps_(nSteps), delta_((max - min)/nSteps)
        { 
            QL_REQUIRE(copula->numFactors() == 1, 
                "Inhomogeneous model not implemented for multifactor");
        }
    // Write another constructor sending the LM factors and recoveries.
    protected:
        Distribution lossDistrib(const Date& d) const;
    public:
      Real expectedTrancheLoss(const Date& d) const override {
          return lossDistrib(d).cumulativeExcessProbability(attachAmount_, detachAmount_);
          // This one if the distribution is over the whole loss structure:
          // but it becomes very expensive
          /*
          return lossDistrib(d).trancheExpectedValue(
              attachAmount_, detachAmount_);
          */
      }
      Real percentile(const Date& d, Real percentile) const override {
          Real portfLoss = lossDistrib(d).confidenceLevel(percentile);
          return std::min(std::max(portfLoss - attachAmount_, 0.), detachAmount_ - attachAmount_);
      }
      Real expectedShortfall(const Date& d, Probability percentile) const override {
          Distribution dist = lossDistrib(d);
          dist.tranche(attachAmount_, detachAmount_);
          return dist.expectedShortfall(percentile);
      }

    protected:
        const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> > copula_;
        Size nBuckets_;
        mutable Real attach_, detach_, notional_, attachAmount_, detachAmount_;
        mutable std::vector<Real> notionals_;
    private:
        // integration:
        //  \todo move integration to latent model types when moving to a 
        //  multifactor version
        const Real max_;// redundant?
        const Real min_;
        const Real nSteps_;
        const Real delta_; 
    };
    // \todo Add other loss distribution statistics
    typedef InhomogeneousPoolLossModel<GaussianCopulaPolicy> 
        IHGaussPoolLossModel;
    typedef InhomogeneousPoolLossModel<TCopulaPolicy> IHStudentPoolLossModel;

    //-----------------------------------------------------------------------

    template<class CP>
    void InhomogeneousPoolLossModel<CP>::resetModel()
    {
        // need to be capped now since the limit amounts might be over the 
        //  remaining notional (think amortizing)
        attach_ = std::min(basket_->remainingAttachmentAmount() / 
            basket_->remainingNotional(), 1.);
        detach_ = std::min(basket_->remainingDetachmentAmount() / 
            basket_->remainingNotional(), 1.);
        notional_ = basket_->remainingNotional();
        notionals_ = basket_->remainingNotionals();
        attachAmount_ = basket_->remainingAttachmentAmount();
        detachAmount_ = basket_->remainingDetachmentAmount();

        copula_->resetBasket(basket_.currentLink());
    }

    template<class CP>
    Distribution InhomogeneousPoolLossModel<CP>::lossDistrib(
        const Date& d) const 
    {
        LossDistBucketing bucktLDistBuff(nBuckets_, detachAmount_);

        std::vector<Real> lgd;// switch to a mutable cache member
        std::vector<Real> recoveries = copula_->recoveries();
        std::transform(recoveries.begin(), recoveries.end(), 
                       std::back_inserter(lgd),
                       subtract_from<Real>(1.0));
        std::transform(lgd.begin(), lgd.end(), notionals_.begin(), 
                       lgd.begin(), std::multiplies<Real>());
        std::vector<Real> prob = basket_->remainingProbabilities(d);
        for(Size iName=0; iName<prob.size(); iName++)
            prob[iName] = copula_->inverseCumulativeY(prob[iName], iName);

        // integrate locally (1 factor). 
        // use explicitly a 1D latent model object? 
        // \todo Use a library integrator here and in the homogeneous case.
        Distribution dist(nBuckets_, 0.0, 
            detachAmount_);
            //notional_);
        std::vector<Real> mkft(1, min_ + delta_ /2.);
        for (Size i = 0; i < nSteps_; i++) {
            std::vector<Real> conditionalProbs;
            for(Size iName=0; iName<notionals_.size(); iName++)
                conditionalProbs.push_back(
                copula_->conditionalDefaultProbabilityInvP(prob[iName], iName, 
                    mkft));
            Distribution bld = bucktLDistBuff(lgd, conditionalProbs);
            Real densitydm = delta_ * copula_->density(mkft);
            // also, instead of calling the static method it could be wrapped 
            // through an inlined call in the latent model
            for (Size j = 0; j < nBuckets_; j++)
                dist.addDensity(j, bld.density(j) * densitydm);
            mkft[0] += delta_;
        }
        return dist;
    }


}

#endif
]]></document_content>
  </document>
  <document index="21">
    <source>integralcdoengine.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/


#ifndef quantlib_integral_cdo_engine_hpp
#define quantlib_integral_cdo_engine_hpp

#include <ql/qldefines.hpp>

#ifndef QL_PATCH_SOLARIS

#include <ql/experimental/credit/syntheticcdo.hpp>
#    include <utility>

namespace QuantLib {

    class YieldTermStructure;

    class IntegralCDOEngine : public SyntheticCDO::engine {
    public:
      explicit IntegralCDOEngine(Handle<YieldTermStructure> discountCurve,
                                 Period stepSize = 3 * Months)
      : stepSize_(stepSize), discountCurve_(std::move(discountCurve)) {}
      void calculate() const override;

    protected:
      Period stepSize_;
      Handle<YieldTermStructure> discountCurve_;
    };

}

#endif

#endif
]]></document_content>
  </document>
  <document index="22">
    <source>integralntdengine.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_integral_ntd_engine_hpp
#define quantlib_integral_ntd_engine_hpp

#include <ql/experimental/credit/nthtodefault.hpp>
#include <utility>

namespace QuantLib {

    class YieldTermStructure;

    // Varying recoveries allowed, allow now for heterogeneous notionals
    class IntegralNtdEngine : public NthToDefault::engine {
    public:
      IntegralNtdEngine(const Period& integrationStep, Handle<YieldTermStructure> discountCurve)
      : discountCurve_(std::move(discountCurve)), integrationStepSize_(integrationStep) {}
      void calculate() const override;

    protected:
      Handle<YieldTermStructure> discountCurve_;
      Period integrationStepSize_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="23">
    <source>interpolatedaffinehazardratecurve.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2015 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_interpolated_affine_hazard_rate_curve_hpp
#define quantlib_interpolated_affine_hazard_rate_curve_hpp

#include <ql/stochasticprocess.hpp>
#include <ql/experimental/credit/onefactoraffinesurvival.hpp>
#include <ql/termstructures/credit/probabilitytraits.hpp>
#include <ql/termstructures/interpolatedcurve.hpp>
#include <ql/termstructures/bootstraphelper.hpp>
#include <utility>

namespace QuantLib {

    /*! DefaultProbabilityTermStructure based on interpolation of a 
    deterministic hazard rate component plus a stochastic one factor 
    rate.
    */
    /*
    The hazard rate structure here refers to the deterministic term 
    structure added on top of the affine model intensity. It is typically
    employed to match the current market implied probabilities. The total
    probabilities keep their meaning and are those of the affine model. An
    example of this is the CIR++ model as employed in credit.

    (Although this is not usually the preferred way one can instead match the
    model to price the market.)

    Notice that here, hazardRateImpl(Time) returns the deterministic part of
    the hazard rate and not E[\lambda] This is what the bootstrapping
    requires but it might be confusing.

    \todo Redesign?:
    The Affine model type is meant to model short rates; most methods
    if not all still have sense here, though discounts mean probabilities.
    This is not satisfactory, the affine models might need more structure
    or reusing these classes should be reconsidered.
    \todo Implement forward default methods.
    \todo Implement statistics methods (expected values etc)

    */
    /*! \ingroup defaultprobabilitytermstructures */
    template <class Interpolator>
    class InterpolatedAffineHazardRateCurve
        : public OneFactorAffineSurvivalStructure,
          protected InterpolatedCurve<Interpolator> {
      public:
        InterpolatedAffineHazardRateCurve(
            const std::vector<Date>& dates,
            const std::vector<Rate>& hazardRates,
            const DayCounter& dayCounter,
            const ext::shared_ptr<OneFactorAffineModel>& model,
            const Calendar& cal = Calendar(),
            const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
            const std::vector<Date>& jumpDates = std::vector<Date>(),
            const Interpolator& interpolator = Interpolator());
        InterpolatedAffineHazardRateCurve(const std::vector<Date>& dates,
                                          const std::vector<Rate>& hazardRates,
                                          const DayCounter& dayCounter,
                                          const ext::shared_ptr<OneFactorAffineModel>& model,
                                          const Calendar& calendar,
                                          const Interpolator& interpolator);
        InterpolatedAffineHazardRateCurve(const std::vector<Date>& dates,
                                          const std::vector<Rate>& hazardRates,
                                          const DayCounter& dayCounter,
                                          const ext::shared_ptr<OneFactorAffineModel>& model,
                                          const Interpolator& interpolator);
        //! \name TermStructure interface
        //@{
        Date maxDate() const override;
        //@}
        //! \name other inspectors
        //@{
        const std::vector<Time>& times() const;
        const std::vector<Date>& dates() const;
        const std::vector<Real>& data() const;
        const std::vector<Rate>& hazardRates() const;
        std::vector<std::pair<Date, Real> > nodes() const;
        //@}
      protected:
        InterpolatedAffineHazardRateCurve(
            const DayCounter&,
            const ext::shared_ptr<OneFactorAffineModel>& model,
            const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
            const std::vector<Date>& jumpDates = std::vector<Date>(),
            const Interpolator& interpolator = Interpolator());
        InterpolatedAffineHazardRateCurve(
            const Date& referenceDate,
            const DayCounter&,
            const ext::shared_ptr<OneFactorAffineModel>& model,
            const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
            const std::vector<Date>& jumpDates = std::vector<Date>(),
            const Interpolator& interpolator = Interpolator());
        InterpolatedAffineHazardRateCurve(
            Natural settlementDays,
            const Calendar&,
            const DayCounter&,
            const ext::shared_ptr<OneFactorAffineModel>& model,
            const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
            const std::vector<Date>& jumpDates = std::vector<Date>(),
            const Interpolator& interpolator = Interpolator());
        //! \name DefaultProbabilityTermStructure implementation
        //@{
        //! Returns the deterministic hazard rate component.
        Real hazardRateImpl(Time) const override;
        Probability survivalProbabilityImpl(Time) const override;

      public:
        using DefaultProbabilityTermStructure::hazardRate;
    protected:
        /*! Probability of default conditional to the realization of a given
        value of the stochastic part of the hazard rate at a prior time (and
        thus to survival at that time).
        \f$ P_{surv}(\tau>tTarget|F_{tFwd}) \f$
        */
      Probability
      conditionalSurvivalProbabilityImpl(Time tFwd, Time tTarget, Real yVal) const override;
      //@}

      mutable std::vector<Date> dates_;

    private:
      void initialize();
    };


    namespace detail {
        // hazard rate compensation TS for affine models
        const Real minHazardRateComp = -1.0;
    }

    /*! Piecewise (deterministic) plus affine (stochastic) terms composed
        hazard rate
    */
    struct AffineHazardRate {
        // interpolated curve type
        template <class Interpolator>
        struct curve {
            typedef InterpolatedAffineHazardRateCurve<Interpolator> type;
        };
        // helper class
        typedef BootstrapHelper<DefaultProbabilityTermStructure> helper;

        // start of curve data
        static Date initialDate(const DefaultProbabilityTermStructure* c) {
            return c->referenceDate();
        }
        // dummy value at reference date
        static Real initialValue(const DefaultProbabilityTermStructure*) {
            return detail::avgHazardRate;
        }

        // guesses
        template <class C>
        static Real guess(Size i,
                          const C* c,
                          bool validData,
                          Size) // firstAliveHelper
        {
            if (validData) // previous iteration value
                return c->data()[i];

            if (i==1) // first pillar
                return 0.0001;
               // return detail::avgHazardRate;

            // extrapolate
            Date d = c->dates()[i];
            /* Uneasy about the naming: Here we are bootstrapping only the
             deterministic part of the intensity it might be a better idea to
             have a different naming when having these two components.
             What is meant here is the deterministic part of a ++model type
            */
            return c->hazardRate(d, true);
        }

        // constraints
        template <class C>
        static Real minValueAfter(Size i,
                                  const C* c,
                                  bool validData,
                                  Size) // firstAliveHelper
        {
            if (validData) {
                Real r = *(std::min_element(c->data().begin(),
                                            c->data().end()));
                return r/2.0;
            }
            return detail::minHazardRateComp;
            ///return QL_EPSILON;
        }
        template <class C>
        static Real maxValueAfter(Size i,
                                  const C* c,
                                  bool validData,
                                  Size) // firstAliveHelper
        {
            if (validData) {
                Real r = *(std::max_element(c->data().begin(),
                                            c->data().end()));
                return r*2.0;
            }
            // no constraints.
            // We choose as max a value very unlikely to be exceeded.
            return detail::maxHazardRate;
        }
        // update with new guess
        static void updateGuess(std::vector<Real>& data,
                                Real rate,
                                Size i) {
            data[i] = rate;
            if (i==1)
                data[0] = rate; // first point is updated as well
        }
        // upper bound for convergence loop
        static Size maxIterations() { return 30; }
    };


    // inline definitions

    template <class T>
    inline Date InterpolatedAffineHazardRateCurve<T>::maxDate() const {
        return dates_.back();
    }

    template <class T>
    inline const std::vector<Time>&
    InterpolatedAffineHazardRateCurve<T>::times() const {
        return this->times_;
    }

    template <class T>
    inline const std::vector<Date>&
    InterpolatedAffineHazardRateCurve<T>::dates() const {
        return dates_;
    }

    template <class T>
    inline const std::vector<Real>&
    InterpolatedAffineHazardRateCurve<T>::data() const {
        return this->data_;
    }

    template <class T>
    inline const std::vector<Rate>&
    InterpolatedAffineHazardRateCurve<T>::hazardRates() const {
        return this->data_;
    }

    template <class T>
    inline std::vector<std::pair<Date, Real> >
    InterpolatedAffineHazardRateCurve<T>::nodes() const {
        std::vector<std::pair<Date, Real> > results(dates_.size());
        for (Size i=0; i<dates_.size(); ++i)
            results[i] = std::make_pair(dates_[i], this->data_[i]);
        return results;
    }

    #ifndef __DOXYGEN__

    // template definitions

    template <class T>
    Real InterpolatedAffineHazardRateCurve<T>::hazardRateImpl(Time t) const {
        if (t <= this->times_.back())
            return this->interpolation_(t, true);

        // deterministic flat hazard rate extrapolation
        return this->data_.back();
    }

    // notice it is rewritten and no call is made to hazardRateImpl
    template <class T>
    Probability
    InterpolatedAffineHazardRateCurve<T>::survivalProbabilityImpl(
        Time t) const 
    {
        // the way x0 is defined:
        Real initValHR = std::pow(model_->dynamics()->process()->x0(), 2);

        if (t == 0.0)
            return model_->discountBond(0., t, initValHR);

        Real integral;
        if (t <= this->times_.back()) {
            integral = this->interpolation_.primitive(t, true);
        } else {
            // flat hazard rate extrapolation
            integral = 
                this->interpolation_.primitive(this->times_.back(), true)
                     + this->data_.back()*(t - this->times_.back());
        }
        return std::exp(-integral) * model_->discountBond(0., t, initValHR);
    }

    template <class T>
    Probability
    InterpolatedAffineHazardRateCurve<T>::conditionalSurvivalProbabilityImpl(
        Time tFwd, Time tTarget, Real yVal) const 
    {
        QL_REQUIRE(tFwd <= tTarget, "Probability time in the past.");
        // Still leaves the possibility of sending tFwd=0 and an yVal different
        //   to the initial conditions. In an abstract sense thats all right as
        //   long as it is seen as a zero probability scenario.
        #if defined(QL_EXTRA_SAFETY_CHECKS)
            QL_REQUIRE(tFwd > 0. || yVal == 
                model_->dynamics()->process()->x0(), 
                "Initial value different to process'.");
        #endif
        if (tFwd == 0.) return survivalProbabilityImpl(tTarget);
        if (tFwd - tTarget == 0.0)
            return 1.;

        Real integralTFwd, integralTP;
        if (tFwd <= this->times_.back()) {
            integralTFwd = this->interpolation_.primitive(tFwd, true);
        } else {
            // flat hazard rate extrapolation
            integralTFwd = 
                this->interpolation_.primitive(this->times_.back(), true)
                     + this->data_.back()*(tFwd - this->times_.back());
        }
        if (tTarget <= this->times_.back()) {
            integralTP = this->interpolation_.primitive(tTarget, true);
        } else {
            // flat hazard rate extrapolation
            integralTP = 
                this->interpolation_.primitive(this->times_.back(), true)
                     + this->data_.back()*(tTarget - this->times_.back());
        }

        return std::exp(-(integralTP-integralTFwd)) * 
            model_->discountBond(tFwd, tTarget, yVal );
    }

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const std::vector<Handle<Quote> >& jumps,
        const std::vector<Date>& jumpDates,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(model, dayCounter, jumps, jumpDates), InterpolatedCurve<T>(
                                                                                 interpolator) {}

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        const Date& referenceDate,
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const std::vector<Handle<Quote> >& jumps,
        const std::vector<Date>& jumpDates,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(
          model, referenceDate, Calendar(), dayCounter, jumps, jumpDates),
      InterpolatedCurve<T>(interpolator) {}

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        Natural settlementDays,
        const Calendar& calendar,
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const std::vector<Handle<Quote> >& jumps,
        const std::vector<Date>& jumpDates,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(
          model, settlementDays, calendar, dayCounter, jumps, jumpDates),
      InterpolatedCurve<T>(interpolator) {}

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        const std::vector<Date>& dates,
        const std::vector<Rate>& hazardRates,
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const Calendar& calendar,
        const std::vector<Handle<Quote> >& jumps,
        const std::vector<Date>& jumpDates,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(model, dates.at(0), calendar, dayCounter, jumps, jumpDates),
      InterpolatedCurve<T>(std::vector<Time>(), hazardRates, interpolator), dates_(dates) {
        initialize();
    }

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        const std::vector<Date>& dates,
        const std::vector<Rate>& hazardRates,
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const Calendar& calendar,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(model, dates.at(0), calendar, dayCounter),
      InterpolatedCurve<T>(std::vector<Time>(), hazardRates, interpolator), dates_(dates) {
        initialize();
    }

    template <class T>
    InterpolatedAffineHazardRateCurve<T>::InterpolatedAffineHazardRateCurve(
        const std::vector<Date>& dates,
        const std::vector<Rate>& hazardRates,
        const DayCounter& dayCounter,
        const ext::shared_ptr<OneFactorAffineModel>& model,
        const T& interpolator)
    : OneFactorAffineSurvivalStructure(model, dates.at(0), Calendar(), dayCounter),
      InterpolatedCurve<T>(std::vector<Time>(), hazardRates, interpolator), dates_(dates) {
        initialize();
    }

    template <class T>
    void InterpolatedAffineHazardRateCurve<T>::initialize()
    {
        QL_REQUIRE(dates_.size() >= T::requiredPoints,
                   "not enough input dates given");
        QL_REQUIRE(this->data_.size() == dates_.size(),
                   "dates/data count mismatch");

        this->times_.resize(dates_.size());
        this->times_[0] = 0.0;
        for (Size i=1; i<dates_.size(); ++i) {
            QL_REQUIRE(dates_[i] > dates_[i-1],
                       "invalid date (" << dates_[i] << ", vs "
                       << dates_[i-1] << ")");
            this->times_[i] = dayCounter().yearFraction(dates_[0], dates_[i]);
            QL_REQUIRE(!close(this->times_[i], this->times_[i-1]),
                       "two dates correspond to the same time "
                       "under this curve's day count convention");
        }

        this->interpolation_ =
            this->interpolator_.interpolate(this->times_.begin(),
                                            this->times_.end(),
                                            this->data_.begin());
        this->interpolation_.update();
    }

    #endif

}

#endif
]]></document_content>
  </document>
  <document index="24">
    <source>issuer.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008, 2009 StatPro Italia srl
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file issuer.hpp
    \brief Classes for credit-name handling.
*/

#ifndef quantlib_issuer_hpp
#define quantlib_issuer_hpp

#include <ql/experimental/credit/defaultevent.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <set>
#include <vector>

namespace QuantLib {

    typedef std::set<ext::shared_ptr<DefaultEvent>,
                earlier_than<ext::shared_ptr<DefaultEvent> > >
            DefaultEventSet;

    class Issuer {
      public:
        typedef std::pair<DefaultProbKey,
                          Handle<DefaultProbabilityTermStructure> >
                                                               key_curve_pair;
        /*! The first argument represents the probability of an issuer
            of having any of its bonds with the given seniority,
            currency incurring in that particular event.  The second
            argument represents the history of past events.  Theres no
            check on whether the event list makes sense, events can
            occur several times and several of them can take place on
            the same date.

            To do: add settlement event access
        */
        Issuer(std::vector<key_curve_pair> probabilities = std::vector<key_curve_pair>(),
               DefaultEventSet events = DefaultEventSet());

        Issuer(const std::vector<std::vector<ext::shared_ptr<DefaultType> > >& eventTypes,
               const std::vector<Currency>& currencies,
               const std::vector<Seniority>& seniorities,
               const std::vector<Handle<DefaultProbabilityTermStructure> >& curves,
               DefaultEventSet events = DefaultEventSet());

        //! \name Inspectors
        //@{
        const Handle<DefaultProbabilityTermStructure>&
            defaultProbability(const DefaultProbKey& key) const;

        //@}

        //! \name Utilities
        //@{
        //! If a default event with the required seniority and
        //    restructuring type is found, it is returned for
        //    inspection; otherwise, the method returns an empty pointer.
        ext::shared_ptr<DefaultEvent>
        defaultedBetween(const Date& start,
                         const Date& end,
                         const DefaultProbKey& key,
                         bool includeRefDate = false
                         ) const;

        //@}
        std::vector<ext::shared_ptr<DefaultEvent> >
        defaultsBetween(const Date& start,
                        const Date& end,
                        const DefaultProbKey& contractKey,
                        bool includeRefDate
                        ) const ;
      private:
        //! probabilities of events for each bond collection
        // vector of pairs preferred over maps for performance
        std::vector<std::pair<DefaultProbKey,
            Handle<DefaultProbabilityTermStructure> > > probabilities_;
        //! History of past events affecting this issuer. Notice it is possible
        //    for the same event to occur on the same bond several times along
        //    time.
        DefaultEventSet events_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="25">
    <source>loss.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file loss.hpp
  \brief Pair of loss time and amount, sortable by loss time
*/

#ifndef quantlib_loss_hpp
#define quantlib_loss_hpp

#include <ql/types.hpp>

namespace QuantLib {

class Loss {
    public:
        Loss(Real t = 0.0, Real a = 0.0) : time(t), amount(a) {};
        Real time, amount;
    };

    inline bool operator<(const Loss& l1, const Loss& l2) {
        return (l1.time < l2.time);
    }
    inline bool operator>(const Loss& l1, const Loss& l2) {
        return (l1.time > l2.time);
    }
    inline bool operator==(const Loss& l1, const Loss& l2) {
        return (l1.time == l2.time);
    }
    inline bool operator!=(const Loss& l1, const Loss& l2) {
        return (l1.time != l2.time);
    }

}

#endif
]]></document_content>
  </document>
  <document index="26">
    <source>lossdistribution.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file lossdistribution.hpp
    \brief Loss distributions and probability of n defaults
*/

#ifndef quantlib_loss_distribution_hpp
#define quantlib_loss_distribution_hpp

#include <ql/math/distributions/binomialdistribution.hpp>
#include <ql/experimental/credit/distribution.hpp>
#include <ql/experimental/credit/onefactorcopula.hpp>

namespace QuantLib {

    //! Probability formulas and algorithms
    /*!
      \ingroup probability
    */ 
    class LossDist {
    public:
      LossDist() = default;
      virtual ~LossDist() = default;

      virtual Distribution operator()(const std::vector<Real>& volumes,
                                      const std::vector<Real>& probabilities) const = 0;
      virtual Size buckets() const = 0;
      virtual Real maximum() const = 0;

      /*! Binomial probability of n defaults using prob[0]
       */
      static Real binomialProbabilityOfNEvents(int n, std::vector<Real>& p);

      /*! Binomial probability of at least n defaults using prob[0]
       */
      static Real binomialProbabilityOfAtLeastNEvents(int n, std::vector<Real>& p);
      /*! Probability of exactly n default events
        Xiaofong Ma, "Numerical Methods for the Valuation of Synthetic
        Collateralized Debt Obligations", PhD Thesis,
        Graduate Department of Computer Science, University of Toronto, 2007
        http://www.cs.toronto.edu/pub/reports/na/ma-07-phd.pdf (formula 2.1)
      */
      static std::vector<Real> probabilityOfNEvents(std::vector<Real>& p);

      static Real probabilityOfNEvents(int n, std::vector<Real>& p);

      /*! Probability of at least n defaults
       */
      static Real probabilityOfAtLeastNEvents(int n, std::vector<Real>& p);
    }; 

    //! Probability of N events 
    class ProbabilityOfNEvents {
    public:
        explicit ProbabilityOfNEvents (int n) : n_(n) {}
        Real operator()(std::vector<Real> p) const;
    private:
        Size n_;
    };

    //! Probability of at least N events 
    class ProbabilityOfAtLeastNEvents {
    public:
        explicit ProbabilityOfAtLeastNEvents (int n) : n_(n) {}
        Real operator()(std::vector<Real> p) const;
    private:
        Size n_;
    };

    //! Probability of at least N events 
    class BinomialProbabilityOfAtLeastNEvents {
    public:
        explicit BinomialProbabilityOfAtLeastNEvents(int n) : n_(n) {}
        Real operator()(std::vector<Real> p) const;

      private:
        int n_;
    };

    //! Binomial loss distribution
    /*! Binomial loss distribution
      \ingroup probability
    */
    class LossDistBinomial : public LossDist {
    public:
        LossDistBinomial (Size nBuckets, Real maximum)
            : nBuckets_(nBuckets), maximum_(maximum) {}
        Distribution operator()(Size n, Real volume, Real probability) const;
        Distribution operator()(const std::vector<Real>& volumes,
                                const std::vector<Real>& probabilities) const override;
        Size buckets() const override { return nBuckets_; }
        Real maximum() const override { return maximum_; }
        Real volume() const { return volume_; }
        Size size () const { return n_; }
        std::vector<Real> probability() const { return probability_; }
        std::vector<Real> excessProbability() const { return excessProbability_; }
    private:
        Size nBuckets_;
        Real maximum_;
        mutable Real volume_;
        mutable Size n_;
        mutable std::vector<Real> probability_;
        mutable std::vector<Real> excessProbability_;
    };

    //! Loss Distribution for Homogeneous Pool
    /*! Loss Distribution for Homogeneous Pool

      Loss distribution for equal volumes but varying probabilities of 
      default.

      The method builds the exact loss distribution for a homogeneous pool
      of underlyings iteratively by computing the convolution of the given
      loss distribution with the "loss distribution" of an additional credit
      following 
      
      Xiaofong Ma, "Numerical Methods for the Valuation of Synthetic
      Collateralized Debt Obligations", PhD Thesis, 
      Graduate Department of Computer Science, University of Toronto, 2007  
      http://www.cs.toronto.edu/pub/reports/na/ma-07-phd.pdf (formula 2.1)

      avoiding numerical instability of the algorithm by

      John Hull and Alan White, "Valuation of a CDO and nth to default CDS 
      without Monte Carlo simulation", Journal of Derivatives 12, 2, 2004 

      \ingroup probability
     */
    class LossDistHomogeneous : public LossDist {
    public:
        LossDistHomogeneous (Size nBuckets, Real maximum)
            : nBuckets_(nBuckets), maximum_(maximum),
              n_(0), volume_(0.0) {}
        Distribution operator()(Real volume, 
                                const std::vector<Real>& probabilities) const;
        Distribution operator()(const std::vector<Real>& volumes,
                                const std::vector<Real>& probabilities) const override;
        Size buckets() const override { return nBuckets_; }
        Real maximum() const override { return maximum_; }
        Size size () const { return n_; }
        Real volume() const { return volume_; }
        std::vector<Real> probability() const { return probability_; }
        std::vector<Real> excessProbability() const { return excessProbability_; }
    private:
        Size nBuckets_;
        Real maximum_;
        mutable Size n_;
        mutable Real volume_;
        mutable std::vector<Real> probability_;
        mutable std::vector<Real> excessProbability_;
    };

    //! Loss distribution with Hull-White bucketing 
    /*! Loss distribution with Hull-White bucketing 

      Loss distribution for varying volumes and probabilities of default, 
      independence assumed.

      The implementation of the loss distribution follows 

      John Hull and Alan White, "Valuation of a CDO and nth to default CDS 
      without Monte Carlo simulation", Journal of Derivatives 12, 2, 2004. 

      \ingroup probability
    */
    class LossDistBucketing : public LossDist {
    public:
        LossDistBucketing (Size nBuckets, Real maximum, 
                           Real epsilon = 1e-6)
            : nBuckets_(nBuckets), maximum_(maximum), epsilon_(epsilon) {}
        Distribution operator()(const std::vector<Real>& volumes,
                                const std::vector<Real>& probabilities) const override;
        Size buckets() const override { return nBuckets_; }
        Real maximum() const override { return maximum_; }

      private:
        int locateTargetBucket (Real loss, Size i0 = 0) const;

        Size nBuckets_;
        Real maximum_;
        Real epsilon_;
    };

    //! Loss distribution with Monte Carlo simulation
    /*!
      Loss distribution for varying volumes and probabilities of default
      via Monte Carlo simulation of independent default events.

      \ingroup probability
    */
    class LossDistMonteCarlo : public LossDist {
    public:
        LossDistMonteCarlo (Size nBuckets, Real maximum, Size simulations,
                            long seed = 42, Real epsilon = 1e-6)
            : nBuckets_(nBuckets), maximum_(maximum), 
              simulations_(simulations), seed_(seed), epsilon_(epsilon) {}
        Distribution operator()(const std::vector<Real>& volumes,
                                const std::vector<Real>& probabilities) const override;
        Size buckets() const override { return nBuckets_; }
        Real maximum() const override { return maximum_; }

      private:
        Size nBuckets_;
        Real maximum_;
        Size simulations_;
        long seed_;
        Real epsilon_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="27">
    <source>midpointcdoengine.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_midpoint_cdo_engine_hpp
#define quantlib_midpoint_cdo_engine_hpp

#include <ql/qldefines.hpp>

#ifndef QL_PATCH_SOLARIS

#include <ql/experimental/credit/syntheticcdo.hpp>
#    include <utility>

namespace QuantLib {

    class YieldTermStructure;

    //! CDO base engine taking schedule steps

    /* The engine obtains the cdo reference basket from its arguments and it 
    is expecting it to have a default model assigned. 
    */
    /* FIX ME: ASSUMES basket->expectedTrancheLoss(endDate) includes past 
    realized losses (between cdo inception and calculation time) .... what if 
    basket inception is not the same as CDO's ?????

    \todo non tested under realized defaults. JTD metrics might be invalid
    */
    class MidPointCDOEngine : public SyntheticCDO::engine {
    public:
      explicit MidPointCDOEngine(Handle<YieldTermStructure> discountCurve)
      : discountCurve_(std::move(discountCurve)) {}
      void calculate() const override;

    protected:
      Handle<YieldTermStructure> discountCurve_;
    };

}

#endif

#endif
]]></document_content>
  </document>
  <document index="28">
    <source>nthtodefault.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file nthtodefault.hpp
    \brief N-th to default swap
*/

#ifndef quantlib_nth_to_default_hpp
#define quantlib_nth_to_default_hpp

#include <ql/instrument.hpp>
#include <ql/cashflow.hpp>
#include <ql/default.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/time/schedule.hpp>

namespace QuantLib {

    class YieldTermStructure;
    class Claim;
    class Basket;

    //--------------------------------------------------------------------------
    //! N-th to default swap
    /*! A NTD instrument exchanges protection against the nth default
        in a basket of underlying credits for premium payments based
        on the protected notional amount.

        The pricing is analogous to the pricing of a CDS instrument
        which represents protection against default of a single
        underlying credit.  The only difference is the calculation of
        the probability of default.  In the CDS case, it is the
        probabilty of single name default; in the NTD case the
        probability of at least N defaults in the portfolio of
        underlying credits.

        This probability is computed using the algorithm in
        John Hull and Alan White, "Valuation of a CDO and nth to
        default CDS without Monte Carlo simulation", Journal of
        Derivatives 12, 2, 2004.

        The algorithm allows for varying probability of default across
        the basket. Otherwise, for identical probabilities of default,
        the probability of n defaults is given by the binomial
        distribution.

        Default correlation is modeled using a one-factor Gaussian copula
        approach.

        The class is tested against data in Hull-White (see reference
        above.)
    */
    class NthToDefault : public Instrument {
      public:
        class arguments;
        class results;
        class engine;

        //! This product is 'digital'; the basket might be tranched but this is 
        //  not relevant to it.
        NthToDefault(const ext::shared_ptr<Basket>& basket,
                Size n,
                Protection::Side side,
                const Schedule& premiumSchedule,
                Rate upfrontRate,
                Rate premiumRate,
                const DayCounter& dayCounter,
                Real nominal,
                bool settlePremiumAccrual);

        bool isExpired() const override;

        // inspectors
        Rate premium() const { return premiumRate_; }
        Real nominal() const { return nominal_; }
        DayCounter dayCounter() const { return dayCounter_; }
        Protection::Side side() const { return side_; }
        Size rank() const { return n_; }
        Size basketSize() const;

        const Date& maturity() const {return premiumSchedule_.endDate();}//???

        const ext::shared_ptr<Basket>& basket() const {return basket_;}

        // results
        Rate fairPremium() const;
        Real premiumLegNPV() const;
        Real protectionLegNPV() const;
        Real errorEstimate() const;

        void setupArguments(PricingEngine::arguments*) const override;
        void fetchResults(const PricingEngine::results*) const override;

      private:
        void setupExpired() const override;

        ext::shared_ptr<Basket> basket_;
        Size n_;
        Protection::Side side_;
        Real nominal_;
        Schedule premiumSchedule_;
        Rate premiumRate_;
        Rate upfrontRate_;
        DayCounter dayCounter_;
        bool settlePremiumAccrual_;

        Leg premiumLeg_;/////////////////// LEG AND SCHEDULE BOTH MEMBERS..... REVISE THIS!

        // results
        mutable Rate premiumValue_;
        mutable Real protectionValue_;
        mutable Real upfrontPremiumValue_;
        mutable Real fairPremium_;
        mutable Real errorEstimate_;
    };



    class NthToDefault::arguments : public virtual PricingEngine::arguments {
    public:
        arguments() : side(Protection::Side(-1)),
                      premiumRate(Null<Real>()),
                      upfrontRate(Null<Real>()) {}
        void validate() const override;

        ext::shared_ptr<Basket> basket;
        Protection::Side side;
        Leg premiumLeg;

        Size ntdOrder;
        bool settlePremiumAccrual;
        Real notional;// ALL NAMES WITH THE SAME WEIGHT, NOTIONAL IS NOT MAPPED TO THE BASKET HERE, this does not have to be that way, its perfectly possible to have irreg notionals...
        Real premiumRate;
        Rate upfrontRate;
    };

    class NthToDefault::results : public Instrument::results {
    public:
      void reset() override;
      Real premiumValue;
      Real protectionValue;
      Real upfrontPremiumValue;
      Real fairPremium;
      Real errorEstimate;
    };

    //! NTD base engine
    class NthToDefault::engine : 
        public GenericEngine<NthToDefault::arguments, 
                             NthToDefault::results> { };

}

#endif
]]></document_content>
  </document>
  <document index="29">
    <source>onefactoraffinesurvival.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2015 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef onefactor_affine_survival_hpp
#define onefactor_affine_survival_hpp

#include <ql/models/shortrate/onefactormodel.hpp>
#include <ql/stochasticprocess.hpp>
#include <ql/termstructures/credit/hazardratestructure.hpp>
#include <utility>

namespace QuantLib {
    
    /*! Survival probability term structure based on a one factor stochastic
    model of the default intensity.
    */
    /*
    While deriving from the hazard rate class the HRTS refers only to the
    deterministic part of the model. The probabilities depend on this 
    component and the stochastic part and are rewritten here.
    Derived classes need to specify the deterministic part 
    of the hazard rate if any (the one returned by 'hazardRateImpl'). It
    is needed for the conditional/forward probabilities.
     */
    class OneFactorAffineSurvivalStructure 
        : public HazardRateStructure {
    public:
        // implement remaining constructors.....
      explicit OneFactorAffineSurvivalStructure(
          ext::shared_ptr<OneFactorAffineModel> model,
          const DayCounter& dayCounter = DayCounter(),
          const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
          const std::vector<Date>& jumpDates = std::vector<Date>())
      : HazardRateStructure(dayCounter, jumps, jumpDates), model_(std::move(model)) {}

      OneFactorAffineSurvivalStructure(
          ext::shared_ptr<OneFactorAffineModel> model,
          const Date& referenceDate,
          const Calendar& cal = Calendar(),
          const DayCounter& dayCounter = DayCounter(),
          const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
          const std::vector<Date>& jumpDates = std::vector<Date>())
      : HazardRateStructure(referenceDate, Calendar(), dayCounter, jumps, jumpDates),
        model_(std::move(model)) {}

      OneFactorAffineSurvivalStructure(
          ext::shared_ptr<OneFactorAffineModel> model,
          Natural settlementDays,
          const Calendar& calendar,
          const DayCounter& dayCounter = DayCounter(),
          const std::vector<Handle<Quote> >& jumps = std::vector<Handle<Quote> >(),
          const std::vector<Date>& jumpDates = std::vector<Date>())
      : HazardRateStructure(settlementDays, calendar, dayCounter, jumps, jumpDates),
        model_(std::move(model)) {}

      //! \name TermStructure interface
      //@{
      // overwrite on mkt models (e.g. bootstraps)
      Date maxDate() const override { return Date::maxDate(); }

      /* Notice this is not calling hazard rate methods, these are
         stochastic now.
      */
      /*!
        Returns the probability at a future time dTgt, conditional to
        survival at a prior time dFwd and to the realization of a particular
        hazard rate value at dFwd.
        \param dFwd Time of the forward survival calculation and HR
                    realization.
        \param dTgt Target time of survival probability.
        \param yVal Realized value of the HR at time dFwd.
        \param extrapolate Allow curve extrapolation.
        \return Survival probability.

        \todo turn into a protected method to be called by
              defaults and survivals
      */
      /*
        Note: curve extrapolation has a different meaning on different curve
          types; for matched to market structures the credit market curves
          would be requested for extrapolation; for affine models on top of
          a static term structure it is this one that will be required for
          extrapolation.
       */
      Probability conditionalSurvivalProbability(const Date& dFwd,
                                                 const Date& dTgt,
                                                 Real yVal,
                                                 bool extrapolate = false) const {
          return conditionalSurvivalProbability(timeFromReference(dFwd), timeFromReference(dTgt),
                                                yVal, extrapolate); 
        }
        Probability conditionalSurvivalProbability(
                Time tFwd, Time tgt, Real yVal,
                bool extrapolate = false) const
        {
            #if defined(QL_EXTRA_SAFETY_CHECKS)
                QL_REQUIRE(tgt >= tFwd, "Incorrect dates ordering.");
            #endif
            checkRange(tFwd, extrapolate);
            checkRange(tgt, extrapolate);
            
            // \todo ADD JUMPS TREATMENT

            return conditionalSurvivalProbabilityImpl(tFwd, tgt, yVal);
        }
        //@}
        // DefaultTermStructure interface
        using DefaultProbabilityTermStructure::hazardRate;
        Rate hazardRate(Time t, bool extrapolate = false) const {
            checkRange(t, extrapolate);
            return hazardRateImpl(t);
        }
    protected:
        //! \name DefaultProbabilityTermStructure implementation
        //@{
      Probability survivalProbabilityImpl(Time) const override;
      Real defaultDensityImpl(Time) const override;
      //@}
      // avoid call super
      // \todo addd date overload
      virtual Probability conditionalSurvivalProbabilityImpl(Time tFwd, Time tgt, Real yVal) const;

      // HazardRateStructure interface
      Real hazardRateImpl(Time) const override {
          // no deterministic component
          return 0.;
      }

        ext::shared_ptr<OneFactorAffineModel> model_;        
    };
    
    inline Probability
        OneFactorAffineSurvivalStructure::survivalProbabilityImpl(
        Time t) const
    {
        Real initValHR =
            model_->dynamics()->shortRate(0., 
                model_->dynamics()->process()->x0());

        return model_->discountBond(0., t, initValHR);
    }

    inline Probability
        OneFactorAffineSurvivalStructure::conditionalSurvivalProbabilityImpl(
            Time tFwd, Time tgt, Real yVal) const {
        return model_->discountBond(tFwd, tgt, yVal);
    }

    inline Real 
        OneFactorAffineSurvivalStructure::defaultDensityImpl(Time t) const {
        Real initValHR = 
            model_->dynamics()->shortRate(0., 
                model_->dynamics()->process()->x0());;

        return hazardRateImpl(t)*survivalProbabilityImpl(t) /
            model_->discountBond(0., t, initValHR);
    }
}

#endif
]]></document_content>
  </document>
  <document index="30">
    <source>onefactorcopula.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file onefactorcopula.hpp
    \brief One-factor copula base class
*/

#ifndef quantlib_one_factor_copula_hpp
#define quantlib_one_factor_copula_hpp

#include <ql/experimental/credit/distribution.hpp>
#include <ql/patterns/lazyobject.hpp>
#include <ql/quote.hpp>
#include <utility>

namespace QuantLib {

    //! Abstract base class for one-factor copula models
    /*! Reference: John Hull and Alan White, The Perfect Copula, June 2006

        Let \f$Q_i(t)\f$ be the cumulative probability of default of
        counterparty i before time t.

        In a one-factor model, consider random variables
        \f[ Y_i = a_i\,M+\sqrt{1-a_i^2}\:Z_i \f]
        where \f$M\f$ and \f$Z_i\f$ have independent zero-mean
        unit-variance distributions and \f$-1\leq a_i \leq 1\f$.  The
        correlation between \f$Y_i\f$ and \f$Y_j\f$ is then
        \f$a_i a_j\f$.

        Let \f$F_Y(y)\f$ be the cumulative distribution function of \f$Y_i\f$.
        \f$y\f$ is mapped to \f$t\f$ such that percentiles match, i.e.
        \f$F_Y(y)=Q_i(t)\f$ or \f$y=F_Y^{-1}(Q_i(t))\f$.

        Now let \f$F_Z(z)\f$ be the cumulated distribution function of
        \f$Z_i\f$.  For given realization of \f$M\f$, this determines
        the distribution of \f$y\f$:
        \f[
        Prob \,(Y_i < y|M) = F_Z \left( \frac{y-a_i\,M}{\sqrt{1-a_i^2}}\right)
        \qquad
        \mbox{or}
        \qquad
        Prob \,(t_i < t|M) = F_Z \left( \frac{F_Y^{-1}(Q_i(t))-a_i\,M}
        {\sqrt{1-a_i^2}}
        \right)
        \f]

        The distribution functions of \f$ M, Z_i \f$ are specified in
        derived classes. The distribution function of \f$ Y \f$ is
        then given by the convolution
        \f[
        F_Y(y) = Prob\,(Y<y) = \int_{-\infty}^\infty\,\int_{-\infty}^{\infty}\:
        D_Z(z)\,D_M(m) \quad
        \Theta \left(y - a\,m - \sqrt{1-a^2}\,z\right)\,dm\,dz,
        \qquad
        \Theta (x) = \left\{
        \begin{array}{ll}
        1 & x \geq 0 \\
        0 & x < 0
        \end{array}\right.
        \f]
        where \f$ D_Z(z) \f$ and \f$ D_M(m) \f$ are the probability
        densities of \f$ Z\f$ and \f$ M, \f$ respectively.

        This convolution can also be written
        \f[
        F(y) = Prob \,(Y < y) =
        \int_{-\infty}^\infty D_M(m)\,dm\:
        \int_{-\infty}^{g(y,a,m)} D_Z(z)\,dz, \qquad
        g(y,a,m) = \frac{y - a\cdot m}{\sqrt{1-a^2}}, \qquad a < 1
        \f]

        or

        \f[
        F(y) = Prob \,(Y < y) =
        \int_{-\infty}^\infty D_Z(z)\,dz\:
        \int_{-\infty}^{h(y,a,z)} D_M(m)\,dm, \qquad
        h(y,a,z) = \frac{y - \sqrt{1 - a^2}\cdot z}{a}, \qquad a > 0.
        \f]

        In general, \f$ F_Y(y) \f$ needs to be computed numerically.

        \todo Improve on simple Euler integration
    */
    class OneFactorCopula : public LazyObject {
      public:
        OneFactorCopula(Handle<Quote> correlation,
                        Real maximum = 5.0,
                        Size integrationSteps = 50,
                        Real minimum = -5.0)
        : correlation_(std::move(correlation)), max_(maximum), steps_(integrationSteps),
          min_(minimum) {
            QL_REQUIRE(correlation_->value() >= -1
                       && correlation_->value() <= 1,
                       "correlation out of range [-1, +1]");
            registerWith(correlation_);
        }

        //! Density function of M.
        /*! Derived classes must override this method and ensure zero
            mean and unit variance.
        */
        virtual Real density(Real m) const = 0;
        //! Cumulative distribution of Z.
        /*! Derived classes must override this method and ensure zero
            mean and unit variance.
        */
        virtual Real cumulativeZ(Real z) const = 0;
        //! Cumulative distribution of Y.
        /*! This is the default implementation based on tabulated
            data. The table needs to be filled by derived classes. If
            analytic calculation is feasible, this method can also be
            overridden.
        */
        virtual Real cumulativeY(Real y) const;
        //! Inverse cumulative distribution of Y.
        /*! This is the default implementation based on tabulated
            data. The table needs to be filled by derived classes. If
            analytic calculation is feasible, this method can also be
            overridden.
        */
        virtual Real inverseCumulativeY(Real p) const;

        //! Single correlation parameter
        Real correlation() const;

        //! Conditional probability
        /*! \f[
            \hat p(m) = F_Z \left( \frac{F_Y^{-1}(p)-a\,m}{\sqrt{1-a^2}}\right)
            \f]
        */
        Real conditionalProbability(Real prob,
                                    Real m) const;

        //! Vector of conditional probabilities
        /*! \f[
            \hat p_i(m) = F_Z \left( \frac{F_Y^{-1}(p_i)-a\,m}{\sqrt{1-a^2}}
            \right)
            \f]
        */
        std::vector<Real> conditionalProbability(const std::vector<Real>& prob,
                                                 Real m) const;

        /*! Integral over the density \f$ \rho(m) \f$ of M and the conditional
            probability related to p:

            \f[
            \int_{-\infty}^\infty\,dm\,\rho(m)\,
            F_Z \left( \frac{F_Y^{-1}(p)-a\,m}{\sqrt{1-a^2}}\right)
            \f]
        */
        Real integral(Real p) const {
            QL_REQUIRE(p >= 0 && p <= 1, "probability p=" << p
                       << " out of range [0,1]");
            calculate();

            Real avg = 0;
            for (Size k = 0; k < steps(); k++) {
                Real pp = conditionalProbability(p, m(k));
                avg += pp * densitydm(k);
            }
            return avg;
        }

        /*! Integral over the density \f$ \rho(m) \f$ of M and a
            one-dimensional function \f$ f \f$ of conditional
            probabilities related to the input vector of probabilities p:

            \f[
            \int_{-\infty}^\infty\,dm\,\rho(m)\, f (\hat p_1, \hat p_2, \dots,
            \hat p_N), \qquad
            \hat p_i (m) = F_Z \left( \frac{F_Y^{-1}(p_i)-a\,m}{\sqrt{1-a^2}}
            \right)
            \f]
        */
        template <class F>
        Real integral(const F& f, std::vector<Real>& probabilities) const {
            calculate();

            Real avg = 0.0;
            for (Size i = 0; i < steps_; i++) {
                std::vector<Real> conditional
                    = conditionalProbability(probabilities, m(i));
                Real prob = f(conditional);
                avg += prob * densitydm(i);
            }
            return avg;
        }

        /*! Integral over the density \f$ \rho(m) \f$ of M and a
            multi-dimensional function \f$ f \f$ of conditional
            probabilities related to the input vector of probabilities p:

            \f[
            \int_{-\infty}^\infty\,dm\,\rho(m)\, f (\hat p_1, \hat p_2, \dots,
            \hat p_N), \qquad
            \hat p_i = F_Z \left( \frac{F_Y^{-1}(p_i)-a\,m}{\sqrt{1-a^2}}\right)
            \f]
        */
        template <class F>
        Distribution integral(const F& f,
                              const std::vector<Real>& nominals,
                              const std::vector<Real>& probabilities) const {
            calculate();

            Distribution dist(f.buckets(), 0.0, f.maximum());
            for (Size i = 0; i < steps(); i++) {
                std::vector<Real> conditional
                    = conditionalProbability(probabilities, m(i));
                Distribution d = f(nominals, conditional);
                for (Size j = 0; j < dist.size(); j++)
                    dist.addDensity(j, d.density(j) * densitydm(i));
            }
            return dist;
        }

        /*! Check moments (unit norm, zero mean and unit variance) of
            the distributions of M, Z, and Y by numerically
            integrating the respective density.  Parameter tolerance
            is the maximum tolerable absolute error.
        */
        int checkMoments(Real tolerance) const;

      protected:
        Handle<Quote> correlation_;
        mutable Real max_;
        mutable Size steps_;
        mutable Real min_;

        // Tabulated numerical solution of the cumulated distribution of Y
        mutable std::vector<Real> y_;
        mutable std::vector<Real> cumulativeY_;

        //private:
        // utilities for simple Euler integrations over the density of M
        Size steps() const;

        // i not used yet, might allow varying grid size
        // for the copula integration in the future
        Real dm(Size i) const;

        Real m(Size i) const;
        Real densitydm(Size i) const;
    };

    inline Real OneFactorCopula::correlation() const {
        calculate();
        return correlation_->value();
    }

    inline Size OneFactorCopula::steps() const {
        return steps_;
    }

    inline Real OneFactorCopula::dm(Size) const {
        return (max_ - min_)/ steps_;
    }

    inline Real OneFactorCopula::m(Size i) const {
        QL_REQUIRE(i < steps_, "index out of range");
        return min_ + dm(i) * i + dm(i) / 2;
    }

    inline Real OneFactorCopula::densitydm(Size i) const {
        QL_REQUIRE(i < steps_, "index out of range");
        return density(m(i)) * dm(i);
    }

}

#endif
]]></document_content>
  </document>
  <document index="31">
    <source>onefactorgaussiancopula.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file onefactorgaussiancopula.hpp
    \brief One-factor Gaussian copula
*/

#ifndef quantlib_one_factor_gaussian_copula_hpp
#define quantlib_one_factor_gaussian_copula_hpp

#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/math/distributions/normaldistribution.hpp>

namespace QuantLib {

    //! One-factor Gaussian Copula
    /*! The copula model
        \f[ Y_i = a_i\,M+\sqrt{1-a_i^2}\:Z_i \f]
        is specified here by setting the desnity function for all
        variables, \f$ M, Z,\f$ and also \f$ Y \f$ to the standard
        normal distribution
        \f$ \phi(x) = \exp(-x^2/2) / \sqrt{2\pi}. \f$
    */
    class OneFactorGaussianCopula : public OneFactorCopula {
      public:
        explicit OneFactorGaussianCopula(const Handle<Quote>& correlation,
                                         Real maximum = 5, Size integrationSteps = 50)
        : OneFactorCopula (correlation, maximum, integrationSteps) {
            registerWith(correlation_);
        }
        Real density(Real m) const override;
        Real cumulativeZ(Real z) const override;
        /*! overrides the base class implementation based on table data */
        Real cumulativeY(Real y) const override;
        Real testCumulativeY (Real y) const;
        /*! overrides the base class implementation based on table data */
        Real inverseCumulativeY(Real p) const override;

      private:
        // nothing to be done when correlation changes
        void performCalculations() const override {}

        NormalDistribution density_;
        CumulativeNormalDistribution cumulative_;
        InverseCumulativeNormal inverseCumulative_;
    };

    inline Real OneFactorGaussianCopula::density (Real m) const {
        return density_(m);
    }

    inline Real OneFactorGaussianCopula::cumulativeZ (Real z) const {
        return cumulative_(z);
    }

    inline Real OneFactorGaussianCopula::cumulativeY (Real y) const {
        return cumulative_(y);
    }

    inline Real OneFactorGaussianCopula::inverseCumulativeY (Real p) const {
        return inverseCumulative_(p);
    }

}

#endif
]]></document_content>
  </document>
  <document index="32">
    <source>onefactorstudentcopula.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file onefactorstudentcopula.hpp
    \brief One-factor Student-t copula
*/

#ifndef quantlib_one_factor_student_copula_hpp
#define quantlib_one_factor_student_copula_hpp

#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/math/distributions/studenttdistribution.hpp>
#include <ql/math/distributions/normaldistribution.hpp>

namespace QuantLib {

    //! One-factor Double Student t-Copula
    /*! The copula model
        \f[ Y_i = a_i\,M+\sqrt{1-a_i^2}\:Z_i \f]

        is specified here by setting the probability density functions
        for \f$ Z_i \f$ (\f$ D_Z \f$) and \f$ M \f$ (\f$ D_M \f$) to
        Student t-distributions with \f$ N_z \f$ and \f$ N_m \f$
        degrees of freedom, respectively.

        The variance of the Student t-distribution with \f$ \nu \f$
        degrees of freedom is \f$ \nu / (\nu - 2) \f$. Since the
        copula approach requires zero mean and unit variance
        distributions, variables \f$ Z \f$ and \f$ M \f$ are scaled by
        \f$ \sqrt{(N_z - 2) / N_z} \f$ and \f$ \sqrt{(N_m - 2) / N_m}, \f$
        respectively.

        \todo Improve performance/accuracy of the calculation of
              inverse cumulative Y. Tabulate and store it for selected
              correlations?
    */
    class OneFactorStudentCopula : public OneFactorCopula {
      public:
        OneFactorStudentCopula (const Handle<Quote>& correlation,
                                int nz, int nm,
                                Real maximum = 10, Size integrationSteps = 200);

        Real density(Real m) const override;
        Real cumulativeZ(Real z) const override;

      private:
        //! Observer interface
        void performCalculations() const override;

        StudentDistribution density_;              // density of M
        CumulativeStudentDistribution cumulative_; // cumulated density of Z
        int nz_;                                   // degrees of freedom of Z
        int nm_;                                   // degrees of freedom of M

        Real scaleM_; // scaling for m to ensure unit variance
        Real scaleZ_; // scaling for z to ensure unit variance

        // This function is used to update the table of the cumulative
        // distribution of Y. It is invoked by performCalculations() when the
        // correlation handle is amended.
        Real cumulativeYintegral (Real y) const;
    };

    inline Real OneFactorStudentCopula::density (Real m) const {
        return density_(m / scaleM_) / scaleM_;
    }

    inline Real OneFactorStudentCopula::cumulativeZ (Real z) const {
        return cumulative_(z / scaleZ_);
    }


    //! One-factor Gaussian-Student t-Copula
    /*! The copula model
        \f[ Y_i = a_i\,M+\sqrt{1-a_i^2}\:Z_i \f]

        is specified here by setting the probability density functions
        for \f$ Z_i \f$ (\f$ D_Z \f$) to a Student t-distributions
        with \f$ N_z \f$ degrees of freedom, and for \f$ M \f$
        (\f$ D_M \f$) to a Gaussian.

        The variance of the Student t-distribution with \f$ \nu \f$
        degrees of freedom is \f$ \nu / (\nu - 2) \f$. Since the
        copula approach requires zero mean and unit variance
        distributions, \f$ Z \f$ is scaled by \f$ \sqrt{(N_z - 2) /
        N_z}.\f$

        \todo Improve performance/accuracy of the calculation of
              inverse cumulative Y. Tabulate and store it for selected
              correlations?
    */
    class OneFactorGaussianStudentCopula : public OneFactorCopula {
      public:
        OneFactorGaussianStudentCopula (const Handle<Quote>& correlation,
                                        int nz,
                                        Real maximum = 10,
                                        Size integrationSteps = 200);

        Real density(Real m) const override;
        Real cumulativeZ(Real z) const override;

      private:
        //! Observer interface
        void performCalculations() const override;

        NormalDistribution density_;               // density of M
        CumulativeStudentDistribution cumulative_; // cumulated density of Z
        int nz_;                                   // degrees of freedom of Z

        Real scaleZ_; // scaling for z to ensure unit variance

        // This function is used to update the table of the cumulative
        // distribution of Y. It is invoked by performCalculations() when the
        // correlation handle is amended.
        Real cumulativeYintegral (Real y) const;
    };

    inline Real OneFactorGaussianStudentCopula::density (Real m) const {
        return density_(m);
    }

    inline Real OneFactorGaussianStudentCopula::cumulativeZ (Real z) const {
        return cumulative_(z / scaleZ_);
    }


    //! One-factor Student t - Gaussian Copula
    /*! The copula model
        \f[ Y_i = a_i\,M+\sqrt{1-a_i^2}\:Z_i \f]
        is specified here by setting the probability density functions
        for \f$ Z_i \f$ (\f$ D_Z \f$) to a Gaussian and for \f$ M \f$
        (\f$ D_M \f$) to a Student t-distribution with \f$ N_m \f$
        degrees of freedom.

        The variance of the Student t-distribution with \f$ \nu \f$
        degrees of freedom is \f$ \nu / (\nu - 2) \f$. Since the
        copula approach requires zero mean and unit variance
        distributions, \f$ M \f$ is scaled by \f$ \sqrt{(N_m - 2) /
        N_m}. \f$

        \todo Improve performance/accuracy of the calculation of
              inverse cumulative Y. Tabulate and store it for selected
              correlations?
    */
    class OneFactorStudentGaussianCopula : public OneFactorCopula {
      public:
        OneFactorStudentGaussianCopula (const Handle<Quote>& correlation,
                                        int nm,
                                        Real maximum = 10,
                                        Size integrationSteps = 200);

        Real density(Real m) const override;
        Real cumulativeZ(Real z) const override;

      private:
        //! Observer interface
        void performCalculations() const override;

        StudentDistribution density_;              // density of M
        CumulativeNormalDistribution cumulative_;  // cumulated density of Z
        int nm_;                                   // degrees of freedom of M

        Real scaleM_; // scaling for m to ensure unit variance

        // This function is used to update the table of the cumulative
        // distribution of Y. It is invoked by performCalculations() when the
        // correlation handle is amended.
        Real cumulativeYintegral (Real y) const;
    };

    inline Real OneFactorStudentGaussianCopula::density (Real m) const {
        return density_(m / scaleM_) / scaleM_;
    }

    inline Real OneFactorStudentGaussianCopula::cumulativeZ (Real z) const {
        return cumulative_(z);
    }

}


#endif
]]></document_content>
  </document>
  <document index="33">
    <source>pool.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file pool.hpp
    \brief pool of issuers
*/

#ifndef quantlib_pool_hpp
#define quantlib_pool_hpp

#include <ql/utilities/disposable.hpp>
#include <ql/experimental/credit/issuer.hpp>
#include <map>

namespace QuantLib {

    class Pool {
      public:
        Pool();
        Size size() const;
        void clear();
        bool has (const std::string& name) const;
        void add (const std::string& name, const Issuer& issuer, 
            const DefaultProbKey& contractTrigger = NorthAmericaCorpDefaultKey(
                Currency(), SeniorSec, Period(), 1.));
        const Issuer& get (const std::string& name) const;
        const DefaultProbKey& defaultKey(const std::string& name) const;
        void setTime(const std::string& name, Real time);
        Real getTime (const std::string& name) const;
        const std::vector<std::string>& names() const;
        Disposable<std::vector<DefaultProbKey> > defaultKeys() const;
    private:
        // \todo: needs to cehck all defaul TS have the same ref date? here or
        //   where used? e.g. simulations.
        std::map<std::string,Issuer> data_;
        std::map<std::string,Real> time_;
        std::vector<std::string> names_;
        /*! default events seniority and currency this name enters the basket 
        with. Determines to which event/probability this pool referes to. */
        std::map<std::string, DefaultProbKey> defaultKeys_;
    };

}


#endif
]]></document_content>
  </document>
  <document index="34">
    <source>randomdefaultlatentmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_randomdefault_latent_model_hpp
#define quantlib_randomdefault_latent_model_hpp

#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/experimental/math/gaussiancopulapolicy.hpp>
#include <ql/experimental/math/latentmodel.hpp>
#include <ql/experimental/math/tcopulapolicy.hpp>
#include <ql/math/beta.hpp>
#include <ql/math/functional.hpp>
#include <ql/math/randomnumbers/mt19937uniformrng.hpp>
#include <ql/math/randomnumbers/sobolrsg.hpp>
#include <ql/math/solvers1d/brent.hpp>
#include <ql/math/statistics/histogram.hpp>
#include <ql/math/statistics/riskstatistics.hpp>
#include <ql/tuple.hpp>
#include <utility>

/* Intended to replace
    ql\experimental\credit\randomdefaultmodel.Xpp
*/

namespace QuantLib {

    /*! Simulation event trait class template forward declaration.
    Each latent model will be modelling different entities according to the
    meaning of the model function which depends on the random \$ Y_i\$
    variables. Still the generation of the factors and variables it is common to
    any model. Only within a given transformation function the model and event
    generated becomes concrete.

    However here these simulations are already made specific to a default event.
    Yet other variables contingent to default can be modelled (recovery,
    market...) So we are placed in a less generic stage where default is
    modelled possibly jointly with other unespecified magnitudes.

    Another role of this trait class is to compact in memory the simulation
    data. The statistic post processing needs to have the results stored in
    memory and simulations can not be consumed at generation time, typically
    because some statistics are conditional on others (e.g. ESF) or/and
    parametric (percentile, etc...)

    Simulation events do not derive from each other, and they are specialized
    for each type; duck typing applies for variable names (see the statistic
    methods)
    */
    // replaces class Loss
    template <class simEventOwner> struct simEvent;


    /*! Base class for latent model monte carlo simulation. Independent of the
    copula type and the generator.
    Generates the factors and variable samples and determines event threshold
    but it is not responsible for actual event specification; thats the derived
    classes responsibility according to what they model.
    Derived classes need mainly to implement nextSample (Worker::nextSample in
    the multithreaded version) to compute the simulation event generated, if
    any, from the latent variables sample. They also have the accompanying
    event trait to specify.
    */
    /* CRTP used for performance to avoid virtual table resolution in the Monte
    Carlo. Not only in sample generation but access; quite an amount of time can
    go into statistics computation, for a portfolio of tens of thousands
    positions that part of the problem will be starting to overtake the
    simulation costs.

    \todo: someone with sound experience on cache misses look into this, the
    statistics will be getting memory in and out of the cpu heavily and it
    might be possible to get performance out of that.
    \todo: parallelize the statistics computation, things like Var/ESF splits
    are very expensive.
    \todo: consider another design, taking the statistics outside the models.
    */
    template<template <class, class> class derivedRandomLM, class copulaPolicy,
        class USNG = SobolRsg>
    class RandomLM : public virtual LazyObject,
                     public virtual DefaultLossModel {
    private:
        // Takes the parents type, all children have the same type, the
        // random generation is performed in this class only.
        typedef typename LatentModel<copulaPolicy>::template FactorSampler<USNG>
            copulaRNG_type;
    protected:
      RandomLM(Size numFactors, Size numLMVars, copulaPolicy copula, Size nSims, BigNatural seed)
      : seed_(seed), numFactors_(numFactors), numLMVars_(numLMVars), nSims_(nSims),
        copula_(std::move(copula)) {}

      void update() override {
          simsBuffer_.clear();
          // tell basket to notify instruments, etc, we are invalid
          if (!basket_.empty())
              basket_->notifyObservers();
          LazyObject::update();
        }

        void performCalculations() const override {
            static_cast<const derivedRandomLM<copulaPolicy, USNG>* >(
                this)->initDates();//in update?
            copulasRng_ = ext::make_shared<copulaRNG_type>(copula_, seed_);
            performSimulations();
        }

        void performSimulations() const {
            // Next sequence should determine the event and push it into buffer
            for (Size i = nSims_; i != 0U; i--) {
                const std::vector<Real>& sample =
                    copulasRng_->nextSequence().value;
                static_cast<const derivedRandomLM<copulaPolicy, USNG>* >(
                    this)->nextSample(sample);
            // alternatively make call an explicit local method...
            }
        }

        /* Method to access simulation results and avoiding a copy of
        each thread results buffer. PerformCalculations should have been called.
        Here in the monothread version this method is redundant/trivial but
        serves to detach the statistics access to the way the simulations are
        stored.
        */
        const std::vector<simEvent<derivedRandomLM<copulaPolicy, USNG> > >&
            getSim(const Size iSim) const { return simsBuffer_[iSim]; }

        /* Allows statistics to be written generically for fixed and random
        recovery rates. */
        Real getEventRecovery(
            const simEvent<derivedRandomLM<copulaPolicy, USNG> >& evt) const
        {
            return static_cast<const derivedRandomLM<copulaPolicy, USNG>* >(
                this)->getEventRecovery(evt);
        }

        //! \name Statistics, DefaultLossModel interface.
        // These are virtual and allow for children-specific optimization and
        //   variance reduction. The virtual table is ok, they are not part
        //   of the simulation.
        //@{
        /*! Returns the probaility of having a given or larger number of
        defaults in the basket portfolio at a given time.
        */
        Probability probAtLeastNEvents(Size n, const Date& d) const override;
        /*! Order of results refers to the simulated (super)pool not the
        basket's pool.
        Notice that this statistic suffers from heavy dispersion. To see
        techniques to improve it (not implemented here) see:
        Joshi, M., D. Kainth. 2004. Rapid and accurate development of prices
        and Greeks for nth to default credit swaps in the Li model. Quantitative
        Finance, Vol. 4. Institute of Physics Publishing, London, UK, 266-275
        and:
        Chen, Z., Glasserman, P. 'Fast pricing of basket default swaps' in
        Operations Research Vol. 56, No. 2, March/April 2008, pp. 286-303
        */
        Disposable<std::vector<Probability> > probsBeingNthEvent(Size n,
                                                                 const Date& d) const override;
        //! Pearsons' default probability correlation.
        Real defaultCorrelation(const Date& d, Size iName, Size jName) const override;
        Real expectedTrancheLoss(const Date& d) const override;
        virtual std::pair<Real, Real> expectedTrancheLossInterval(const Date& d,
            Probability confidencePerc) const;
        Disposable<std::map<Real, Probability> > lossDistribution(const Date& d) const override;
        virtual Histogram computeHistogram(const Date& d) const;
        Real expectedShortfall(const Date& d, Real percent) const override;
        Real percentile(const Date& d, Real percentile) const override;
        /*! Returns the VaR value for a given percentile and the 95 confidence
        interval of that value. */
        virtual ext::tuple<Real, Real, Real> percentileAndInterval(
            const Date& d, Real percentile) const;
        /*! Distributes the total VaR amount along the portfolio counterparties.
            The passed loss amount is in loss units.
        */
        Disposable<std::vector<Real> > splitVaRLevel(const Date& date, Real loss) const override;
        /*! Distributes the total VaR amount along the portfolio
            counterparties.

            Provides confidence interval for split so that portfolio
            optimization can be performed outside those limits.

            The passed loss amount is in loss units.
        */
        virtual Disposable<std::vector<std::vector<Real> > > splitVaRAndError(
            const Date& date, Real loss, Probability confInterval) const;
        //@}
    public:
      ~RandomLM() override = default;

    private:
        BigNatural seed_;
    protected:
        const Size numFactors_;
        const Size numLMVars_;

        const Size nSims_;

        mutable std::vector<std::vector<simEvent<derivedRandomLM<copulaPolicy,
            USNG > > > > simsBuffer_;

        mutable copulaPolicy copula_;
        mutable ext::shared_ptr<copulaRNG_type> copulasRng_;

        // Maximum time inversion horizon
        static const Size maxHorizon_ = 4050; // over 11 years
        // Inversion probability limits are computed by children in initdates()
    };


    /* ---- Statistics ---------------------------------------------------  */

    template<template <class, class> class D, class C, class URNG>
    Probability RandomLM<D, C, URNG>::probAtLeastNEvents(Size n,
        const Date& d) const
    {
        calculate();
        Date today = Settings::instance().evaluationDate();

        QL_REQUIRE(d>today, "Date for statistic must be in the future.");
        // casted to natural to avoid warning, we have just checked the sign
        Natural val = d.serialNumber() - today.serialNumber();

        if(n==0) return 1.;

        Real counts = 0.;
        for(Size iSim=0; iSim < nSims_; iSim++) {
            Size simCount = 0;
            const std::vector<simEvent<D<C, URNG> > >& events =
                getSim(iSim);
            for(Size iEvt=0; iEvt < events.size(); iEvt++)
                // duck type on the members:
                if(val > events[iEvt].dayFromRef) simCount++;
            if(simCount >= n) counts++;
        }
        return counts/nSims_;
        // \todo Provide confidence interval
    }

    template<template <class, class> class D, class C, class URNG>
    Disposable<std::vector<Probability> >
        RandomLM<D, C, URNG>::probsBeingNthEvent(Size n,
            const Date& d) const
    {
        calculate();
        Size basketSize = basket_->size();

        QL_REQUIRE(n>0 && n<=basketSize, "Impossible number of defaults.");
        Date today = Settings::instance().evaluationDate();

        QL_REQUIRE(d>today, "Date for statistic must be in the future.");
        // casted to natural to avoid warning, we have just checked the sign
        Natural val = d.serialNumber() - today.serialNumber();

        std::vector<Probability> hitsByDate(basketSize, 0.);
        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);
            std::map<unsigned short, unsigned short> namesDefaulting;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                // if event is within time horizon...
                if(val > events[iEvt].dayFromRef)
                    //...count it. notice insertion sorts by date.
                    namesDefaulting.insert(std::make_pair<unsigned short,
                      unsigned short>(events[iEvt].dayFromRef,
                        events[iEvt].nameIdx));
            }
            if(namesDefaulting.size() >= n) {
                std::map<unsigned short, unsigned short>::const_iterator
                    itdefs = namesDefaulting.begin();
                // locate nth default in time:
                std::advance(itdefs, n-1);
                // update statistic:
                hitsByDate[itdefs->second]++;
            }
        }
        std::transform(hitsByDate.begin(), hitsByDate.end(),
                       hitsByDate.begin(),
                       divide_by<Real>(Real(nSims_)));
        return hitsByDate;
        // \todo Provide confidence interval
    }


    template<template <class, class> class D, class C, class URNG>
    Real RandomLM<D, C, URNG>::defaultCorrelation(const Date& d,
        Size iName, Size jName) const
    {
        // a control variate with the probabilities is possible
        calculate();
        Date today = Settings::instance().evaluationDate();

        QL_REQUIRE(d>today, "Date for statistic must be in the future.");
        // casted to natural to avoid warning, we have just checked the sign
        Natural val = d.serialNumber() - today.serialNumber();

        Real expectedDefiDefj = 0.;// E[1_i 1_j]
        // the rest of magnitudes have known values (probabilities) but that
        //   would distort the simulation results.
        Real expectedDefi = 0.;
        Real expectedDefj = 0.;
        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);
            Real imatch = 0., jmatch = 0.;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                if((val > events[iEvt].dayFromRef) &&
                   (events[iEvt].nameIdx == iName)) imatch = 1.;
                if((val > events[iEvt].dayFromRef) &&
                   (events[iEvt].nameIdx == jName)) jmatch = 1.;
            }
            expectedDefiDefj += imatch * jmatch;
            expectedDefi += imatch;
            expectedDefj += jmatch;
        }
        expectedDefiDefj = expectedDefiDefj / (nSims_-1);// unbiased
        expectedDefi = expectedDefi / nSims_;
        expectedDefj = expectedDefj / nSims_;

        return (expectedDefiDefj - expectedDefi*expectedDefj) /
            std::sqrt((expectedDefi*expectedDefj*(1.-expectedDefi)
                *(1.-expectedDefj)));
        // \todo Provide confidence interval
    }


    template<template <class, class> class D, class C, class URNG>
    Real RandomLM<D, C, URNG>::expectedTrancheLoss(
        const Date& d) const {
            return expectedTrancheLossInterval(d, 0.95).first;
    }


    template<template <class, class> class D, class C, class URNG>
    std::pair<Real, Real> RandomLM<D, C, URNG>::expectedTrancheLossInterval(
        const Date& d, Probability confidencePerc) const
    {
        calculate();
        Date today = Settings::instance().evaluationDate();
        Date::serial_type val = d.serialNumber() - today.serialNumber();

        Real attachAmount = basket_->attachmentAmount();
        Real detachAmount = basket_->detachmentAmount();

        // Real trancheLoss= 0.;
        GeneralStatistics lossStats;
        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);

            Real portfSimLoss=0.;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                // if event is within time horizon...
                if(val > static_cast<Date::serial_type>(
					   events[iEvt].dayFromRef)) {
                    Size iName = events[iEvt].nameIdx;
                    // ...and is contained in the basket.
                        portfSimLoss +=
                            basket_->exposure(basket_->names()[iName],
                                Date(events[iEvt].dayFromRef +
                                    today.serialNumber())) *
                                        (1.-getEventRecovery(events[iEvt]));
               }
            }
            lossStats.add(// d  ates? current losses? realized defaults, not yet
                std::min(std::max(portfSimLoss - attachAmount, 0.),
                    detachAmount - attachAmount) );
        }
        return std::make_pair(lossStats.mean(), lossStats.errorEstimate() *
            InverseCumulativeNormal::standard_value(0.5*(1.+confidencePerc)));
    }


    template<template <class, class> class D, class C, class URNG>
    Disposable<std::map<Real, Probability> >
        RandomLM<D, C, URNG>::lossDistribution(const Date& d) const {

        Histogram hist = computeHistogram(d);
        std::map<Real, Probability> distrib;

        // prob of losses less or equal to
        Real suma = hist.frequency(0);
        distrib.insert(std::make_pair(0., suma));
        for(Size i=1; i<hist.bins(); i++) {
            suma += hist.frequency(i);
            distrib.insert(std::make_pair( hist.breaks()[i-1], suma ));
        }
        return distrib;
    }


    template<template <class, class> class D, class C, class URNG>
    Histogram RandomLM<D, C, URNG>::computeHistogram(const Date& d) const {
        std::vector<Real> data;
        std::set<Real> keys;// attainable loss values
        keys.insert(0.);
        Date today = Settings::instance().evaluationDate();
        Date::serial_type val = d.serialNumber() - today.serialNumber();
        // redundant test? should have been tested by the basket caller?
        QL_REQUIRE(d >= today,
            "Requested percentile date must lie after computation date.");
        calculate();

        Real attachAmount = basket_->attachmentAmount();
        Real detachAmount = basket_->detachmentAmount();

        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);

            Real portfSimLoss=0.;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                if(val > static_cast<Date::serial_type>(
					 events[iEvt].dayFromRef)) {
                    Size iName = events[iEvt].nameIdx;
          // test needed (here and the others) to reuse simulations:
          //          if(basket_->pool()->has(copula_->pool()->names()[iName]))
                        portfSimLoss +=
                            basket_->exposure(basket_->names()[iName],
                                Date(events[iEvt].dayFromRef +
                                    today.serialNumber())) *
                                        (1.-getEventRecovery(events[iEvt]));
                }
            }
            data.push_back(std::min(std::max(portfSimLoss - attachAmount, 0.),
                detachAmount - attachAmount));
            keys.insert(data.back());
        }
        // avoid using as many points as in the simulation.
        Size nPts = std::min<Size>(data.size(), 150);// fix
        return Histogram(data.begin(), data.end(), nPts);
    }


    template<template <class, class> class D, class C, class URNG>
    Real RandomLM<D, C, URNG>::expectedShortfall(const Date& d,
        Real percent) const {

        const Date today = Settings::instance().evaluationDate();
        QL_REQUIRE(d >= today,
            "Requested percentile date must lie after computation date.");
        calculate();

        Real attachAmount = basket_->attachmentAmount();
        Real detachAmount = basket_->detachmentAmount();

        Date::serial_type val = d.serialNumber() - today.serialNumber();
        if(val <= 0) return 0.;// plus basket realized losses

        //GenericRiskStatistics<GeneralStatistics> statsX;
        std::vector<Real> losses;
        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);
            Real portfSimLoss=0.;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                if(val > static_cast<Date::serial_type>(
					  events[iEvt].dayFromRef)) {
                    Size iName = events[iEvt].nameIdx;
                    // ...and is contained in the basket.
                    //if(basket_->pool()->has(copula_->pool()->names()[iName]))
                        portfSimLoss +=
                            basket_->exposure(basket_->names()[iName],
                                Date(events[iEvt].dayFromRef +
                                    today.serialNumber())) *
                                        (1.-getEventRecovery(events[iEvt]));
                }
            }
            portfSimLoss = std::min(std::max(portfSimLoss - attachAmount, 0.),
                detachAmount - attachAmount);
            losses.push_back(portfSimLoss);
        }

        std::sort(losses.begin(), losses.end());
        Real posit = std::ceil(percent * nSims_);
        posit = posit >= 0. ? posit : 0.;
        Size position = static_cast<Size>(posit);
        Real perctlInf = losses[position];//q_{\alpha}

        // the prob of values strictly larger than the quantile value.
        Probability probOverQ =
            static_cast<Real>(std::distance(losses.begin() + position,
                losses.end())) / static_cast<Real>(nSims_);

        return ( perctlInf * (1.-percent-probOverQ) +//<-correction term
            std::accumulate(losses.begin() + position, losses.end(), 
			    Real(0.))/nSims_
                )/(1.-percent);

        /* Alternative ESF definition; find the first loss larger than the
        one of the percentile. Notice the choice here, the expected shortfall
        is understood in the sense that we are looking for the average given
        than losses are above a certain value rather than above a certain
        probability:
        (Unlikely to be the algorithm of choice)*/
        /*
        std::vector<Real>::iterator itPastPerc =
            std::find_if(losses.begin() + position, losses.end(),
                         greater_or_equal_to<Real>(perctlInf));
        // notice if the sample is flat at the end this might be zero
        Size pointsOverVal = nSims_ - std::distance(itPastPerc, losses.end());
        return pointsOverVal == 0 ? 0. :
            std::accumulate(itPastPerc, losses.end(), 0.) / pointsOverVal;
        */

        /* For the definition of ESF see for instance: 'Quantitative Risk
        Management' by A.J. McNeil, R.Frey and P.Embrechts, princeton series in
        finance, 2005; equations on page 39 sect 2.12:
        $q_{\alpha}(F) = inf{x \in R : F(x) \le \alpha}$
        and equation 2.25 on p. 45:
        $ESF_{\alpha} = \frac{1}{1-\alpha} [E(L; L \ge q_{\alpha} ) +
            q_{\alpha} (1-\alpha-P(L \ge q_{\alpha})) ]$
        The second term accounts for non continuous distributions.
        */
    }


    template<template <class, class> class D, class C, class URNG>
    Real RandomLM<D, C, URNG>::percentile(const Date& d, Real perc) const {
        // need to specify return type in tuples' get is parametric
        return ext::get<0>(percentileAndInterval(d, perc));
    }


    /* See Appendix-A of "Evaluating value-at-risk methodologies: Accuracy
        versus computational time.", M. Pritsker, Wharton FIC, November 1996
    Strictly speaking this gives the interval with a 95% probability of
    the true value being within the interval; which is different to the error
    of the stimator just computed. See the reference for a discussion.
    */
    template<template <class, class> class D, class C, class URNG>
    ext::tuple<Real, Real, Real> // disposable?
        RandomLM<D, C, URNG>::percentileAndInterval(const Date& d,
            Real percentile) const {

        QL_REQUIRE(percentile >= 0. && percentile <= 1.,
            "Incorrect percentile");
        calculate();

        Real attachAmount = basket_->attachmentAmount();
        Real detachAmount = basket_->detachmentAmount();

        std::vector<Real> rankLosses;
        Date today = Settings::instance().evaluationDate();
        Date::serial_type val = d.serialNumber() - today.serialNumber();
        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);
            Real portfSimLoss=0.;
            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                if(val > static_cast<Date::serial_type>(
					 events[iEvt].dayFromRef)) {
                    Size iName = events[iEvt].nameIdx;
                 //   if(basket_->pool()->has(copula_->pool()->names()[iName]))
                        portfSimLoss +=
                            basket_->exposure(basket_->names()[iName],
                                Date(events[iEvt].dayFromRef +
                                    today.serialNumber())) *
                                        (1.-getEventRecovery(events[iEvt]));
                }
            }
            portfSimLoss = std::min(std::max(portfSimLoss - attachAmount, 0.),
                detachAmount - attachAmount);
            // update dataset for rank stat:
            rankLosses.push_back(portfSimLoss);
        }

        std::sort(rankLosses.begin(), rankLosses.end());
        Size quantilePosition = static_cast<Size>(floor(nSims_*percentile));
        Real quantileValue = rankLosses[quantilePosition];

        // compute confidence interval:
        const Probability confInterval = 0.95;// as an argument?
        Real lowerPercentile, upperPercentile;
        Size r = quantilePosition - 1;
        Size s = quantilePosition + 1;
        bool rLocked = false,
            sLocked = false;
        // Size rfinal = 0,
        //      sfinal = 0;
        for(Size delta=1; delta < quantilePosition; delta++) {
            Real cached =
                incompleteBetaFunction(Real(s), Real(nSims_+1-s),
                                       percentile, 1.e-8, 500);
            Real pMinus =
            /* There was a fix in the repository on the gammadistribution. It
            might impact these, it might be neccesary to multiply these values
            by '-1'*/
                incompleteBetaFunction(Real(r+1), Real(nSims_-r),
                                       percentile, 1.e-8, 500)
                - cached;
            Real pPlus  =
                incompleteBetaFunction(Real(r), Real(nSims_-r+1),
                                       percentile, 1.e-8, 500)
                - cached;
            if((pMinus > confInterval) && !rLocked ) {
                // rfinal = r + 1;
               rLocked = true;
            }
            if((pPlus >= confInterval) && !sLocked) {
                // sfinal = s;
                sLocked = true;
            }
            if(rLocked && sLocked) break;
            r--;
            s++;
            s = std::min(nSims_-1, s);
        }
        lowerPercentile = rankLosses[r];
        upperPercentile = rankLosses[s];

        return {quantileValue, lowerPercentile, upperPercentile};
    }


    template<template <class, class> class D, class C, class URNG>
    Disposable<std::vector<Real> > RandomLM<D, C, URNG>::splitVaRLevel(
        const Date& date, Real loss) const
    {
        std::vector<Real> varLevels = splitVaRAndError(date, loss, 0.95)[0];
        // turn relative units into absolute:
        std::transform(varLevels.begin(), varLevels.end(), varLevels.begin(),
                       multiply_by<Real>(loss));
        return varLevels;
    }


    // parallelize this one(if possible), it is really expensive
    template<template <class, class> class D, class C, class URNG>
    /* FIX ME: some trouble on limit cases, like zero loss or no losses over the
    requested level.*/
    Disposable<std::vector<std::vector<Real> > >
        RandomLM<D, C, URNG>::splitVaRAndError(const Date& date, Real loss,
            Probability confInterval) const
    {
        /* Check 'loss' value integrity: i.e. is within tranche limits? (should
            have been done basket...)*/
        calculate();

        Real attachAmount = basket_->attachmentAmount();
        Real detachAmount = basket_->detachmentAmount();
        Size numLiveNames = basket_->remainingSize();

        std::vector<Real> split(numLiveNames, 0.);
        std::vector<GeneralStatistics> splitStats(numLiveNames,
            GeneralStatistics());
        Date today = Settings::instance().evaluationDate();
        Date::serial_type val = date.serialNumber() - today.serialNumber();

        for(Size iSim=0; iSim < nSims_; iSim++) {
            const std::vector<simEvent<D<C, URNG> > >& events = getSim(iSim);
            Real portfSimLoss=0.;
            //std::vector<Real> splitBuffer(numLiveNames_, 0.);
            std::vector<simEvent<D<C, URNG> > > splitEventsBuffer;

            for(Size iEvt=0; iEvt < events.size(); iEvt++) {
                if(val > static_cast<Date::serial_type>(
					 events[iEvt].dayFromRef)) {
                    Size iName = events[iEvt].nameIdx;
                // if(basket_->pool()->has(copula_->pool()->names()[iName])) {
                        portfSimLoss +=
                            basket_->exposure(basket_->names()[iName],
                                Date(events[iEvt].dayFromRef +
                                    today.serialNumber())) *
                                        (1.-getEventRecovery(events[iEvt]));
                        //and will sort later if buffer applies:
                        splitEventsBuffer.push_back(events[iEvt]);
                }
            }
            portfSimLoss = std::min(std::max(portfSimLoss - attachAmount, 0.),
                detachAmount - attachAmount);

            /* second pass; split is conditional to total losses within target
            losses/percentile:  */
            Real ptflCumulLoss = 0.;
            if(portfSimLoss > loss) {
                std::sort(splitEventsBuffer.begin(), splitEventsBuffer.end());
                //NOW THIS:
                split.assign(numLiveNames, 0.);
                /*  if the name triggered a loss in the portf limits assign
                this loss to that name..  */
                for(Size i=0; i<splitEventsBuffer.size(); i++) {
                    Size iName = splitEventsBuffer[i].nameIdx;
                    Real lossName =
            // allows amortizing (others should be like this)
            // basket_->remainingNotionals(Date(simsBuffer_[i].dayFromRef +
            //      today.serialNumber()))[iName] *
                        basket_->exposure(basket_->names()[iName],
                            Date(splitEventsBuffer[i].dayFromRef +
                                today.serialNumber())) *
                                (1.-getEventRecovery(splitEventsBuffer[i]));

                    Real tranchedLossBefore =
                        std::min(std::max(ptflCumulLoss - attachAmount, 0.),
                        detachAmount - attachAmount);
                    ptflCumulLoss += lossName;
                    Real tranchedLossAfter =
                        std::min(std::max(ptflCumulLoss - attachAmount, 0.),
                        detachAmount - attachAmount);
                    // assign new losses:
                    split[iName] += tranchedLossAfter - tranchedLossBefore;
                }
                for(Size iName=0; iName<numLiveNames; iName++) {
                    splitStats[iName].add(split[iName] /
                        std::min(std::max(ptflCumulLoss - attachAmount, 0.),
                            detachAmount - attachAmount) );
                }
            }
        }

        // Compute error in VaR split
        std::vector<Real> means, rangeUp, rangeDown;
        Real confidFactor = InverseCumulativeNormal()(0.5+confInterval/2.);
        for(Size iName=0; iName<numLiveNames; iName++) {
            means.push_back(splitStats[iName].mean());
            Real error = confidFactor * splitStats[iName].errorEstimate() ;
            rangeDown.push_back(means.back() - error);
            rangeUp.push_back(means.back() + error);
        }

        std::vector<std::vector<Real> > results;
        results.push_back(means);
        results.push_back(rangeDown);
        results.push_back(rangeUp);

        return results;
    }




    // --------- Time inversion solver target function: -----------------------

    /* It could be argued that this concept is part of the copula (more generic)
    In general when the modelled magnitude is parametric one can solve for
    inversion to get the parameter value for a given magnitude value (provided
    the modelled variable dependence in invertible). In this particular problem
    the parameter is Time and it is solved here where we are alredy in the
    context of default
    See default transition models for another instance of this inversion.
    Alternatively use the faster trick (flat HR) mentioned in the code or make
    the algorithm parametric on the type of interpolation in the default TS.
    */
    namespace detail {// not template dependent .....move it
        //! Utility for the numerical time solver
        class Root {
          public:
            /* See a faster algorithm (neeeds to locate the points) in
            D.O'KANE p.249 sect 13.5 */
            Root(const Handle<DefaultProbabilityTermStructure>& dts, Real pd)
            : dts_(dts), pd_(pd), curveRef_(dts->referenceDate()) {}
            /* The cast I am forcing here comes from the requirement of 1D
            solvers to take in a target (cost) function of Real domain. It could
            be possible to change the template arg F in the 1D solvers to a
            boost function and then use the (template arg) domain argument type
            of the function for use with the 'guess' and operator() ?
             */
            Real operator()(Real t) const {
                QL_REQUIRE (t >= 0.0, "t < 0");
                /* As long as this doesnt involve modifying a mutable member
                it should be thread safe (they are const methods and access is
                read only)
                */
                return dts_->defaultProbability(curveRef_ +
                    Period(static_cast<Integer>(t), Days), true) - pd_;
            }
          private:
            const Handle<DefaultProbabilityTermStructure> dts_;
            Real pd_;
            const Date curveRef_;
        };
    }

    /*
    ---------------------------------------------------------------------------
    ---------------------------------------------------------------------------
    */

    // move this one to a separte file?
    /*! Random default with deterministic recovery event type.\par
    Stores sims results in a bitfield buffer for lean memory storage.
    Although strictly speaking this is not guaranteed by the compiler it
    amounts to reducing the memory storage by half.
    Some computations, like conditional statistics, precise that all sims
    results be available.
    */
    template<class , class > class RandomDefaultLM;
    template<class copulaPolicy, class USNG>
    struct simEvent<RandomDefaultLM<copulaPolicy, USNG> > {
        simEvent(unsigned int n, unsigned int d)
        : nameIdx(n), dayFromRef(d){}
        unsigned int nameIdx : 16; // can index up to 65535 names
        unsigned int dayFromRef : 16; //indexes up to 65535 days ~179 years
        bool operator<(const simEvent& evt) const {
            return dayFromRef < evt.dayFromRef;
        }
    };

    /*! Default only latent model simulation with trivially fixed recovery
        amounts.
    */
    template<class copulaPolicy, class USNG = SobolRsg>
    class RandomDefaultLM : public RandomLM<RandomDefaultLM, copulaPolicy, USNG>
    {
    private:
        typedef simEvent<RandomDefaultLM> defaultSimEvent;

        // \todo Consider this to be only a ConstantLossLM instead
        const ext::shared_ptr<DefaultLatentModel<copulaPolicy> > model_;
        const std::vector<Real> recoveries_;
        // for time inversion:
        Real accuracy_;
    public:
        // \todo: Allow a constructor building its own default latent model.
      explicit RandomDefaultLM(const ext::shared_ptr<DefaultLatentModel<copulaPolicy> >& model,
                               const std::vector<Real>& recoveries = std::vector<Real>(),
                               Size nSims = 0, // stats will crash on div by zero, FIX ME.
                               Real accuracy = 1.e-6,
                               BigNatural seed = 2863311530UL)
      : RandomLM< ::QuantLib::RandomDefaultLM, copulaPolicy, USNG>(
            model->numFactors(), model->size(), model->copula(), nSims, seed),
        model_(model),
        recoveries_(recoveries.empty() ? std::vector<Real>(model->size(), 0.) : recoveries),
        accuracy_(accuracy) {
          // redundant through basket?
          this->registerWith(Settings::instance().evaluationDate());
          this->registerWith(model_);
        }
        explicit RandomDefaultLM(
            const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> >& model,
            Size nSims = 0,// stats will crash on div by zero, FIX ME.
            Real accuracy = 1.e-6,
            BigNatural seed = 2863311530UL)
        : RandomLM< ::QuantLib::RandomDefaultLM, copulaPolicy, USNG>
            (model->numFactors(), model->size(), model->copula(),
                nSims, seed ),
          model_(model),
          recoveries_(model->recoveries()),
          accuracy_(accuracy)
        {
            // redundant through basket?
            this->registerWith(Settings::instance().evaluationDate());
            this->registerWith(model_);
        }

        // grant access to static polymorphism:
        /* While this works on g++, VC9 refuses to compile it.
        Not completely sure whos right; individually making friends of the
        calling members or writting explicitly the derived class T parameters
        throws the same errors.
        The access is then open to the member fucntions.
        Another solution is to use this http://accu.org/index.php/journals/296

        It might well be that gcc is allowing some c11 features silently, which
        wont pass on a lower gcc version.
        */
        friend class RandomLM< ::QuantLib::RandomDefaultLM, copulaPolicy, USNG>;
    protected:
        void nextSample(const std::vector<Real>& values) const;
        void initDates() const {
            /* Precalculate horizon time default probabilities (used to
              determine if the default took place and subsequently compute its
              event time)
            */
            Date today = Settings::instance().evaluationDate();
            Date maxHorizonDate = today  + Period(this->maxHorizon_, Days);

            const ext::shared_ptr<Pool>& pool = this->basket_->pool();
            for(Size iName=0; iName < this->basket_->size(); ++iName)//use'live'
                horizonDefaultPs_.push_back(pool->get(pool->names()[iName]).
                    defaultProbability(this->basket_->defaultKeys()[iName])
                        ->defaultProbability(maxHorizonDate, true));
        }
        Real getEventRecovery(const defaultSimEvent& evt) const {
            return recoveries_[evt.nameIdx];
        }
        Real expectedRecovery(const Date&, Size iName, const DefaultProbKey&) const override {
            // deterministic
            return recoveries_[iName];
        }

        Real latentVarValue(const std::vector<Real>& factorsSample,
            Size iVar) const {
            return model_->latentVarValue(factorsSample, iVar);
        }
        //allows statistics to know the portfolio size (could be moved to parent
        //invoking duck typing on the variable name or a handle to the basket)
        Size basketSize() const { return model_->size(); }
    private:
      void resetModel() override /*const*/ {
          /* Explore: might save recalculation if the basket is the same
          (some situations, like BC or control variates) in that case do not
          update, only reset the model's basket.
          */
          model_->resetBasket(this->basket_.currentLink());

          QL_REQUIRE(this->basket_->size() == model_->size(),
                     "Incompatible basket and model sizes.");
          QL_REQUIRE(recoveries_.size() == this->basket_->size(),
                     "Incompatible basket and recovery sizes.");
          // invalidate current calculations if any and notify observers
          // NOLINTNEXTLINE(bugprone-parent-virtual-call)
          LazyObject::update();
      }
        // This one and the buffer might be moved to the parent, only some
        //   dates might be specific to a particular model.
        // Default probabilities for each name at the time of the maximun
        //   horizon date. Cached for perf.
        mutable std::vector<Probability> horizonDefaultPs_;
    };





    template<class C, class URNG>
    void RandomDefaultLM<C, URNG>::nextSample(
        const std::vector<Real>& values) const
    {
        const ext::shared_ptr<Pool>& pool = this->basket_->pool();
        // starts with no events
        this->simsBuffer_.push_back(std::vector<defaultSimEvent> ());

        for(Size iName=0; iName<model_->size(); iName++) {
            Real latentVarSample =
                model_->latentVarValue(values, iName);
            Probability simDefaultProb =
               model_->cumulativeY(latentVarSample, iName);
            // If the default simulated lies before the max date:
            if (horizonDefaultPs_[iName] >= simDefaultProb) {
                const Handle<DefaultProbabilityTermStructure>& dfts =
                    pool->get(pool->names()[iName]).// use 'live' names
                    defaultProbability(this->basket_->defaultKeys()[iName]);
                // compute and store default time with respect to the
                //  curve ref date:
                Size dateSTride =
                    static_cast<Size>(Brent().solve(// casted from Real:
                        detail::Root(dfts, simDefaultProb),
                            accuracy_,0.,1.));
                   /*
                   // value if one approximates to a flat HR;
                   //   faster (>x2) but it introduces an error:..
                   // \todo: see how to include this 'polymorphically'.
                   // While not the case in pricing in risk metrics/real
                   //   probabilities the curves are often flat
                    static_cast<Size>(ceil(maxHorizon_ *
                                        std::log(1.-simDefaultProb)
                    /std::log(1.-data_.horizonDefaultPs_[iName])));
                   */
                this->simsBuffer_.back().push_back(defaultSimEvent(iName,
                    dateSTride));
               //emplace_back
            }
        /* Used to remove sims with no events. Uses less memory, faster
        post-statistics. But only if all names in the portfolio have low
        default probability, otherwise is more expensive and sim access has
        to be modified. However low probability is also an indicator that
        variance reduction is needed. */
        }
    }




    // Common usage typedefs (notice they vary in the multithread version)
    // ---------- Gaussian default generators options ------------------------
    /* Uses copula direct normal inversion and MT generator
    typedef RandomDefaultLM<GaussianCopulaPolicy,
        RandomSequenceGenerator<MersenneTwisterUniformRng> >
            GaussianRandomDefaultLM;
    */
    /* Uses BoxMuller for gaussian generation, bypassing copula inversions
    typedef RandomDefaultLM<GaussianCopulaPolicy, RandomSequenceGenerator<
        BoxMullerGaussianRng<MersenneTwisterUniformRng> > >
            GaussianRandomDefaultLM;
    */
    /* Default case, uses the copula inversion directly and sobol sequence */
    typedef RandomDefaultLM<GaussianCopulaPolicy> GaussianRandomDefaultLM;

    // ---------- T default generators options ----------------------------
    /* Uses copula inversion and MT base generation
    typedef RandomDefaultLM<TCopulaPolicy,
      RandomSequenceGenerator<MersenneTwisterUniformRng> > TRandomDefaultLM;
    */
    /* Uses MT and polar direct strudent-T generation
    typedef RandomDefaultLM<TCopulaPolicy,
        RandomSequenceGenerator<PolarStudentTRng<MersenneTwisterUniformRng> > >
            TRandomDefaultLM;
    */
    /* Default case, uses sobol sequence and copula inversion */
    typedef RandomDefaultLM<TCopulaPolicy> TRandomDefaultLM;

}

#endif
]]></document_content>
  </document>
  <document index="35">
    <source>randomdefaultmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file randomdefaultmodel.hpp
    \brief Random default-time scenarios for a pool of credit names
*/

#ifndef quantlib_random_default_model_hpp
#define quantlib_random_default_model_hpp

#include <ql/math/randomnumbers/rngtraits.hpp>
#include <ql/experimental/credit/pool.hpp>
#include <ql/experimental/credit/onefactorcopula.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>

namespace QuantLib {

    //! Base class for random default models
    /*! Provides sequences of random default times for each name in the pool. */
    class RandomDefaultModel : public Observer, public Observable {
    public:
      RandomDefaultModel(const ext::shared_ptr<Pool>& pool,
                         const std::vector<DefaultProbKey>& defaultKeys)
      : pool_(pool), defaultKeys_(defaultKeys) {
          // assuming none defaulted this is true.
          QL_REQUIRE(defaultKeys.size() == pool->size(), "Incompatible pool and keys sizes.");
        }
        ~RandomDefaultModel() override = default;
        void update() override { notifyObservers(); }
        /*!
          Generate a sequence of random default times, one for each name in the
          pool, and store the result in the Pool using method setTime(name).
          tmax denotes the maximum relevant time- default times > tmax are not
          computed but set to tmax + 1 instead to save coputation time.
         */
        virtual void nextSequence(Real tmax = QL_MAX_REAL) = 0;
        virtual void reset() = 0;
    protected:
        ext::shared_ptr<Pool> pool_;
        std::vector<DefaultProbKey> defaultKeys_;
    };

    /*!
      Random default times using a one-factor Gaussian copula.
    */
    class GaussianRandomDefaultModel : public RandomDefaultModel {
    public:
      GaussianRandomDefaultModel(const ext::shared_ptr<Pool>& pool,
                                 const std::vector<DefaultProbKey>& defaultKeys,
                                 const Handle<OneFactorCopula>& copula,
                                 Real accuracy,
                                 long seed);
      void nextSequence(Real tmax = QL_MAX_REAL) override;
      void reset() override;

    private:
        Handle<OneFactorCopula> copula_;
        Real accuracy_;
        long seed_;
        PseudoRandom::rsg_type rsg_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="36">
    <source>randomlosslatentmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/
#ifndef quantlib_randomloss_latent_model_hpp
#define quantlib_randomloss_latent_model_hpp

#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/randomdefaultlatentmodel.hpp>
#include <ql/experimental/credit/spotlosslatentmodel.hpp> 
#include <ql/experimental/math/gaussiancopulapolicy.hpp>
#include <ql/experimental/math/latentmodel.hpp>
#include <ql/experimental/math/tcopulapolicy.hpp>
#include <ql/math/randomnumbers/mt19937uniformrng.hpp>
#include <ql/math/solvers1d/brent.hpp>
#include <cmath>

namespace QuantLib {


    template<class , class > class RandomLossLM;
    template<class copulaPolicy, class USNG>
        struct simEvent<RandomLossLM<copulaPolicy, USNG> > {
            simEvent(unsigned int n, unsigned int d, Real r) 
            : nameIdx(n), dayFromRef(d), 
                // truncates the value:
              compactRR(std::lround(r/rrGranular)) {}
            unsigned int nameIdx : 12; // can index up to 4095 names
            unsigned int dayFromRef : 12; // can index up to 4095 days = 11 yrs
        private:
            unsigned int compactRR : 8;
        public:
            // ..............still one bit left
            bool operator<(const simEvent& evt) const {
                return dayFromRef < evt.dayFromRef; 
            }
            Real recovery() const {
                /* we pay the price of this product (plus the division at 
                construction) for the memory we save. Precission is lost though,
                e.g. figures from 0.0 to 0.00390625/2. are stored as 0.0
                */
                return rrGranular * compactRR;
            }
            static const Real rrGranular;// = 1./256.;// 2^8
    };

#ifndef __DOXYGEN__

    template <class C, class G> const Real 
        simEvent<RandomLossLM<C, G> >::rrGranular = 1./256.;// 2^8

#endif

    /*! Random spot recovery rate loss model simulation for an arbitrary copula.
    */
    template<class copulaPolicy, class USNG = SobolRsg>
    class RandomLossLM : public RandomLM<RandomLossLM, copulaPolicy, USNG>
    {
    private:
        typedef simEvent<RandomLossLM> defaultSimEvent;

        const ext::shared_ptr<SpotRecoveryLatentModel<copulaPolicy> > copula_;
        // for time inversion:
        Real accuracy_;
    public:
        explicit RandomLossLM(
            const ext::shared_ptr<SpotRecoveryLatentModel<copulaPolicy> >& 
                copula,
            Size nSims = 0,
            Real accuracy = 1.e-6, 
            BigNatural seed = 2863311530UL)
        : RandomLM< ::QuantLib::RandomLossLM, copulaPolicy, USNG>
            (copula->numFactors(), copula->size(), copula->copula(), 
                nSims, seed),
          copula_(copula), accuracy_(accuracy)
    {
        // redundant through basket?
        this->registerWith(Settings::instance().evaluationDate());
    }

        // grant access to static polymorphism:
        /* While this works on g++, VC9 refuses to compile it.
        Not completely sure whos right; individually making friends of the 
        calling members or writting explicitly the derived class T parameters 
        throws the same errors.
        The access is then open to the member fucntions.
        */
        friend class RandomLM< ::QuantLib::RandomLossLM, copulaPolicy, USNG>;
    protected:
        void nextSample(const std::vector<Real>& values) const;

        // see note on randomdefaultlatentmodel
        void initDates() const {
            /* Precalculate horizon time default probabilities (used to 
              determine if the default took place and subsequently compute its 
              event time)
            */
            Date today = Settings::instance().evaluationDate();
            Date maxHorizonDate = today  + Period(this->maxHorizon_, Days);

            const ext::shared_ptr<Pool>& pool = this->basket_->pool();
            for(Size iName=0; iName < this->basket_->size(); ++iName)//use'live'
                horizonDefaultPs_.push_back(pool->get(pool->names()[iName]).
                    defaultProbability(this->basket_->defaultKeys()[iName])
                        ->defaultProbability(maxHorizonDate, true));
        }
       Real getEventRecovery(const defaultSimEvent& evt) const {
            return evt.recovery();
        }

        Real latentVarValue(const std::vector<Real>& factorsSample, 
            Size iVar) const {
                return copula_->latentVarValue(factorsSample, iVar);
        }
        Size basketSize() const { return this->basket_->size(); }
        // conditional to default, defined as spot-recovery.
        Real conditionalRecovery(Real latentVarSample, Size iName, 
            const Date& d) const;
    private:
      void resetModel() override {
          /* Explore: might save recalculation if the basket is the same
          (some situations, like BC or control variates) in that case do not
          update, only reset the copula's basket.
          */
          copula_->resetBasket(this->basket_.currentLink());

          QL_REQUIRE(2 * this->basket_->size() == copula_->size(),
                     "Incompatible basket and model sizes.");
          // invalidate current calculations if any and notify observers
          // NOLINTNEXTLINE(bugprone-parent-virtual-call)
          LazyObject::update();
      }
        // Default probabilities for each name at the time of the maximun 
        //   horizon date. Cached for perf.
        mutable std::vector<Probability> horizonDefaultPs_;
    };


    // --------------------------------------------------------------


    template<class C, class URNG>
    void RandomLossLM<C, URNG>::nextSample(
        const std::vector<Real>& values) const 
    {
        const ext::shared_ptr<Pool>& pool = this->basket_->pool();
        this->simsBuffer_.push_back(std::vector<defaultSimEvent> ());

        // half the model is defaults, the other half are RRs...
        for(Size iName=0; iName<copula_->size()/2; iName++) {
            // ...but samples must be full
            /* This is really a trick, we are passing a longer than
            expected set of values in the sample but the last idiosyncratic
            values corresponding to the RR are not used. They are used below
            only if we are in default. This works due to the way the SpotLossLM
            is split in two almost disjoint latent models and that theres no
            check on the vector size in the LM base class.
            */
            Real latentVarSample = 
                copula_->latentVarValue(values, iName);
            Probability simDefaultProb = 
                copula_->cumulativeY(latentVarSample, iName);
            // If the default simulated lies before the max date:
            if (horizonDefaultPs_[iName] >= simDefaultProb) {
                const Handle<DefaultProbabilityTermStructure>& dfts = 
                    pool->get(pool->names()[iName]).  // use 'live' names
                    defaultProbability(this->basket_->defaultKeys()[iName]);
                // compute and store default time with respect to the 
                //  curve ref date:
                Size dateSTride =
                    static_cast<Size>(Brent().solve(// casted from Real:
                    detail::Root(dfts, simDefaultProb), accuracy_, 0., 1.));
                /*
                // value if one approximates to a flat HR; 
                //   faster (>x2) but it introduces an error:..
                // \todo: see how to include this 'polymorphically'. While
                //   not the case in pricing in risk metrics/real  
                //   probabilities the curves are often flat
                static_cast<Size>(ceil(maxHorizon_ * 
                                    std::log(1.-simDefaultProb)
                /std::log(1.-data_.horizonDefaultPs_[iName])));
                */
                // Determine the realized recovery rate:
                /* For this; 'conditionalRecovery' needs to compute the pdef on 
                the realized def event date from the simulation. Yet, this might
                have fallen between todays date and the default TS reference 
                date(usually a two day gap) To avoid requesting a negative time
                probability the date is moved to the TS date 
                Unless the gap is ridiculous this has no practical effect for 
                the RR value*/
                Date today = Settings::instance().evaluationDate();
                Date eventDate = today+Period(static_cast<Integer>(dateSTride), 
                    Days);
                if(eventDate<dfts->referenceDate()) 
                    eventDate = dfts->referenceDate();
                Real latentRRVarSample = 
                    copula_->latentRRVarValue(values, iName);
                Real recovery = 
                    copula_->conditionalRecovery(latentRRVarSample,
                        iName, eventDate);
                this->simsBuffer_.back().push_back(
                  defaultSimEvent(iName, dateSTride, recovery));
                //emplace_back
            }
        /* Used to remove sims with no events. Uses less memory, faster 
        post-statistics. But only if all names in the portfolio have low 
        default probability, otherwise is more expensive and sim access has 
        to be modified. However low probability is also an indicator that 
        variance reduction is needed. */
        //if(simsBuffer.back().empty()) {
        //    emptySims_++;// Size; intilzd to zero
        //    simsBuffer.pop_back();
        //}
        }
    }


    // Common uses: Not valid in multithread version.
    // ---------- Gaussian default generators options ------------------------
    /* Uses copula direct normal inversion and MT generator 
    typedef RandomLossLM<GaussianCopulaPolicy,
        RandomSequenceGenerator<MersenneTwisterUniformRng> >
            GaussianRandomLossLM;
    */
    /* Uses BoxMuller for gaussian generation, bypassing copula inversions
    typedef RandomLossLM<GaussianCopulaPolicy, RandomSequenceGenerator<
        BoxMullerGaussianRng<MersenneTwisterUniformRng> > >
            GaussianRandomLossLM;
    */
    /* Default case, uses the copula inversion directly and sobol sequence */
    typedef RandomLossLM<GaussianCopulaPolicy> GaussianRandomLossLM;

    // ---------- T default generators options ----------------------------
    /*
    typedef RandomLossLM<TCopulaPolicy, 
      RandomSequenceGenerator<MersenneTwisterUniformRng> > TRandomLossLM;
    */
    /*
    typedef RandomLossLM<TCopulaPolicy, 
        RandomSequenceGenerator<PolarStudentTRng<MersenneTwisterUniformRng> > >
            TRandomLossLM;
    */
    typedef RandomLossLM<TCopulaPolicy> TRandomLossLM;

}

#endif
]]></document_content>
  </document>
  <document index="37">
    <source>recoveryratemodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_recovery_rate_model_hpp
#define quantlib_recovery_rate_model_hpp

#include <ql/settings.hpp>
#include <ql/handle.hpp>
#include <ql/experimental/credit/defaultprobabilitykey.hpp>
#include <ql/experimental/credit/recoveryratequote.hpp>

namespace QuantLib {

    /*! Models of the recovery rate provide future values of a recovery
        rate in the event of a default.
    */
    class RecoveryRateModel : public virtual Observable {
      public:
        /*! returns the expected recovery rate at a future time conditional
            on some default event type and seniority.
        */
        virtual Real recoveryValue(const Date& defaultDate,
            const DefaultProbKey& defaultKey = DefaultProbKey()) const {
            // no check on dates...
            return recoveryValueImpl(defaultDate, defaultKey);
        }
        /*! Returns true if the model will return recovery rates for
            the requested seniority.
        */
        virtual bool appliesToSeniority(Seniority) const = 0;
        ~RecoveryRateModel() override = default;

      protected:
        /*! Returns Null<Real> if unable to produce a recovery for
            the requested seniority.
        */
        virtual Real recoveryValueImpl(const Date&,
                                       const DefaultProbKey& defaultKey
                                       ) const = 0;
    };


    /*! Simple Recovery Rate model returning the constant value of the quote
        independently of the date and the seniority.
    */
    class ConstantRecoveryModel : public RecoveryRateModel,
                                  public Observer {
      public:
        explicit ConstantRecoveryModel(const Handle<RecoveryRateQuote>& quote);
        explicit ConstantRecoveryModel(Real recovery,
                                       Seniority sen = NoSeniority);
        void update() override { notifyObservers(); }
        bool appliesToSeniority(Seniority) const override { return true; }

      protected:
        /*! Notice the quote's value is returned without a
            check on a match of the seniorties of the
            quote and the request.
        */
        Real recoveryValueImpl(const Date&, const DefaultProbKey&) const override {
            // no match on requested seniority, all pass
            return quote_->value();
        }

      private:
        Handle<RecoveryRateQuote> quote_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="38">
    <source>recoveryratequote.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_recoveryrate_quote_hpp
#define quantlib_recoveryrate_quote_hpp

#include <ql/quote.hpp>
#include <ql/experimental/credit/defaulttype.hpp>
#include <map>

namespace QuantLib {

    //! Stores a recovery rate market quote and the associated seniority.
    class RecoveryRateQuote : public Quote {
        friend std::map<Seniority, Real> makeIsdaConvMap();
    public:
        /*! Returns a map with the ISDA conventional (values by
            default) of the recovery rate per each ISDA seniority.
        */
        static Real conventionalRecovery(Seniority sen) {
            return IsdaConvRecoveries[sen];
        }
        RecoveryRateQuote(Real value = Null<Real>(),
                          Seniority seniority = NoSeniority);
        //! \name Quote interface
        //@{
        Real value() const override;
        Seniority seniority() const;
        bool isValid() const override;
        //@}
        //! \name Modifiers
        //@{
        //! returns the difference between the new value and the old value
        Real setValue(Real value = Null<Real>());
        void reset();
        //@}

        /*! Turn a set of recoveries into a seniority-recovery map
            (intended to be used in an event construction)
        */
        // member? move to friend?
        template <Size N>
        static std::map<Seniority, Real> makeIsdaMap(const Real (&(arrayIsdaRR))[N]);

      private:
        // Conventional recoveries for ISDA seniorities
        static const Real IsdaConvRecoveries[];
        // The seniority this recovery is quoted for.
        Seniority seniority_;
        // The recovery value. In fractional units.
        Real recoveryRate_;
    };

    inline Seniority RecoveryRateQuote::seniority() const {
        return seniority_;
    }

    inline Real RecoveryRateQuote::value() const {
        QL_ENSURE(isValid(), "invalid Recovery Quote");
        return recoveryRate_;
    }

    inline bool RecoveryRateQuote::isValid() const {
        // not to be consufed with proper initialization [0-1]
        return recoveryRate_!=Null<Real>();/* &&
            seniority_ != NoSeniority;*/
    }

    //! Helper function for conventional recoveries. Returns the ISDA
    //    conventional recovery rates for the ISDA seniorities.
    std::map<Seniority, Real> makeIsdaConvMap();


    // template definitions

    // helpers allow further automatic inclusion of seniorities
    template <Size N>
    std::map<Seniority, Real> RecoveryRateQuote::makeIsdaMap(const Real (&(arrayIsdaRR))[N]) {
        // TO DO: include check on sizes... not to go beyond enum sizes.
        // TO DO: check Reals are valid, i.e. non Null and within [0-1] range
        std::map<Seniority, Real> isdaMap;
        for(Size i=0; i<N; i++) {
            auto isdaType = Seniority(i); // compiler dependent?
            isdaMap[isdaType] = arrayIsdaRR[i];
        }
        return isdaMap;
    }
}

#endif
]]></document_content>
  </document>
  <document index="39">
    <source>recursivelossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009, 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_recursive_loss_model_hpp
#define quantlib_recursive_loss_model_hpp

#include <ql/experimental/credit/constantlosslatentmodel.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <map>
#include <algorithm>

namespace QuantLib {

    /*! Recursive STCDO default loss model for a heterogeneous pool of names. 
    The pool names are heterogeneous in their default probabilities, notionals
    and recovery rates. Correlations are given by the latent model.
    The recursive pricing algorithm used here is described in Andersen, Sidenius
    and Basu; "All your hedges in one basket", Risk, November 2003, pages 67-72

        Notice that using copulas other than Gaussian it is only an
        approximation (see remark on p.68).

        \todo Make the loss unit equal to some small fraction depending on the
        portfolio loss weights (notionals and recoveries). As it is now this
        is ok for pricing but not for risk metrics. See the discussion in O'Kane
        18.3.2
        \todo Intengrands should all use the inverted probabilities for 
        performance instead of calling the copula inversion with the same vals.
    */
    template<class copulaPolicy> 
    class RecursiveLossModel : public DefaultLossModel {
    public:
      explicit RecursiveLossModel(
          const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> >& m,
          // nope! use max common divisor. See O'Kane. Or give both options at least.
          Size nbuckets = 1)
      : copula_(m), nBuckets_(nbuckets) {}

    private:
      /*!
      @param pDefDate Vector of unconditional default probabilities for each
      live name (at the current evaluation date). This is passed instead of
      the date for performance reasons (if in the future other magnitudes
      -e.g. lgd- are contingent on the date they shouldd be passed too).
      */
      Disposable<std::map<Real, Probability> >
      conditionalLossDistrib(const std::vector<Probability>& pDefDate,
                             const std::vector<Real>& mktFactor) const;
      Real expectedConditionalLoss(const std::vector<Probability>& pDefDate, //<< never used!!
                                   const std::vector<Real>& mktFactor) const;
      Disposable<std::vector<Real> > conditionalLossProb(const std::vector<Probability>& pDefDate,
                                                         // const Date& date,
                                                         const std::vector<Real>& mktFactor) const;
      // versions using the P-inverse, deprecate the former
      Disposable<std::map<Real, Probability> >
      conditionalLossDistribInvP(const std::vector<Real>& pDefDate,
                                 // const Date& date,
                                 const std::vector<Real>& mktFactor) const;
      Real expectedConditionalLossInvP(const std::vector<Real>& pDefDate,
                                       // const Date& date,
                                       const std::vector<Real>& mktFactor) const;
    protected:
      void resetModel() override;

    public:
        /*  Expected tranche Loss calculation.
            This is computed from the first equation on page 70 (not numbered)
            Notice that while we want to compute:
            \f[
            EL(t) = \sum_{l_k}l_k P(l;t) =
              \sum_{l_k}l_k \int P(l_k;t|\omega) d\omega q(\omega)
            \f]
            One can invert the sumation and the integral order to:
            \f[
            EL(t) = \int\,q(\omega)\,d\omega\,\sum_{l_k}\,l_k\,P(l_k;t|\omega) =
              \int\,q(\omega)\,d\omega\,EL(t|\omega)
            \f]
            and this is the way it is integrated here. The recursion formula 
            makes it easier this way.
        */
      Real expectedTrancheLoss(const Date& date) const override;
      Disposable<std::vector<Real> > lossProbability(const Date& date) const;
      // REMEBER THIS HAS TO BE MOVED TO A DISTRIBUTION OBJECT.............
      Disposable<std::map<Real, Probability> > lossDistribution(const Date& d) const override;
      // INTEGRATE THEN SEARCH RATHER THAN SEARCH AND THEN INTEGRATE:
      // Here I am not using a search because the point might not be attainable
      //  (loss distrib is not continuous)
      Real percentile(const Date& d, Real percentile) const override;
      Real expectedShortfall(const Date& d, Real perctl) const override;

    protected:
        const ext::shared_ptr<ConstantLossLatentmodel<copulaPolicy> > copula_;
    private:
        // loss model descriptor members
        const Size nBuckets_;
        mutable std::vector<Real> wk_;
        mutable Real lossUnit_;
        //! name to name factor. In the single factor copula:
        //    correl = beta * beta
        // When constructing through a single correlation number the factor is
        //   taken to be the positive swuare root of this number in the copula.
        ////////in the latent model now: mutable std::vector<Real> oneFactorCorrels_;
        // cached remaining basket magnitudes:
        mutable Real attachAmount_, 
            detachAmount_,
            notional_;
        mutable Size remainingBsktSize_;
        mutable std::vector<Real> notionals_;
    };


    typedef RecursiveLossModel<GaussianCopulaPolicy> RecursiveGaussLossModel;

    // Inlines ------------------------------------------------

    template<class CP>
    inline Real RecursiveLossModel<CP>::expectedTrancheLoss(
        const Date& date) const 
    {
/*
        std::map<Real, Probability> dist = lossDistribution(date);

        Real expLoss = 0.;
        std::map<Real, Probability>::iterator distIt = dist.begin();

        while(distIt != dist.end()) {
            Real loss = distIt->first * lossUnit_;
            loss = std::max(std::min(loss, detachAmount_)-attachAmount_, 0.);
            // MIN MAX BUGS ....??
            expLoss += loss * distIt->second;
            distIt++;
        }
        return expLoss ;




    ///////////////////////////////////////////////////////////////////////

        // calculate inverted unconditional Ps first so we save the inversion:
        // TO DO : turn to STL algorithm code
        std::vector<Probability> uncDefProb = 
            basket_->remainingProbabilities(date);

        return copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return expectedConditionalLoss(uncDefProb, v1);
            });
            */

        std::vector<Probability> uncDefProb = 
            basket_->remainingProbabilities(date);
        std::vector<Real> invProb;
        for(Size i=0; i<uncDefProb.size(); ++i)
           invProb.push_back(copula_->inverseCumulativeY(uncDefProb[i], i));
           ///  invProb.push_back(CP::inverseCumulativeY(uncDefProb[i], i));//<-static call
        return copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return expectedConditionalLossInvP(invProb, v1);
            });
    }

    template<class CP>
    inline Disposable<std::vector<Real> > 
    RecursiveLossModel<CP>::lossProbability(const Date& date) const {

        std::vector<Probability> uncDefProb = 
            basket_->remainingProbabilities(date);
        return copula_->integratedExpectedValueV(
            [&](const std::vector<Real>& v1) {
                return conditionalLossProb(uncDefProb, v1);
            });
    }

    // -------------------------------------------------------------------

    template<class CP>
    void RecursiveLossModel<CP>::resetModel() {
        // basket update:
        notionals_ = basket_->remainingNotionals();
        notional_  = basket_->remainingNotional();
        attachAmount_ = basket_->remainingAttachmentAmount();
        detachAmount_ = basket_->remainingDetachmentAmount();
        // model parameters:
        remainingBsktSize_ = notionals_.size();

        copula_->resetBasket(basket_.currentLink());

        std::vector<Real> lgdsTmp, lgds;
        for(Size i=0; i<remainingBsktSize_; ++i)
            lgds.push_back(notionals_[i]*(1.-copula_->recoveries()[i]));
        lgdsTmp = lgds;
        ///////////////std::remove(lgds.begin(), lgds.end(), 0.);
        lgds.erase(std::remove(lgds.begin(), lgds.end(), 0.), lgds.end());
        lossUnit_ = *(std::min_element(lgds.begin(), lgds.end()))
            / nBuckets_;
        for(Size i=0; i<remainingBsktSize_; ++i)
            wk_.push_back(std::floor(lgdsTmp[i]/lossUnit_ + .5));
    }

    // make it return a distribution object?
    template<class CP>
    Disposable<std::map<Real, Probability> > 
        RecursiveLossModel<CP>::lossDistribution(const Date& d) const 
    {
        std::map<Real, Probability> distrib;
        std::vector<Real> values  = lossProbability(d);
        Real sum = 0.;
        for(Size i=0; i<values.size(); ++i) {
            distrib.insert(std::make_pair<Real, Probability>(i * lossUnit_, 
                sum + values[i]));
            sum += values[i];
        }
        return distrib;
    }

    // Integrate then search rather than search and then integrate?
    // Here I am not using a search because the point might be not attainable 
    //   (loss distrib is not continuous) 
    template<class CP>
    Real RecursiveLossModel<CP>::percentile(const Date& d, 
        Real percentile) const 
    {
        std::map<Real, Probability> dist = lossDistribution(d);

        if(dist.begin()->second >=1.) return dist.begin()->first;

        // deterministic case (e.g. date requested is todays date)
        if(dist.size() == 1) return dist.begin()->first;

        if(percentile == 1.) return dist.rbegin()->second;
        if(percentile == 0.) return dist.begin()->second;
        std::map<Real, Probability>::const_iterator itdist = dist.begin();
        while (itdist->second <= percentile) ++itdist;
        Real valPlus = itdist->second;
        Real xPlus   = itdist->first;
        --itdist;  //we're never 1st or last, because of tests above
        Real valMin  = itdist->second;
        Real xMin    = itdist->first;

        // return xPlus-(xPlus-xMin)*(valPlus-percentile)/(valPlus-valMin);
        Real portfLoss =  xPlus-(xPlus-xMin)*(valPlus-percentile)
            /(valPlus-valMin);
        return //remainingNotional_ * 
            std::min(std::max(portfLoss - attachAmount_, 0.), 
                detachAmount_ - attachAmount_);/////(detach_ - attach_);
    }

    template<class CP>
    Real RecursiveLossModel<CP>::expectedShortfall(const Date& d, 
        Real perctl) const 
    {
        if(d == Settings::instance().evaluationDate()) return 0.;
        std::map<Real, Probability> distrib = lossDistribution(d);

        std::map<Real, Probability>::iterator itNxt, itDist = 
            distrib.begin();
        for(; itDist != distrib.end(); ++itDist)
            if(itDist->second >= perctl) break;
        itNxt = itDist;
        --itDist; // what if we are on the first one?!!!

        // One could linearly triangulate the exact point and get extra 
        // precission on the first(broken) period.
        if(itNxt != distrib.end()) { 
            Real lossNxt = std::min(std::max(itNxt->first - attachAmount_, 
                0.), detachAmount_ - attachAmount_);
            Real lossHere = std::min(std::max(itDist->first - attachAmount_,
                0.), detachAmount_ - attachAmount_);

            Real val =  lossNxt - (itNxt->second - perctl) * 
                (lossNxt - lossHere) / (itNxt->second - itDist->second); 
            Real suma = (itNxt->second - perctl) * (lossNxt + val) * .5;
            ++itDist; ++itNxt;
            do{
                lossNxt = std::min(std::max(itNxt->first - attachAmount_, 
                    0.), detachAmount_ - attachAmount_);
                lossHere = std::min(std::max(itDist->first - attachAmount_, 
                    0.), detachAmount_ - attachAmount_);
                suma += .5 * (lossHere + lossNxt) * (itNxt->second - 
                    itDist->second);
                ++itDist; ++itNxt;
            }while(itNxt != distrib.end());
            return suma / (1.-perctl);
        }
        return 0.;// well, we are in error....  fix: FAIL
    }

    template<class CP>
    Disposable<std::map<Real, Probability> >
        RecursiveLossModel<CP>::conditionalLossDistrib(
            const std::vector<Probability>& pDefDate, 
            //const Date& date,
            const std::vector<Real>& mktFactor) const 
    {
        //eq. 10 p.68
        //attainable losses distribution, recursive algorithm
        const std::vector<Probability>& uncDefProb = pDefDate;// alias, remove

        std::map<Real, Probability> pIndepDistrib;
        ////////  K=0
        pIndepDistrib.insert(std::make_pair(0., 1.));
        for(Size iName=0; iName<remainingBsktSize_; ++iName) {
            Probability pDef =
                copula_->conditionalDefaultProbability(uncDefProb[iName], iName,
                                                mktFactor);
            ////// iterate on all possible losses in the distribution:
            std::map<Real, Probability> pDistTemp;
            auto distIt = pIndepDistrib.begin();
            while(distIt != pIndepDistrib.end()) {
              ///   update prob if this name does not default
              auto matchIt = pDistTemp.find(distIt->first);
              if (matchIt != pDistTemp.end()) {
                  matchIt->second += distIt->second * (1. - pDef);
                }else{
                    pDistTemp.insert(std::make_pair(distIt->first,
                        distIt->second * (1.-pDef)));
                }
              ////   and if it does
                matchIt = pDistTemp.find(distIt->first + wk_[iName]);
                if(matchIt != pDistTemp.end()) {
                    matchIt->second += distIt->second * pDef;
                }else{
                    pDistTemp.insert(std::make_pair(
                        distIt->first+wk_[iName], distIt->second * pDef));
                }
                ++distIt;
            }
           /////  copy back
            pIndepDistrib = pDistTemp;
        }
        /* Apply tranche limits now .... mind you this could be done outside*/
        ////  to be done....
        return pIndepDistrib;
    }

    template<class CP>
    Disposable<std::map<Real, Probability> >
        // twice?! rewrite one in terms of the other, this is a duplicate!
        RecursiveLossModel<CP>::conditionalLossDistribInvP(
            const std::vector<Real>& invpDefDate, 
            //const Date& date,
            const std::vector<Real>& mktFactor) const 
    {
        // eq. 10 p.68
        // attainable losses distribution, recursive algorithm

        std::map<Real, Probability> pIndepDistrib;
        // K=0
        pIndepDistrib.insert(std::make_pair(0., 1.));
        for(Size iName=0; iName<remainingBsktSize_; ++iName) {
            Probability pDef =
                copula_->conditionalDefaultProbabilityInvP(invpDefDate[iName], 
                    iName, mktFactor);

            // iterate on all possible losses in the distribution:
            std::map<Real, Probability> pDistTemp;
            auto distIt = pIndepDistrib.begin();
            while(distIt != pIndepDistrib.end()) {
                // update prob if this name does not default
                auto matchIt = pDistTemp.find(distIt->first);
                if(matchIt != pDistTemp.end()) {
                    matchIt->second += distIt->second * (1.-pDef);
                }else{
                    pDistTemp.insert(std::make_pair(distIt->first,
                        distIt->second * (1.-pDef)));
                }
                // and if it does
                matchIt = pDistTemp.find(distIt->first + wk_[iName]);
                if(matchIt != pDistTemp.end()) {
                    matchIt->second += distIt->second * pDef;
                }else{
                    pDistTemp.insert(std::make_pair(
                        distIt->first+wk_[iName], distIt->second * pDef));
                }
                ++distIt;
            }
            // copy back
            pIndepDistrib = pDistTemp;
        }
        /* Apply tranche limits now .... mind you this could be done outside*/
        return pIndepDistrib;
    }




    /*
    Bugs here???. The max min on the tranche looks 
    wrong. It is better to have a tranche function since that way we can avoid 
    adding up losses over all the posible losses rather than just over the 
    tranche limits.
    */
    //! Portfolio loss conditional to the market factor value
    template<class CP>
    Real RecursiveLossModel<CP>::expectedConditionalLoss(
        const std::vector<Probability>& pDefDate, 
        //const Date& date,
        const std::vector<Real>& mktFactor) const 
    {
        std::map<Real, Probability> pIndepDistrib =
            conditionalLossDistrib(pDefDate, mktFactor);

        // get the expected value subject to the value of the market
        //   factor.
        Real expLoss = 0.;
        //---------------------------------------------------------------
        /* This is the original (easy to read) loop which I have partially
             unroll below to take profit of the fact that once we go over
             the tranche top the loss amount is fixed:
        */
        auto distIt = pIndepDistrib.begin();

        while(distIt != pIndepDistrib.end()) {
            Real loss = distIt->first * lossUnit_;
     //       loss = std::max(std::min(loss, detachAmount_)-attachAmount_, 0.);
            loss = std::min(std::max(loss - attachAmount_, 0.), 
                detachAmount_ - attachAmount_);
            // MIN MAX BUGS ....??
            expLoss += loss * distIt->second;
            ++distIt;
        }
        return expLoss ;
    }

    template<class CP>
    // again, I am duplicating code.
    Real RecursiveLossModel<CP>::expectedConditionalLossInvP(
                                 const std::vector<Real>& invPDefDate, 
                                 //const Date& date,
                                 const std::vector<Real>& mktFactor) const 
    {
        std::map<Real, Probability> pIndepDistrib =
            conditionalLossDistribInvP(invPDefDate, mktFactor);

        // get the expected value subject to the value of the market
        //   factor.
        Real expLoss = 0.;
        //---------------------------------------------------------------
        /* This is the original (easy to read) loop which I have partially
             unroll below to take profit of the fact that once we go over
             the tranche top the loss amount is fixed:
        */
        auto distIt = pIndepDistrib.begin();

        while(distIt != pIndepDistrib.end()) {
            Real loss = distIt->first * lossUnit_;
   //         loss = std::max(std::min(loss, detachAmount_)-attachAmount_, 0.);
            loss = std::min(std::max(loss - attachAmount_, 0.), 
                detachAmount_ - attachAmount_);
            // MIN MAX BUGS ....???
            expLoss += loss * distIt->second;
            ++distIt;
        }
        return expLoss ;
    }

    template<class CP>
    Disposable<std::vector<Real> > RecursiveLossModel<CP>::conditionalLossProb(
        const std::vector<Probability>& pDefDate, 
        //const Date& date,
        const std::vector<Real>& mktFactor) const 
    {
        std::map<Real, Probability> pIndepDistrib =
            conditionalLossDistrib(pDefDate, mktFactor);

        std::vector<Real> results;
        auto distIt = pIndepDistrib.begin();
        while(distIt != pIndepDistrib.end()) {
            //Real loss = distIt->first * loss_unit_
            //                    ;
            //loss = std::max(std::min(loss,
            //    results_.xMax)-results_.xMin, 0.);
            //expLoss += loss * distIt->second;

            results.push_back(distIt->second);
             ++distIt;
        }
        return results;
    }

}

#endif
]]></document_content>
  </document>
  <document index="40">
    <source>riskyassetswap.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008, 2009 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file riskyassetswap.hpp
    \brief Risky asset-swap instrument
*/

#ifndef quantlib_risky_asset_swap_hpp
#define quantlib_risky_asset_swap_hpp

#include <ql/instrument.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <ql/termstructures/yieldtermstructure.hpp>
#include <ql/termstructures/credit/defaultprobabilityhelpers.hpp>
#include <ql/time/schedule.hpp>

namespace QuantLib {

    //! Risky asset-swap instrument
    class RiskyAssetSwap : public Instrument {
      public:
        RiskyAssetSwap(bool fixedPayer,
                       Real nominal,
                       Schedule fixedSchedule,
                       Schedule floatSchedule,
                       DayCounter fixedDayCounter,
                       DayCounter floatDayCounter,
                       Rate spread,
                       Rate recoveryRate_,
                       Handle<YieldTermStructure> yieldTS,
                       Handle<DefaultProbabilityTermStructure> defaultTS,
                       Rate coupon = Null<Rate>());

        Real fairSpread ();

        Real floatAnnuity() const;

        Real nominal() const { return nominal_; }
        Rate spread() const { return spread_; }
        bool fixedPayer() const { return fixedPayer_; }

      private:
        void setupExpired() const override;
        bool isExpired() const override;
        void performCalculations() const override;

        Real fixedAnnuity() const;
        Real parCoupon() const;
        Real recoveryValue() const;
        Real riskyBondPrice() const;

        // calculated values
        mutable Real fixedAnnuity_;
        mutable Real floatAnnuity_;
        mutable Real parCoupon_;
        mutable Real recoveryValue_;
        mutable Real riskyBondPrice_;

        // input
        bool fixedPayer_;
        Real nominal_;
        Schedule fixedSchedule_, floatSchedule_;
        DayCounter fixedDayCounter_, floatDayCounter_;
        Rate spread_;
        Rate recoveryRate_;
        Handle<YieldTermStructure> yieldTS_;
        Handle<DefaultProbabilityTermStructure> defaultTS_;
        mutable Real coupon_;
    };


    // risky-asset-swap helper for probability-curve bootstrap
    class AssetSwapHelper : public DefaultProbabilityHelper {
      public:
        AssetSwapHelper(const Handle<Quote>& spread,
                        const Period& tenor,
                        Natural settlementDays,
                        Calendar calendar,
                        const Period& fixedPeriod,
                        BusinessDayConvention fixedConvention,
                        DayCounter fixedDayCount,
                        const Period& floatPeriod,
                        BusinessDayConvention floatConvention,
                        DayCounter floatDayCount,
                        Real recoveryRate,
                        const RelinkableHandle<YieldTermStructure>& yieldTS,
                        const Period& integrationStepSize = Period());
        Real impliedQuote() const override;
        void setTermStructure(DefaultProbabilityTermStructure*) override;

      private:
        void update() override;
        void initializeDates();

        Period tenor_;
        Natural settlementDays_;
        Calendar calendar_;
        BusinessDayConvention fixedConvention_;
        Period fixedPeriod_;
        DayCounter fixedDayCount_;
        BusinessDayConvention floatConvention_;
        Period floatPeriod_;
        DayCounter floatDayCount_;
        Real recoveryRate_;
        RelinkableHandle<YieldTermStructure> yieldTS_;
        Period integrationStepSize_;

        Date evaluationDate_;
        ext::shared_ptr<RiskyAssetSwap> asw_;
        RelinkableHandle<DefaultProbabilityTermStructure> probability_;
    };

}

#endif

]]></document_content>
  </document>
  <document index="41">
    <source>riskyassetswapoption.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file riskyassetswapoption.hpp
    \brief option on risky asset swap
*/

#ifndef quantlib_risky_asset_swap_option_hpp
#define quantlib_risky_asset_swap_option_hpp

#include <ql/experimental/credit/riskyassetswap.hpp>

namespace QuantLib {

    //! %Option on risky asset swap
    /*! \ingroup credit */
    class RiskyAssetSwapOption : public Instrument {
      public:
        RiskyAssetSwapOption(ext::shared_ptr<RiskyAssetSwap> asw,
                             const Date& expiry,
                             Rate marketSpread,
                             Volatility spreadVolatility);

      private:
        bool isExpired() const override;
        void performCalculations() const override;

        ext::shared_ptr<RiskyAssetSwap> asw_;
        Date expiry_;
        Rate marketSpread_;
        Volatility spreadVolatility_;
    };

}

#endif
]]></document_content>
  </document>
  <document index="42">
    <source>riskybond.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file riskybond.hpp
    \brief Defaultable bonds
*/

#ifndef quantlib_riskybond_hpp
#define quantlib_riskybond_hpp

#include <ql/instrument.hpp>
#include <ql/experimental/credit/issuer.hpp>
#include <ql/default.hpp>
#include <ql/time/schedule.hpp>
#include <ql/time/daycounter.hpp>
#include <ql/cashflow.hpp>
#include <ql/indexes/iborindex.hpp>
#include <ql/patterns/lazyobject.hpp>
#include <ql/termstructures/yieldtermstructure.hpp>
#include <ql/experimental/credit/pool.hpp>
#include <ql/termstructures/defaulttermstructure.hpp>
#include <ql/currency.hpp>

namespace QuantLib {

    /*! Base class for default risky bonds
      \ingroup credit
    */
    class RiskyBond : public Instrument {
    public:
        /*! The value is contingent to survival, i.e., the knockout
            probability is considered.  To compute the npv given that
            the issuer has survived, use the riskfreeNPV().

            In each of the \f$n\f$ coupon periods, we can calculate the value
            in the case of survival and default, assuming that the issuer
            can only default in the middle of a coupon period. We denote this time
            \f$T_{i}^{mid}=\frac{T_{i-1}+T_{i}}{2}\f$.

            Given survival we receive the full cash flow (both coupons and notional).
            The time \f$t\f$ value of these payments are given by
            \f[
                \sum_{i=1}^{n}CF_{i}P(t,T_{i})Q(T_{i}<\tau)
            \f]
            where \f$P(t,T)\f$ is the time \f$T\f$ discount bond
            and \f$Q(T<\tau)\f$ is the time \f$T\f$ survival probability.
            \f$n\f$ is the number of coupon periods. This takes care of the payments
            in the case of survival.

            Given default we receive only a fraction of the notional at default.
            \f[
                \sum_{i=1}^{n}Rec N(T_{i}^{mid}) P(t,T_{i}^{mid})Q(T_{i-1}<\tau\leq T_{i})
            \f]
            where \f$Rec\f$ is the recovery rate and \f$N(T)\f$ is the time T notional. The default probability can be
            rewritten as
            \f[
                Q(T_{i-1}<\tau\leq T_{i})=Q(T_{i}<\tau)-Q(T_{i-1}<\tau)=(1-Q(T_{i}\geq\tau))-(1-Q(T_{i-1}\geq\tau))=Q(T_{i-1}\geq\tau)-Q(T_{i}\geq\tau)
            \f]
        */
      RiskyBond(std::string name,
                Currency ccy,
                Real recoveryRate,
                Handle<DefaultProbabilityTermStructure> defaultTS,
                Handle<YieldTermStructure> yieldTS,
                Natural settlementDays = 0,
                Calendar calendar = Calendar());
      ~RiskyBond() override = default;
      virtual std::vector<ext::shared_ptr<CashFlow> > cashflows() const = 0;
      std::vector<ext::shared_ptr<CashFlow> > expectedCashflows();
      virtual Real notional(Date date = Date::minDate()) const = 0;
      virtual Date effectiveDate() const = 0;
      virtual Date maturityDate() const = 0;
      virtual std::vector<ext::shared_ptr<CashFlow> > interestFlows() const = 0;
      virtual std::vector<ext::shared_ptr<CashFlow> > notionalFlows() const = 0;
      Real riskfreeNPV() const;
      Real totalFutureFlows() const;
      std::string name() const;
      Currency ccy() const;
      Handle<YieldTermStructure> yieldTS() const;
      Handle<DefaultProbabilityTermStructure> defaultTS() const;
      Real recoveryRate() const;
      //! \name Instrument interface
      //@{
      bool isExpired() const override;
      //@}
    protected:
      void setupExpired() const override;
      void performCalculations() const override;

    private:
        std::string name_;
        Currency ccy_;
        Real recoveryRate_;
        Handle<DefaultProbabilityTermStructure> defaultTS_;
        Handle<YieldTermStructure> yieldTS_;
    protected:
        // engines data
        Natural settlementDays_;
        Calendar calendar_;
    };

    inline std::string RiskyBond::name() const {
        return name_;
    }

    inline Currency RiskyBond::ccy() const {
        return ccy_;
    }

    inline Handle<YieldTermStructure> RiskyBond::yieldTS() const {
        return yieldTS_;
    }

    inline Handle<DefaultProbabilityTermStructure>
    RiskyBond::defaultTS() const {
        return defaultTS_;
    }

    inline Real RiskyBond::recoveryRate() const {
        return recoveryRate_;
    }

    /*! Default risky fixed bond
      \ingroup credit
    */
    class RiskyFixedBond : public RiskyBond {
    public:
      RiskyFixedBond(const std::string& name,
                     const Currency& ccy,
                     Real recoveryRate,
                     const Handle<DefaultProbabilityTermStructure>& defaultTS,
                     const Schedule& schedule,
                     Real rate,
                     DayCounter dayCounter,
                     BusinessDayConvention paymentConvention,
                     std::vector<Real> notionals,
                     const Handle<YieldTermStructure>& yieldTS,
                     Natural settlementDays = 0);
      std::vector<ext::shared_ptr<CashFlow> > cashflows() const override;
      Real notional(Date date = Date::minDate()) const override;
      Date effectiveDate() const override;
      Date maturityDate() const override;
      std::vector<ext::shared_ptr<CashFlow> > interestFlows() const override;
      std::vector<ext::shared_ptr<CashFlow> > notionalFlows() const override;

    private:
      Schedule schedule_;
      Real rate_;
      DayCounter dayCounter_;
      // BusinessDayConvention paymentConvention_;
      std::vector<Real> notionals_;
      std::vector<ext::shared_ptr<CashFlow> > leg_;
      std::vector<ext::shared_ptr<CashFlow> > interestLeg_;
      std::vector<ext::shared_ptr<CashFlow> > redemptionLeg_;
    };


    /*! Default risky floating bonds
      \ingroup credit
    */
    class RiskyFloatingBond : public RiskyBond {
    public:
      RiskyFloatingBond(const std::string& name,
                        const Currency& ccy,
                        Real recoveryRate,
                        const Handle<DefaultProbabilityTermStructure>& defaultTS,
                        const Schedule& schedule,
                        ext::shared_ptr<IborIndex> index,
                        Integer fixingDays,
                        Real spread,
                        std::vector<Real> notionals,
                        const Handle<YieldTermStructure>& yieldTS,
                        Natural settlementDays = 0);
      std::vector<ext::shared_ptr<CashFlow> > cashflows() const override;
      Real notional(Date date = Date::minDate()) const override;
      Date effectiveDate() const override;
      Date maturityDate() const override;
      std::vector<ext::shared_ptr<CashFlow> > interestFlows() const override;
      std::vector<ext::shared_ptr<CashFlow> > notionalFlows() const override;

    private:
        Schedule schedule_;
        ext::shared_ptr<IborIndex> index_;
        DayCounter dayCounter_;
        Integer fixingDays_;
        Real spread_;
        // BusinessDayConvention paymentConvention_;
        std::vector<Real> notionals_;
        std::vector<ext::shared_ptr<CashFlow> > leg_;
        std::vector<ext::shared_ptr<CashFlow> > interestLeg_;
        std::vector<ext::shared_ptr<CashFlow> > redemptionLeg_;
    };

}


#endif
]]></document_content>
  </document>
  <document index="43">
    <source>saddlepointlossmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_saddle_point_lossmodel_hpp
#define quantlib_saddle_point_lossmodel_hpp

#include <ql/tuple.hpp>
#include <ql/math/solvers1d/brent.hpp>
#include <ql/math/solvers1d/newton.hpp>
#include <ql/experimental/credit/basket.hpp>
#include <ql/experimental/credit/defaultlossmodel.hpp>
#include <ql/experimental/credit/constantlosslatentmodel.hpp>

namespace QuantLib {

    /*! \brief Saddle point portfolio credit default loss model.\par
      Default Loss model implementing the Saddle point expansion 
      integrations on several default risk metrics. Codepence is dealt 
      through a latent model making the integrals conditional to the latent 
      model factor. Latent variables are integrated indirectly.\par
    See:\par
    <b>Taking to the saddle</b> by R.Martin, K.Thompson and C.Browne; RISK JUNE 
        2001; p.91\par
    <b>The saddlepoint method and portfolio optionalities</b> R.Martin in Risk 
        December 2006\par
    <b>VAR: who contributes and how much?</b> R.Martin, K.Thompson and 
        C.Browne RISK AUGUST 2001\par
    <b>Shortfall: Who contributes and how much?</b> R. J. Martin, Credit Suisse 
        January 3, 2007 \par
    <b>Don't Fall from the Saddle: the Importance of Higher Moments of Credit 
        Loss Distributions</b> J.Annaert, C.Garcia Joao Batista, J.Lamoot, 
        G.Lanine February 2006, Gent University\par
    <b>Analytical techniques for synthetic CDOs and credit default risk 
        measures</b> A. Antonov, S. Mechkovy, and T. Misirpashaevz; 
        NumeriX May 23, 2005 \par
    <b>Computation of VaR and VaR contribution in the Vasicek portfolio credit 
        loss model: a comparative study</b> X.Huang, C.W.Oosterlee, M.Mesters
        Journal of Credit Risk (75-96) Volume 3/ Number 3, Fall 2007 \par
    <b>Higher-order saddlepoint approximations in the Vasicek portfolio credit 
        loss model</b> X.Huang, C.W.Oosterlee, M.Mesters  Journal of 
        Computational Finance (93-113) Volume 11/Number 1, Fall 2007 \par
    While more expensive, a high order expansion is used here; see the paper by 
    Antonov et al for the terms retained.\par
    For a discussion of an alternative to fix the error at low loss levels 
    (more relevant to pricing than risk metrics) see: \par
    <b>The hybrid saddlepoint method for credit portfolios</b> by A.Owen, 
    A.McLeod and K.Thompson; in Risk, August 2009. This is not implemented here
    though (yet?...)\par
    For the more general context mathematical theory see: <b>Saddlepoint 
    approximations with applications</b> by R.W. Butler, Cambridge series in 
    statistical and probabilistic mathematics. 2007 \par
    \todo Some portfolios show instabilities in the high order expansion terms.
    \todo Methods here are calling and integrating using the unconditional 
        probabilities without inverting them first; quite a lot of calls to 
        the copula inversion can be avoided; this should improve performance.
    \todo Revise the model for stability of the saddle point calculation. The
        search for the point does not convege in extreme cases; e.g. very high
        value of all the factors; factors for each variable not ordered from 
        high to low,...
    */

    /* The treatment of recovery wont work with random recoveries, they should
    be passed to the conditional methods in the same way as the probabilities.
    */

    /*
    TO DO:
    -> Failing when the tranche upper loss limit goes over the max attainable 
        loss.

    - With 15 quadrature points things are OK but 25 gives me -1#IND000 errors 
            (over region around the EL I think) 
    - Silly bug when calling some methods on todays date (zero time). 
            ProbDef = 0 there
    - VaR <- tranched?????!
    - ESF <- tranched????!!
    - VaR split
    - ESF split?

    When introducing defaults; somewhere, (after an update?) there should be 
    a check that: copula_->basketSize() EQUALS remainingBasket_.size()
    */
    template<class CP> 
    class SaddlePointLossModel : public DefaultLossModel {
    public:
        explicit SaddlePointLossModel(
            const ext::shared_ptr<ConstantLossLatentmodel<CP> >& m)
            : copula_(m) { }
    protected:
        // ----------- Cumulants and derivatives auxiliary functions ---------

        /*! Returns the cumulant generating function (zero-th order 
        expansion term) conditional to the mkt factor:
            \f$ K = \sum_j ln(1-p_j + p_j e^{N_j \times lgd_j \times s}) \f$
        */
        Real CumulantGeneratingCond(
            const std::vector<Real>& invUncondProbs,
            Real lossFraction,// saddle pt
            const std::vector<Real>&  mktFactor) const;
        /*! Returns the first derivative of the cumulant generating function 
        (first order expansion term) conditional to the mkt factor:
           \f$ K1 = \sum_j \frac{p_j \times N_j \times LGD_j \times 
                e^{N_j \times LGD_j \times s}} \
                             {1-p_j + p_j e^{N_j \times LGD_j \times s}} \f$
           One of its properties is that its value at zero is the portfolio 
           expected loss (in fractional units). Its value at infinity is the 
           max attainable portfolio loss. To be understood conditional to the 
           market factor.
        */
        Real CumGen1stDerivativeCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, // in fract loss units... humm not really
            const std::vector<Real>&  mktFactor) const;
        /*! Returns the second derivative of the cumulant generating function 
        (first order expansion term) conditional to the mkt factor:
            \f$ K2 = \sum_j \frac{p_j \times (N_j \times LGD_j)^2 \times 
                e^{N_j \times LGD_j \times s}}
                             {1-p_j + p_j e^{N_j \times LGD_j \times s}}
                      - (\frac{p_j \times N_j \times LGD_j \times e^{N_j \times 
                      LGD_j \times s}}
                             {1-p_j + p_j e^{N_j \times LGD_j \times s}})^2 \f$
        */
        Real CumGen2ndDerivativeCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, 
            const std::vector<Real>&  mktFactor) const;
        Real CumGen3rdDerivativeCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, 
            const std::vector<Real>&  mktFactor) const;
        Real CumGen4thDerivativeCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, 
            const std::vector<Real>&  mktFactor) const ;
        /*! Returns the cumulant and second to fourth derivatives together.
          Included for optimization, most methods work on expansion of these 
          terms.
          Alternatively use a local private buffer member? */
        ext::tuple<Real, Real, Real, Real> CumGen0234DerivCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, 
            const std::vector<Real>&  mktFactor) const;
        ext::tuple<Real, Real> CumGen02DerivCond(
            const std::vector<Real>& invUncondProbs,
            Real saddle, 
            const std::vector<Real>&  mktFactor) const;

        /* Unconditional cumulants. Because this class integrates the various
          statistics it provides in indirect mode they are never used. 
          Provided for completeness/extendability */
        /*! Returns the cumulant generating function (zero-th order expansion
          term) weighting the conditional value by the prob density of the 
          market factor, called by integrations */
        Real CumulantGenerating(const Date& date, Real s) const;
        Real CumGen1stDerivative(const Date& date, Real s) const;
        Real CumGen2ndDerivative(const Date& date, Real s) const;
        Real CumGen3rdDerivative(const Date& date, Real s) const;
        Real CumGen4thDerivative(const Date& date, Real s) const;
        
        // -------- Saddle point search functions ---------------------------
        class SaddleObjectiveFunction {
            const SaddlePointLossModel& me_;
            Real targetValue_;
            const std::vector<Real>& mktFactor_;
            const std::vector<Real>& invUncondProbs_;
        public:
            //! The passed target is in fractional loss units
            SaddleObjectiveFunction(const SaddlePointLossModel& me,
                                    const Real target,
                                    const std::vector<Real>& invUncondProbs,
                                    const std::vector<Real>& mktFactor
                                    )
            : me_(me), 
              targetValue_(target), 
              mktFactor_(mktFactor), 
              invUncondProbs_(invUncondProbs)
            {}
            Real operator()(const Real x) const {
                return me_.CumGen1stDerivativeCond(invUncondProbs_, x, 
                    mktFactor_) - targetValue_;
            }
            Real derivative(Real x) const {
                return me_.CumGen2ndDerivativeCond(invUncondProbs_, x, 
                    mktFactor_);
            }
        };

        /*! Calculates the mkt-fct-conditional saddle point for the loss level 
            given and the probability passed. 
            The date is implicitly given through the probability. Performance 
            requires to pass the probabilities for that date. Otherwise once we
            integrate this over the market factor we would be computing the same
            probabilities over and over. While this works fine here some models
            of the recovery rate might require the date.

            The passed lossLevel is in total portfolio loss fractional units.

            \todo Improve convergence speed (which is bad at the moment).See 
            discussion in several places; references above and The Oxford 
            Handbook of CD, sect 2.9
        */
        Real findSaddle(
            const std::vector<Real>& invUncondProbs,
            Real lossLevel,
            const std::vector<Real>& mktFactor, 
            Real accuracy = 1.0e-3,//1.e-4
            Natural maxEvaluations = 50
            ) const;

        class SaddlePercObjFunction {
            const SaddlePointLossModel& me_;
            Real targetValue_;
            Date date_;
        public:
            SaddlePercObjFunction(
                const SaddlePointLossModel& me,
                const Real target,
                const Date& date)
            : me_(me), targetValue_(1.-target), date_(date) {}
            /*! The passed x is the _tranche_ loss fraction */
            Real operator()(const Real x) const {
                return me_.probOverLoss(date_, x) - targetValue_;
            }
        };
        // Functionality, Provides various portfolio statistics---------------
    public:
        /*! Returns the loss amount at the requested date for which the 
        probability of lossing that amount or less is equal to the value passed.
        */
      Real percentile(const Date& d, Probability percentile) const override;

    protected:
        /*! Conditional (on the mkt factor) prob of a loss fraction of
            the the tranched portfolio.

            The trancheLossFract parameter is the fraction over the
            tranche notional and must be in [0,1].
        */
        Probability probOverLossCond( 
            const std::vector<Real>& invUncondProbs,
            Real trancheLossFract, 
            const std::vector<Real>& mktFactor) const;
        Probability probOverLossPortfCond1stOrder(
            const std::vector<Real>& invUncondProbs,
            Real loss, 
            const std::vector<Real>& mktFactor) const;
    public:
      Probability probOverLoss(const Date& d, Real trancheLossFract) const override;

      Disposable<std::map<Real, Probability> > lossDistribution(const Date& d) const override;

    protected:
        /*! 
            Probability of having losses in the portfolio due to default 
            events equal or larger than a given absolute loss value on a 
            given date conditional to the latent model factor.
            The integral expression on the expansion is the first order 
            integration as presented in several references, see for instance; 
            equation 8 in R.Martin, K.Thompson, and C. Browne 's 
            'Taking to the Saddle', Risk Magazine, June 2001, page 91

            The passed loss is in absolute value.
        */
        Probability probOverLossPortfCond(
            const std::vector<Real>& invUncondProbs,
            Real loss, 
            const std::vector<Real>& mktFactor) const;
    public:
        Probability probOverPortfLoss(const Date& d, Real loss) const;
        Real expectedTrancheLoss(const Date& d) const override;

      protected:
        /*!
        Probability density of having losses in the total portfolio (untranched)
        due to default events equal to a given value on a given date conditional
        to the latent model factor.
        Based on the integrals of the expected shortfall. 
        */
        Probability probDensityCond(const std::vector<Real>& invUncondProbs,
            Real loss, const std::vector<Real>& mktFactor) const;
    public:
        Probability probDensity(const Date& d, Real loss) const;
    protected:
        Disposable<std::vector<Real> > splitLossCond(
            const std::vector<Real>& invUncondProbs,
            Real loss, std::vector<Real> mktFactor) const;
        Real expectedShortfallFullPortfolioCond(
            const std::vector<Real>& invUncondProbs,
            Real lossPerc, const std::vector<Real>& mktFactor) const;
        Real expectedShortfallTrancheCond(
            const std::vector<Real>& invUncondProbs,
            Real lossPerc, Probability percentile, 
            const std::vector<Real>& mktFactor) const;
        Disposable<std::vector<Real> > expectedShortfallSplitCond(
            const std::vector<Real>& invUncondProbs,
            Real lossPerc, const std::vector<Real>& mktFactor) const;
    public:
        /*! Sensitivities of the individual names to a given portfolio loss 
            value due to defaults. It returns ratios to the total structure 
            notional, which aggregated add up to the requested loss value.
            Notice then that it refers to the total portfolio, not the tranched
            basket.
            \todo  Fix this.
            \par
            see equation 8 in <b>VAR: who contributes and how much?</b> by 
            R.Martin, K.Thompson, and C. Browne in Risk Magazine, August 2001

            The passed loss is the loss amount level at which we want
            to request the sensitivity. Equivalent to a percentile.
        */
      Disposable<std::vector<Real> > splitVaRLevel(const Date& date, Real loss) const override;
      Real expectedShortfall(const Date& d, Probability percentile) const override;

    protected:
        Real conditionalExpectedLoss(
            const std::vector<Real>& invUncondProbs,
            const std::vector<Real>& mktFactor) const;
        Real conditionalExpectedTrancheLoss(
            const std::vector<Real>& invUncondProbs,
            const std::vector<Real>& mktFactor) const;

        void resetModel() override {
            remainingNotionals_ = basket_->remainingNotionals();
            remainingNotional_  = basket_->remainingNotional();
            attachRatio_ = std::min(basket_->remainingAttachmentAmount() 
                / basket_->remainingNotional(), 1.);
            detachRatio_ = std::min(basket_->remainingDetachmentAmount() 
                / basket_->remainingNotional(), 1.);
            copula_->resetBasket(basket_.currentLink());
        }

        const ext::shared_ptr<ConstantLossLatentmodel<CP> > copula_;
        // cached todays arguments values
        mutable Size remainingSize_;
        mutable std::vector<Real> remainingNotionals_;
        mutable Real remainingNotional_;
        // remaining basket levels:
        mutable Real attachRatio_, detachRatio_;
        /*
        // Just for testing the ESF direct integration, not for release, 
        //   this is very inneficient:
        class ESFIntegrator {
        public:
            ESFIntegrator(const SaddlePointLossModel& me,
                const Date& date,
                Real lossPercentileFract//,
                //const std::vector<Real>& mktFactor
                )
                : me_(me), date_(date),lossPercentileFract_(lossPercentileFract)
            {}


            Real operator()(Real x) const {
                return me_.densityTrancheLoss(date_, x + lossPercentileFract_) 
                    * (x + lossPercentileFract_);
            }

            Real lossPercentileFract_;
            Date date_;
            //  const std::vector<Real>& mktFactor_;
            const SaddlePointLossModel& me_;
        };
        */
    };


    // -- Inlined integrations------------------------------------------------

    // Unconditional Moments and derivatives. --------------------------------
    template<class CP>
    inline Real SaddlePointLossModel<CP>::CumulantGenerating(
        const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return CumulantGeneratingCond(invUncondProbs, s, v1);
            });
    }

    template<class CP>
    inline Real SaddlePointLossModel<CP>::CumGen1stDerivative(
        const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

       return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return CumGen1stDerivativeCond(invUncondProbs, s, v1);
           });
    }

    template<class CP>
    inline Real SaddlePointLossModel<CP>::CumGen2ndDerivative(
        const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return CumGen2ndDerivativeCond(invUncondProbs, s, v1);
           });
    }

    template<class CP>
    inline Real SaddlePointLossModel<CP>::CumGen3rdDerivative(
        const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return CumGen3rdDerivativeCond(invUncondProbs, s, v1);
           });
    }

    template<class CP>
    inline Real SaddlePointLossModel<CP>::CumGen4thDerivative(
        const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return CumGen4thDerivativeCond(invUncondProbs, s, v1);
           });
    }

    template<class CP>
    inline Probability SaddlePointLossModel<CP>::probOverLoss(
        const Date& d, Real trancheLossFract) const 
    {
        // avoid computation:
        if (trancheLossFract >= 
            // time dependent soon:
            basket_->detachmentAmount()) return 0.;

        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(d);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return probOverLossCond(invUncondProbs, trancheLossFract, v1);
           });
    }

    template<class CP>
    inline Probability SaddlePointLossModel<CP>::probOverPortfLoss(
        const Date& d, Real loss) const 
    {
        std::vector<Probability> invUncondProbs = 
            basket_->remainingProbabilities(d);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return probOverLossPortfCond(invUncondProbs, loss, v1);
           });
    }

    template<class CP>
    inline Real SaddlePointLossModel<CP>::expectedTrancheLoss(
        const Date& d) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(d);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return conditionalExpectedTrancheLoss(invUncondProbs, v1);
           });
    }

    template<class CP>
    inline Probability SaddlePointLossModel<CP>::probDensity(
        const Date& d, Real loss) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(d);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValue(
           [&](const std::vector<Real>& v1) {
               return probDensityCond(invUncondProbs, loss, v1);
           });
    }

    template<class CP>
    inline Disposable<std::vector<Real> > 
    SaddlePointLossModel<CP>::splitVaRLevel(const Date& date, Real s) const 
    {
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(date);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
            copula_->inverseCumulativeY(invUncondProbs[i], i);

        return copula_->integratedExpectedValueV(
           [&](const std::vector<Real>& v1) {
               return splitLossCond(invUncondProbs, s, v1);
           });
    }







    /* ------------------------------------------------------------------------
                    Conditional Moments and derivatives. 

        Notice that in all this methods the date dependence is implicitly
        present in the unconditional probabilities. But, as in other LMs, it
        is redundant and expensive to perform the call to the probabilities in
        these methods since they are integrands.
       ---------------------------------------------------------------------- */

    template<class CP>
    Real SaddlePointLossModel<CP>::CumulantGeneratingCond(
        const std::vector<Real>& invUncondProbs,
        Real lossFraction,
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real sum = 0.;

        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            sum += std::log(1. - pBuffer + 
                pBuffer * std::exp(remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName],
                    iName, mktFactor)) * lossFraction / remainingNotional_));
        }
       return sum;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::CumGen1stDerivativeCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle,
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real sum = 0.;

        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            // loss in fractional units
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;
            Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            sum += lossInDef * midFactor / (1.-pBuffer + midFactor);
        }
       return sum;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::CumGen2ndDerivativeCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle, 
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real sum = 0.;

        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            // loss in fractional units
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;
            Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            Real denominator = 1.-pBuffer + midFactor;
            sum += lossInDef * lossInDef * midFactor / denominator - 
                std::pow(lossInDef * midFactor / denominator , 2.);
        }
       return sum;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::CumGen3rdDerivativeCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle, 
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real sum = 0.;

        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;

            const Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            const Real denominator = 1.-pBuffer + midFactor;

            const Real& suma0 = denominator;
            const Real suma1  = lossInDef * midFactor;
            const Real suma2  = lossInDef * suma1;
            const Real suma3  = lossInDef * suma2;

            sum += (suma3 + (2.*std::pow(suma1, 3.)/suma0 - 
                3.*suma1*suma2)/suma0)/suma0;
        }
       return sum;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::CumGen4thDerivativeCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle, 
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real sum = 0.;

        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;

            Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            Real denominator = 1.-pBuffer + midFactor;

            const Real& suma0 = denominator;
            const Real suma1  = lossInDef * midFactor;
            const Real suma2  = lossInDef * suma1;
            const Real suma3  = lossInDef * suma2;
            const Real suma4  = lossInDef * suma3;

            sum += (suma4 + (-4.*suma1*suma3 - 3.*suma2*suma2 + 
                (12.*suma1*suma1*suma2 - 
                    6.*std::pow(suma1,4.)/suma0)/suma0)/suma0)/suma0;
        }
       return sum;
    }

    template<class CP>
    ext::tuple<Real, Real, Real, Real> /// DISPOSABLE????
        SaddlePointLossModel<CP>::CumGen0234DerivCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle, 
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real deriv0 = 0.,
             //deriv1 = 0.,
             deriv2 = 0.,
             deriv3 = 0.,
             deriv4 = 0.;
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;

            Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            Real denominator = 1.-pBuffer + midFactor;

            const Real& suma0 = denominator;
            const Real suma1  = lossInDef * midFactor;
            const Real suma2  = lossInDef * suma1;
            const Real suma3  = lossInDef * suma2;
            const Real suma4  = lossInDef * suma3;

            // To do: optimize these:
            deriv0 += std::log(suma0);
            //deriv1 += suma1 / suma0;
            deriv2 += suma2 / suma0 - std::pow(suma1 / suma0 , 2.);
            deriv3 += (suma3 + (2.*std::pow(suma1, 3.)/suma0 - 
                3.*suma1*suma2)/suma0)/suma0;
            deriv4 += (suma4 + (-4.*suma1*suma3 - 3.*suma2*suma2 + 
                (12.*suma1*suma1*suma2 - 
                    6.*std::pow(suma1,4.)/suma0)/suma0)/suma0)/suma0;
        }
        return {deriv0, deriv2, deriv3, deriv4};
    }

    template<class CP>
    ext::tuple<Real, Real> /// DISPOSABLE???? 
        SaddlePointLossModel<CP>::CumGen02DerivCond(
        const std::vector<Real>& invUncondProbs,
        Real saddle, 
        const std::vector<Real>&  mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real deriv0 = 0.,
             //deriv1 = 0.,
             deriv2 = 0.;
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor)) / remainingNotional_;

            Real midFactor = pBuffer * std::exp(lossInDef * saddle);
            Real denominator = 1.-pBuffer + midFactor;

            const Real& suma0 = denominator;
            const Real suma1  = lossInDef * midFactor;
            const Real suma2  = lossInDef * suma1;

            // To do: optimize these:
            deriv0 += std::log(suma0);
            //deriv1 += suma1 / suma0;
            deriv2 += suma2 / suma0 - std::pow(suma1 / suma0 , 2.);
        }
        return {deriv0, deriv2};
    }

    // ----- Saddle point search ----------------------------------------------

    template<class CP>
    Real SaddlePointLossModel<CP>::findSaddle(
        const std::vector<Real>& invUncondPs,
        Real lossLevel, // in total portfolio loss fractional unit 
        const std::vector<Real>& mktFactor, 
        Real accuracy,
        Natural maxEvaluations
        ) const 
    {
        // \to do:
        // REQUIRE that loss level is below the max loss attainable in 
        //   the portfolio, otherwise theres no solution...
        SaddleObjectiveFunction f(*this, lossLevel, invUncondPs, mktFactor);

        Size nNames = remainingNotionals_.size();
        std::vector<Real> lgds;
        for(Size iName=0; iName<nNames; iName++)
            lgds.push_back(remainingNotionals_[iName] * 
            (1.-copula_->conditionalRecoveryInvP(invUncondPs[iName], iName,
                mktFactor)) );

        // computed limits:
        // position of the name with the largest relative exposure loss (i.e.:
        //   largest: N_i LGD_i / N_{total})
        Size iNamMax = std::distance(lgds.begin(), 
            std::max_element(lgds.begin(), lgds.end()) );
        // gap to be considered zero at the negative side of the logistic 
        //   inversion:
        static const Real deltaMin = 1.e-5;
        //
        Probability pMaxName = copula_->conditionalDefaultProbabilityInvP(
            invUncondPs[iNamMax], iNamMax, mktFactor);
        // aproximates the  saddle pt corresponding to this minimum; finds 
        //   it by using only the smallest logistic term and thus this is 
        //   smaller than the true value:
        Real saddleMin = 1./(lgds[iNamMax]/remainingNotional_) * 
            std::log(deltaMin*(1.-pMaxName)/
                (pMaxName*lgds[iNamMax]/remainingNotional_-pMaxName*deltaMin));
        // and the associated minimum loss is approximately: (this is thence 
        //   the minimum loss we can resolve/invert)
        Real minLoss = 
            CumGen1stDerivativeCond(invUncondPs, saddleMin, mktFactor);

        // If we are below the loss resolution it returns approximating 
        //  by the minimum/maximum attainable point. Typically the functionals
        //  to integrate will have a low dependency on this point.
        if(lossLevel < minLoss) return saddleMin;

        Real saddleMax = 1./(lgds[iNamMax]/remainingNotional_) * 
            std::log((lgds[iNamMax]/remainingNotional_
                -deltaMin)*(1.-pMaxName)/(pMaxName*deltaMin));
        Real maxLoss = 
            CumGen1stDerivativeCond(invUncondPs, saddleMax, mktFactor);
        if(lossLevel > maxLoss) return saddleMax;

        Brent solverBrent;
        Real guess = (saddleMin+saddleMax)/2.;
        /*
            (lossLevel - 
                CumGen1stDerivativeCond(invUncondPs, lossLevel, mktFactor))
                /CumGen2ndDerivativeCond(invUncondPs, lossLevel, mktFactor);
        if(guess > saddleMax) guess = (saddleMin+saddleMax)/2.;
        */
        solverBrent.setMaxEvaluations(maxEvaluations);
        return solverBrent.solve(f, accuracy, guess, saddleMin, saddleMax);
    }


    // ----- Statistics -------------------------------------------------------


    template<class CP>
    Real SaddlePointLossModel<CP>::percentile(const Date& d, 
        Probability percentile) const 
    {
        //this test should be in the calling basket...?
        QL_REQUIRE(percentile >=0. && percentile <=1., 
            "Incorrect percentile value.");

        // still this does not tackle the situation where we have cumulated 
        //   losses from previous defaults:
        if(d <= Settings::instance().evaluationDate()) return 0.;

        // Trivial cases when the percentile is outside the prob range 
        //   associated to the tranche limits:
        if(percentile <= 1.-probOverLoss(d, 0.)) return 0.;
        if(percentile >= 1.-probOverLoss(d, 1.)) 
            return basket_->remainingTrancheNotional();

        SaddlePercObjFunction f(*this, percentile, d);
        Brent solver;
        solver.setMaxEvaluations(100);
        Real minVal = QL_EPSILON;

        Real maxVal = 1.-QL_EPSILON; 
        Real guess = 0.5;

        Real solut = solver.solve(f, 1.e-4, guess, minVal, maxVal);
        return basket_->remainingTrancheNotional() * solut;
    }

    template<class CP>
    Probability SaddlePointLossModel<CP>::probOverLossCond(
        const std::vector<Real>& invUncondPs,
        Real trancheLossFract, 
        const std::vector<Real>& mktFactor) const {
        Real portfFract = attachRatio_ + trancheLossFract * 
            (detachRatio_-attachRatio_);// these are remaining ratios
        
        // for non-equity losses add the probability jump at zero tranche 
        //   losses (since this method returns prob of losing more or 
        //   equal to)
        ////////////////---       if(trancheLossFract <= QL_EPSILON) return 1.;
        return 
            probOverLossPortfCond(invUncondPs,
            //below; should substract realized loses. Use remaining amounts??
                portfFract * basket_->basketNotional(),
                mktFactor);
    }

    template<class CP>
    Disposable<std::map<Real, Probability> > 
        SaddlePointLossModel<CP>::lossDistribution(const Date& d) const {
        std::map<Real, Probability> distrib;
        static const Real numPts = 500.;
        for(Real lossFraction=1./numPts; lossFraction<0.45; 
            lossFraction+= 1./numPts)
            distrib.insert(std::make_pair<Real, Probability>(
                lossFraction * remainingNotional_ , 
                  1.-probOverPortfLoss(d, lossFraction* remainingNotional_ )));
        return distrib;
    }

    /*  NOTICE THIS IS ON THE TOTAL PORTFOLIO ---- UNTRANCHED..............
        Probability of having losses in the portfolio due to default 
        events equal or larger than a given absolute loss value on a 
        given date conditional to the latent model factor.
        The integral expression on the expansion is the first order 
        integration as presented in several references, see for instance; 
        equation 8 in R.Martin, K.Thompson, and C. Browne 's 
        'Taking to the Saddle', Risk Magazine, June 2001, page 91

        The loss is passed in absolute value.
    */
    template<class CP>
    Probability SaddlePointLossModel<CP>::probOverLossPortfCond(
        const std::vector<Real>& invUncondProbs,
        Real loss, 
        const std::vector<Real>& mktFactor) const 
    {
        /* This is taking in the unconditional probabilites non inverted. See if
        the callers can be written taking the inversion already; if they are 
        performing it thats a perf hit. At least this can be seen to be true
        for the recovery call (but rand rr are not intended to be used yet)
        */
       // return probOverLossPortfCond1stOrder(d, loss, mktFactor);
        if (loss <= QL_EPSILON) return 1.;

        Real relativeLoss = loss / remainingNotional_;
        if (relativeLoss >= 1.-QL_EPSILON) return 0.;

        const Size nNames = remainingNotionals_.size();

        Real averageRecovery_ = 0.;
        for(Size iName=0; iName < nNames; iName++)
            averageRecovery_ += copula_->conditionalRecoveryInvP(
                invUncondProbs[iName], iName, mktFactor);
        averageRecovery_ = averageRecovery_ / nNames;

        Real maxAttLossFract = 1.-averageRecovery_;
        if(relativeLoss > maxAttLossFract) return 0.;

        Real saddlePt = findSaddle(invUncondProbs,
            relativeLoss, mktFactor);

        ext::tuple<Real, Real, Real, Real> cumulants = 
            CumGen0234DerivCond(invUncondProbs, 
                saddlePt, mktFactor);
        Real baseVal = ext::get<0>(cumulants);
        Real secondVal = ext::get<1>(cumulants);
        Real K3Saddle = ext::get<2>(cumulants);
        Real K4Saddle = ext::get<3>(cumulants);

        Real saddleTo2 = saddlePt * saddlePt;
        Real saddleTo3 = saddleTo2 * saddlePt;
        Real saddleTo4 = saddleTo3 * saddlePt;
        Real saddleTo6 = saddleTo4 * saddleTo2;
        Real K3SaddleTo2 = K3Saddle*K3Saddle;

        if(saddlePt > 0.) { // <-> (loss > condEL)
            Real exponent = baseVal - relativeLoss * saddlePt + 
                .5 * saddleTo2 * secondVal;
            if( std::abs(exponent) > 700.) return 0.;
            return 
                std::exp(exponent)
                * CumulativeNormalDistribution()(-std::abs(saddlePt)*
                    std::sqrt(/*saddleTo2 **/secondVal))

                // high order corrections:
                * (1. - saddleTo3*K3Saddle/6. + saddleTo4*K4Saddle/24. + 
                    saddleTo6*K3SaddleTo2/72.) 
                /*
                // FIX ME: this term introduces at times numerical 
                //   instabilty (shows up in percentile computation)
                + (3.*secondVal*(1.-secondVal*saddleTo2)*
                        (saddlePt*K4Saddle-4.*K3Saddle)
                    - saddlePt*K3SaddleTo2*(3.-saddleTo2*secondVal + 
                            saddleTo4*secondVal*secondVal)) 
                     / (72.*M_SQRTPI*M_SQRT_2*std::pow(secondVal, 5./2.) ) 
                 */
                 ;
        }else if(saddlePt==0.){// <-> (loss == condEL)
            return .5;
        }else {// <->(loss < condEL)
            Real exponent = baseVal - relativeLoss * saddlePt + 
                .5 * saddleTo2 * secondVal;
            if( std::abs(exponent) > 700.) return 0.;
            return 
                1.-
                std::exp(exponent)
                * CumulativeNormalDistribution()(-std::abs(saddlePt)
                    * std::sqrt(/*saddleTo2 **/secondVal))// static call?

                // high order corrections:
                * (1. - saddleTo3*K3Saddle/6. + saddleTo4*K4Saddle/24. + 
                    saddleTo6*K3SaddleTo2/72.) 
                /*
                  + (3.*secondVal*(1.-secondVal*saddleTo2)*
                    (saddlePt*K4Saddle-4.*K3Saddle)
                  - saddlePt*K3SaddleTo2*(3.-saddleTo2*secondVal +
                        saddleTo4*secondVal*secondVal)) 
                    / (72.*M_SQRTPI*M_SQRT_2*std::pow(secondVal, 5./2.) ) 
                */
                ;
        }
    }

    template<class CP>
    // cheaper; less terms retained; yet the cost lies in the saddle point calc
    Probability SaddlePointLossModel<CP>::probOverLossPortfCond1stOrder(
        const std::vector<Real>& invUncondPs,
        Real loss, 
        const std::vector<Real>& mktFactor) const 
    {
        if (loss <= QL_EPSILON) return 1.;
        const Size nNames = remainingNotionals_.size();

        Real relativeLoss = loss / remainingNotional_;
        if(relativeLoss >= 1.-QL_EPSILON) return 0.;

        // only true for constant recovery models......?
        Real averageRecovery_ = 0.;
        for(Size iName=0; iName < nNames; iName++)
            averageRecovery_ += 
            copula_->conditionalRecoveryInvP(invUncondPs[iName], iName, 
            mktFactor);  
        averageRecovery_ = averageRecovery_ / nNames;

        Real maxAttLossFract = 1.-averageRecovery_;
        if(relativeLoss > maxAttLossFract) return 0.;

        Real saddlePt = findSaddle(invUncondPs,
            relativeLoss, mktFactor);

        ext::tuple<Real, Real> cumulants = 
            CumGen02DerivCond(invUncondPs,
                saddlePt, mktFactor);
        Real baseVal = ext::get<0>(cumulants);
        Real secondVal = ext::get<1>(cumulants);

        Real saddleTo2 = saddlePt * saddlePt;

        if(saddlePt > 0.) { // <-> (loss > condEL)
            Real exponent = baseVal - relativeLoss * saddlePt + 
                .5 * saddleTo2 * secondVal;
            if( std::abs(exponent) > 700.) return 0.;
            return 
                // dangerous exponential; fix me
                std::exp(exponent)
                /*  std::exp(baseVal - relativeLoss * saddlePt 
                    + .5 * saddleTo2 * secondVal)*/
                * CumulativeNormalDistribution()(-std::abs(saddlePt)*
                    std::sqrt(/*saddleTo2 **/secondVal));
        }else if(saddlePt==0.){// <-> (loss == condEL)
            return .5;
        }else {// <->(loss < condEL)
            Real exponent = baseVal - relativeLoss * saddlePt + 
                .5 * saddleTo2 * secondVal;
            if( std::abs(exponent) > 700.) return 0.;

            return 
                1.-
               /* std::exp(baseVal - relativeLoss * saddlePt 
               + .5 * saddleTo2 * secondVal)*/
                std::exp(exponent)
                * CumulativeNormalDistribution()(-std::abs(saddlePt)*
                    std::sqrt(/*saddleTo2 **/secondVal));
        }
    }


    /*!   NOTICE THIS IS ON THE TOTAL PORTFOLIO ---- UNTRANCHED
    Probability density of having losses in the portfolio due to default 
        events equal to a given value on a given date conditional to the w
        latent model factor.
        Based on the integrals of the expected shortfall. See......refernce.
    */
    template<class CP>
    Probability SaddlePointLossModel<CP>::probDensityCond(
        const std::vector<Real>& invUncondPs,
        Real loss,
        const std::vector<Real>& mktFactor) const 
    {
        if (loss <= QL_EPSILON) return 0.;

        Real relativeLoss = loss / remainingNotional_;
        Real saddlePt = findSaddle(invUncondPs,
            relativeLoss, mktFactor);

        ext::tuple<Real, Real, Real, Real> cumulants = 
            CumGen0234DerivCond(invUncondPs,
            saddlePt, mktFactor);
        /// access them directly rather than through this copy
        Real K0Saddle = ext::get<0>(cumulants);
        Real K2Saddle = ext::get<1>(cumulants);
        Real K3Saddle = ext::get<2>(cumulants);
        Real K4Saddle = ext::get<3>(cumulants);
        /* see, for instance R.Martin "he saddle point method and portfolio 
        optionalities." in Risk December 2006 p.93 */
        //\todo the exponentials below are dangerous and agressive, tame them.
        return 
            (
            1.
            + K4Saddle
                /(8.*std::pow(K2Saddle, 2.))
            - 5.*std::pow(K3Saddle,2.)
                /(24.*std::pow(K2Saddle, 3.))
            ) * std::exp(K0Saddle - saddlePt * relativeLoss)
             / (std::sqrt(2. * M_PI * K2Saddle));
    }

    /*    NOTICE THIS IS ON THE TOTAL PORTFOLIO ---- UNTRANCHED..
        Sensitivities of the individual names to a given portfolio loss value 
        due to defaults. It returns ratios to the total structure notional, 
        which aggregated add up to the requested loss value.

    see equation 8 in 'VAR: who contributes and how much?' by R.Martin, 
    K.Thompson, and C. Browne in Risk Magazine, August 2001

    The passed loss is the loss amount level at which we want to
    request the sensitivity.  Equivalent to a percentile.
    */
    template<class CP>
    Disposable<std::vector<Real> > SaddlePointLossModel<CP>::splitLossCond(
        const std::vector<Real>& invUncondProbs,
        Real loss, 
        std::vector<Real> mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        std::vector<Real> condContrib(nNames, 0.);
        if (loss <= QL_EPSILON) return condContrib;

        Real saddlePt = findSaddle(invUncondProbs, loss / remainingNotional_, 
            mktFactor);

        for(Size iName=0; iName<nNames; iName++) {
            Probability pBuffer = 
                copula_->conditionalDefaultProbabilityInvP(
                    invUncondProbs[iName], iName, mktFactor);
            Real lossInDef = remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor));
            Real midFactor = pBuffer * 
                std::exp(lossInDef * saddlePt/ remainingNotional_);
            Real denominator = 1.-pBuffer + midFactor;

            condContrib[iName] = lossInDef * midFactor / denominator; 
        }
        return condContrib;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::conditionalExpectedLoss(
        const std::vector<Real>& invUncondProbs,
        const std::vector<Real>& mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real eloss = 0.;
        /// USE STL.....-------------------
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = copula_->conditionalDefaultProbabilityInvP(
                invUncondProbs[iName], iName, mktFactor);
            eloss += pBuffer * remainingNotionals_[iName] *
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                    iName, mktFactor));
        }
        return eloss;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::conditionalExpectedTrancheLoss(
        const std::vector<Real>& invUncondProbs,
        const std::vector<Real>& mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        Real eloss = 0.;
        /// USE STL.....-------------------
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = copula_->conditionalDefaultProbabilityInvP(
                invUncondProbs[iName], iName, mktFactor);
            eloss += 
                pBuffer * remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName], 
                iName, mktFactor));
        }
        return std::min(
            std::max(eloss - attachRatio_ * remainingNotional_, 0.), 
                (detachRatio_ - attachRatio_) * remainingNotional_);
    }

    template<class CP>
    Disposable<std::vector<Real> > 
        SaddlePointLossModel<CP>::expectedShortfallSplitCond(
            const std::vector<Real>& invUncondProbs,
            Real lossPerc, const std::vector<Real>& mktFactor) const 
    {
        const Size nNames = remainingNotionals_.size();
        std::vector<Real> lgds;
        for(Size iName=0; iName<nNames; iName++)
            lgds.push_back(remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName],
                    iName, mktFactor))); 
        std::vector<Real> vola(nNames, 0.), mu(nNames, 0.);
        Real volaTot = 0., muTot = 0.;
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = copula_->conditionalDefaultProbabilityInvP(
                invUncondProbs[iName], iName, mktFactor);
            mu[iName] = lgds[iName] * pBuffer / remainingNotionals_[iName];
            muTot += lgds[iName] * pBuffer;
            vola[iName] = lgds[iName] * lgds[iName] * pBuffer * (1.-pBuffer) 
                / remainingNotionals_[iName];
            volaTot += lgds[iName] * lgds[iName] * pBuffer * (1.-pBuffer) ;
        }
        for (Size iName=0; iName < nNames; iName++)
            vola[iName] = vola[iName] / volaTot;

        std::vector<Real> esfPartition(nNames, 0.);
        for(Size iName=0; iName < nNames; iName++) {
            Real uEdisp = (lossPerc-muTot)/std::sqrt(volaTot);
            esfPartition[iName] = mu[iName]
                * CumulativeNormalDistribution()(uEdisp) // static call?
                + vola[iName] * NormalDistribution()(uEdisp);
        }
        return esfPartition;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::expectedShortfallTrancheCond(
        const std::vector<Real>& invUncondProbs,
        Real lossPerc, // value 
        Probability percentile,
        const std::vector<Real>& mktFactor) const 
    {
        /* TO DO: this is too crude, a general expression valid for all 
        situations is possible (with no extra cost as long as the loss limits 
        are checked).
        */
        //tranche correction term:
        Real correctionTerm = 0.;
        Real probLOver = probOverLossPortfCond(invUncondProbs,
            basket_->detachmentAmount(), mktFactor);
        if(basket_->attachmentAmount() > QL_EPSILON) {
            if(lossPerc < basket_->attachmentAmount()) {
                correctionTerm = ( (basket_->detachmentAmount() 
                    - 2.*basket_->attachmentAmount())*
                        probOverLossPortfCond(invUncondProbs, lossPerc, 
                            mktFactor)
                    + basket_->attachmentAmount() * probLOver )/(1.-percentile);
            }else{
                correctionTerm = ( (percentile-1)*basket_->attachmentAmount()
                    + basket_->detachmentAmount() * probLOver
                    )/(1.-percentile);
            }
        }

        return expectedShortfallFullPortfolioCond(invUncondProbs, 
            std::max(lossPerc, basket_->attachmentAmount()), mktFactor)
            + expectedShortfallFullPortfolioCond(invUncondProbs, 
                basket_->detachmentAmount(), mktFactor)
            - correctionTerm;
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::expectedShortfallFullPortfolioCond(
        const std::vector<Real>& invUncondProbs,
        Real lossPerc, // value 
        const std::vector<Real>& mktFactor) const 
    {
        /* This version is based on: Martin 2006 paper and on the expression 
        in 'SaddlePoint approximation of expected shortfall for transformed 
        means' S.A. Broda and M.S.Paolella , Amsterdam School of Economics 
        discussion paper, available online.
        */
        Real lossPercRatio = lossPerc  /remainingNotional_;
        Real elCond = 0.;
        const Size nNames = remainingNotionals_.size();

        /// use stl algorthms
        for(Size iName=0; iName < nNames; iName++) {
            Probability pBuffer = copula_->conditionalDefaultProbabilityInvP(
                invUncondProbs[iName], iName, mktFactor);
            elCond += pBuffer * remainingNotionals_[iName] * 
                (1.-copula_->conditionalRecoveryInvP(invUncondProbs[iName],
                    iName, mktFactor));
        }
        Real saddlePt = findSaddle(invUncondProbs, lossPercRatio, mktFactor);

        // Martin 2006:
        return 
            elCond * probOverLossPortfCond(invUncondProbs, lossPerc, mktFactor)
              + (lossPerc - elCond) * probDensityCond(invUncondProbs, lossPerc,
                    mktFactor) /saddlePt;

        // calling the EL tranche
        // return elCond - expectedEquityLossCond(d, lossPercRatio, mktFactor);

        /*
        // Broda and Paolella:
        Real elCondRatio = elCond / remainingNotional_;

        ext::tuple<Real, Real, Real, Real> cumulants = 
            CumGen0234DerivCond(uncondProbs, 
                saddlePt, mktFactor);
        Real K0Saddle = ext::get<0>(cumulants);///USE THEM DIRECTLY
        Real K2Saddle = ext::get<1>(cumulants);

        Real wq = std::sqrt(2. * saddlePt * lossPercRatio - 2. * K0Saddle);
        //std::sqrt(-2. * saddlePt * lossPerc + 2. * K0Saddle);????
        Real factor = 1.;
        if(saddlePt<0.) {
            wq = -wq;
            factor = -1.;
        }

        Real numNames = static_cast<Real>(nNames);

        Real term1 = CumulativeNormalDistribution()(wq)// * std::sqrt(numNames)
            * elCond ;
        Real term2 = .5 * M_2_SQRTPI * M_SQRT1_2 * (1./std::sqrt(numNames))
            * exp(-wq*wq * numNames/2.)*(elCond/wq - 
                lossPerc/(saddlePt * std::sqrt(K2Saddle)));
        return term1 + term2;
        */
    }

    template<class CP>
    Real SaddlePointLossModel<CP>::expectedShortfall(const Date&d, 
        Probability percProb) const 
    {
        // assuming I have the tranched one.
        Real lossPerc = percentile(d, percProb);

        // check the trivial case when the loss is over the detachment limit 
        //   to avoid computation:
        Real trancheAmount = basket_->trancheNotional() * 
            (detachRatio_-attachRatio_);
        //assumed the amount includes the realized loses
        if(lossPerc >= trancheAmount) return trancheAmount;
        //SHOULD CHECK NOW THE OPPOSITE LIMIT ("zero" losses)....
        std::vector<Real> invUncondProbs = 
            basket_->remainingProbabilities(d);
        for(Size i=0; i<invUncondProbs.size(); i++)
            invUncondProbs[i] = 
                copula_->inverseCumulativeY(invUncondProbs[i], i);

        // Integrate with the tranche or the portfolio according to the limits.
        return copula_->integratedExpectedValue(
            [&](const std::vector<Real>& v1) {
                return expectedShortfallFullPortfolioCond(invUncondProbs, lossPerc, v1);
            }) / (1.-percProb);

    /* test:?
        return std::inner_product(integrESFPartition.begin(), 
        integrESFPartition.end(), remainingNotionals_.begin(), 0.);
    */        

    }



}

#endif
]]></document_content>
  </document>
  <document index="44">
    <source>spotlosslatentmodel.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2014 Jose Aparicio

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

#ifndef quantlib_spotlosslatentmodel_hpp
#define quantlib_spotlosslatentmodel_hpp

#include <ql/experimental/credit/defaultprobabilitylatentmodel.hpp>

namespace QuantLib {

    /*! \brief Random spot recovery rate latent variable portfolio model.\par
    See: \par
    <b>A Spot Stochastic Recovery Extension of the Gaussian Copula</b> N.Bennani
         and J.Maetz, MPRA July 2009 \par
    <b>Extension of Spot Recovery model for Gaussian Copula</b> H.Li, October
        2009,  MPRA \par
    The model is adpated here for a multifactor set up and a generic copula so 
    it can be used for pricing in single factor mode or for risk metrics in its
    multifactor version.\par
    \todo Rewrite this model: the distribution of the spot recovery given
    default could be given as a functional of rr_i with the market factors and
    the rest of methods depend on this. That would offer a family of models.
    \todo Implement eq. 45 to have the EL(t) and be able to integrate the model
    */
    template <class copulaPolicy>
    class SpotRecoveryLatentModel : public LatentModel<copulaPolicy> {
    public:
        // resolve LM interface:
        using LatentModel<copulaPolicy>::factorWeights;
        using LatentModel<copulaPolicy>::inverseCumulativeY;
        using LatentModel<copulaPolicy>::cumulativeY;
        using LatentModel<copulaPolicy>::latentVarValue;
        using LatentModel<copulaPolicy>::integratedExpectedValue;
    private:
        const std::vector<Real> recoveries_;
        const Real modelA_;
        // products of default and recoveries factors, see refs ('covariances')
        std::vector<Real> crossIdiosyncFctrs_;
        mutable Size numNames_;
        mutable ext::shared_ptr<Basket> basket_;
        ext::shared_ptr<LMIntegration> integration_;
    protected:
        //! access to integration:
      const ext::shared_ptr<LMIntegration>& integration() const override { return integration_; }

    private:
        typedef typename copulaPolicy::initTraits initTraits;
    public:
        SpotRecoveryLatentModel(
            const std::vector<std::vector<Real> >& factorWeights,
            const std::vector<Real>& recoveries,
            Real modelA,
            LatentModelIntegrationType::LatentModelIntegrationType integralType,
            const initTraits& ini = initTraits()
            );

        void resetBasket(const ext::shared_ptr<Basket>& basket) const;
        Probability conditionalDefaultProbability(const Date& date, Size iName,
            const std::vector<Real>& mktFactors) const;
        Probability conditionalDefaultProbability(Probability prob, Size iName,
            const std::vector<Real>& mktFactors) const;
        Probability conditionalDefaultProbabilityInvP(Real invCumYProb, 
            Size iName, 
            const std::vector<Real>& m) const;
        /*! Expected conditional spot recovery rate. Conditional on a set of 
        systemic factors and default returns the integrated attainable recovery 
        values. \par
        Corresponds to a multifactor generalization of the model in eq. 44 
        on p.15 of <b>Extension of Spot Recovery Model for Gaussian Copula</b> 
        Hui Li. 2009  Only remember that \f$\rho_l Z \f$ there is here 
        (multiple betas): 
        \f$ \sum_k \beta_{ik}^l Z_k \f$ and that \f$ \rho_d \rho_l \f$ there is
        here: 
        \f$ \sum_k \beta_{ik}^d \beta_{ik}^l \f$ \par
        (d,l corresponds to first and last set of betas) 
        */
        Real expCondRecovery/*conditionalRecovery*/(const Date& d, Size iName, 
                                 const std::vector<Real>& mktFactors) const;
        Real expCondRecoveryP(Real uncondDefP, Size iName, 
                                 const std::vector<Real>& mktFactors) const;
        Real expCondRecoveryInvPinvRR(Real invUncondDefP, Real invUncondRR,
            Size iName, const std::vector<Real>& mktFactors) const;
        /*! Implements equation 42 on p.14 (second).
            Remember that for this call to make sense the sample used must be 
            one leading to a default. Theres no check on this. This member
            typically to be used within a simulation.
        */
        Real conditionalRecovery(Real latentVarSample, Size iName, 
            const Date& d) const;
        /*! Due to the way the latent model is splitted in two parts, we call 
        the base class for the default sample and the LM owned here for the RR 
        model sample. This sample only makes sense if it led to a default.
        @param allFactors All sampled factors, default and RR valiables.
        @param iName The index of the name for which we want the RR sample

        \todo Write vector version for all names' RRs 
        */
        Real latentRRVarValue(const std::vector<Real>& allFactors, 
            Size iName) const;
        Real conditionalExpLossRR(const Date& d, Size iName, 
            const std::vector<Real>& mktFactors) const;
        Real conditionalExpLossRRInv(Real invP, Real invRR, Size iName, 
            const std::vector<Real>& mktFactors) const;
        /*! Single name expected loss.\par 
        The main reason of this method is for the testing of this model. The 
        model is coherent in that it preserves the single name expected loss
        and thus is coherent with the single name CDS market when used in the
        pricing context. i.e. it should match: \f$pdef_i(d) \times RR_i \f$
        */
        Real expectedLoss(const Date& d, Size iName) const;
    };


    typedef SpotRecoveryLatentModel<GaussianCopulaPolicy> GaussianSpotLossLM;
    typedef SpotRecoveryLatentModel<TCopulaPolicy> TSpotLossLM;


    // ------------------------------------------------------------------------

    template <class CP>
    inline void
    SpotRecoveryLatentModel<CP>::resetBasket(const ext::shared_ptr<Basket>& basket) const {
        basket_ = basket;
        // in the future change 'size' to 'liveSize'
        QL_REQUIRE(basket_->size() == numNames_, 
            "Incompatible new basket and model sizes.");
    }

    template<class CP>
    inline Probability 
        SpotRecoveryLatentModel<CP>::conditionalDefaultProbability(
        const Date& date, 
        Size iName, const std::vector<Real>& mktFactors) const 
    {
        const ext::shared_ptr<Pool>& pool = basket_->pool();
        Probability pDefUncond =
            pool->get(pool->names()[iName]).
            defaultProbability(basket_->defaultKeys()[iName])
              ->defaultProbability(date);
        return conditionalDefaultProbability(pDefUncond, iName, mktFactors);
    }

    template<class CP>
    inline Probability 
        SpotRecoveryLatentModel<CP>::conditionalDefaultProbability(
        Probability prob, 
        Size iName, const std::vector<Real>& mktFactors) const 
    {
        // we can be called from the outside (from an integrable loss model)
        //   but we are called often at integration points. This or
        //   consider a list of friends.
    #if defined(QL_EXTRA_SAFETY_CHECKS)
        QL_REQUIRE(basket_, "No portfolio basket set.");
    #endif
        /*Avoid redundant call to minimum value inversion (might be \infty),
        and this independently of the copula function.
        */
        if (prob < 1.e-10) return 0.;// use library macro...
        return conditionalDefaultProbabilityInvP(
            inverseCumulativeY(prob, iName), iName, mktFactors);
    }

    template<class CP>
    inline Probability 
        SpotRecoveryLatentModel<CP>::conditionalDefaultProbabilityInvP(
        Real invCumYProb, 
        Size iName, 
        const std::vector<Real>& m) const 
    {
        Real sumMs = 
            std::inner_product(this->factorWeights_[iName].begin(), 
                               this->factorWeights_[iName].end(), m.begin(), 0.);
        Real res = this->cumulativeZ((invCumYProb - sumMs) / 
                this->idiosyncFctrs_[iName] );
        #if defined(QL_EXTRA_SAFETY_CHECKS)
        QL_REQUIRE (res >= 0. && res <= 1.,
                    "conditional probability " << res << "out of range");
        #endif
    
        return res;
    }

    template<class CP>
    inline Real 
        SpotRecoveryLatentModel<CP>::expCondRecovery(const Date& d, 
        Size iName,
        const std::vector<Real>& mktFactors) const 
    {
    #if defined(QL_EXTRA_SAFETY_CHECKS)
        QL_REQUIRE(mktFactors.size() == this->numFactors(), 
        "Realization of market factors and latent model size do not match");
    #endif
        const ext::shared_ptr<Pool>& pool = basket_->pool();
        Probability pDefUncond =
            pool->get(pool->names()[iName]).
            defaultProbability(basket_->defaultKeys()[iName])
              ->defaultProbability(d);

        return expCondRecoveryP(pDefUncond, iName, mktFactors);
    }

    template<class CP>
    inline Real SpotRecoveryLatentModel<CP>::expCondRecoveryP(
        Real uncondDefP, Size iName, const std::vector<Real>& mktFactors) const 
    {
        return expCondRecoveryInvPinvRR(
            inverseCumulativeY(uncondDefP, iName), 
            inverseCumulativeY(recoveries_[iName], iName + numNames_),
            iName, mktFactors);
    }

    template<class CP>
    Real SpotRecoveryLatentModel<CP>::expCondRecoveryInvPinvRR(
        Real invUncondDefP, 
        Real invUncondRR, 
        Size iName, 
        const std::vector<Real>& mktFactors) const 
    {
        const std::vector<std::vector<Real> >& fctrs_ = factorWeights();
        //Size iRR = iName + basket_->size();// should be live pool
        const Real sumMs =
          std::inner_product(fctrs_[iName].begin(), fctrs_[iName].end(), 
              mktFactors.begin(), 0.);
        const Real sumBetaLoss = 
          std::inner_product(fctrs_[iName + numNames_].begin(),
              fctrs_[iName + numNames_].end(),
              fctrs_[iName + numNames_].begin(), 
              0.);
        return this->cumulativeZ((sumMs + std::sqrt(1.-crossIdiosyncFctrs_[iName])
                 * std::sqrt(1.+modelA_*modelA_) * 
                   invUncondRR
            - std::sqrt(crossIdiosyncFctrs_[iName]) * 
                invUncondDefP
                )
            / std::sqrt(1.- sumBetaLoss + modelA_*modelA_ * 
                (1.-crossIdiosyncFctrs_[iName])) );
    }

    template<class CP>
    Real SpotRecoveryLatentModel<CP>::conditionalRecovery(Real latentVarSample,
        Size iName, const Date& d) const 
    {
        const ext::shared_ptr<Pool>& pool = basket_->pool();

        // retrieve the default probability for this name
        const Handle<DefaultProbabilityTermStructure>& dfts = 
            pool->get(basket_->names()[iName]).defaultProbability(
                basket_->defaultKeys()[iName]);
        const Probability pdef = dfts->defaultProbability(d, true);
        // before asking for -\infty
        if (pdef < 1.e-10) return 0.;

        Size iRecovery = iName + numNames_;// should be live pool
        return cumulativeY(
            (latentVarSample - std::sqrt(crossIdiosyncFctrs_[iName]) 
                * inverseCumulativeY(pdef, iName)) / 
                (modelA_ * std::sqrt(1.-crossIdiosyncFctrs_[iName]))
            // cache the sqrts
            // cache this factor.
            +std::sqrt(1.+ 1./(modelA_*modelA_)) * 
                inverseCumulativeY(recoveries_[iName], iRecovery) 
            , iRecovery);
    }

    template<class CP>
    inline Real SpotRecoveryLatentModel<CP>::latentRRVarValue(
        const std::vector<Real>& allFactors, 
        Size iName) const 
    {
        Size iRecovery = iName + numNames_;// should be live pool
        return latentVarValue(allFactors, iRecovery);
    }

    template<class CP>
    inline Real SpotRecoveryLatentModel<CP>::conditionalExpLossRR(const Date& d,
        Size iName, 
        const std::vector<Real>& mktFactors) const 
    {
        const ext::shared_ptr<Pool>& pool = basket_->pool();
        Probability pDefUncond =
            pool->get(pool->names()[iName]).
            defaultProbability(basket_->defaultKeys()[iName])
              ->defaultProbability(d);

        Real invP = inverseCumulativeY(pDefUncond, iName);
        Real invRR = inverseCumulativeY(recoveries_[iName], iName + numNames_);

        return conditionalExpLossRRInv(invP, invRR, iName, mktFactors);
    }

    template<class CP>
    inline Real SpotRecoveryLatentModel<CP>::conditionalExpLossRRInv(
        Real invP, 
        Real invRR,
        Size iName, 
        const std::vector<Real>& mktFactors) const 
    {
        return conditionalDefaultProbabilityInvP(invP, iName, mktFactors)
            * (1.-this->conditionalRecoveryInvPinvRR(invP, invRR, iName, mktFactors));
    }

    template<class CP>
    inline Real SpotRecoveryLatentModel<CP>::expectedLoss(const Date& d, 
        Size iName) const 
    {
        const ext::shared_ptr<Pool>& pool = basket_->pool();
        Probability pDefUncond =
            pool->get(pool->names()[iName]).
            defaultProbability(basket_->defaultKeys()[iName])
              ->defaultProbability(d);

        Real invP = inverseCumulativeY(pDefUncond, iName);
        Real invRR = inverseCumulativeY(recoveries_[iName], iName + numNames_);

        return integratedExpectedValue(
            [&](const std::vector<Real>& v){
                return conditionalExpLossRRInv(invP, invRR, iName, v);
            });
    }

    template<class CP>
    SpotRecoveryLatentModel<CP>::SpotRecoveryLatentModel(
        const std::vector<std::vector<Real> >& factorWeights,
        const std::vector<Real>& recoveries,
        Real modelA,
        LatentModelIntegrationType::LatentModelIntegrationType integralType,
        const typename CP::initTraits& ini
        ) 
    : LatentModel<CP>(factorWeights, ini),
      recoveries_(recoveries), 
      modelA_(modelA),
      numNames_(factorWeights.size()/2),
      integration_(LatentModel<CP>::IntegrationFactory::
        createLMIntegration(factorWeights[0].size(), integralType))
    {
        QL_REQUIRE(factorWeights.size() % 2 == 0, 
         "Number of RR variables must be equal to number of default variables");
        QL_REQUIRE(recoveries.size() == numNames_ , 
         "Number of recoveries does not match number of defaultable entities.");

        // reminder: first betas are default, last ones are recovery 
        for(Size iName=0; iName<numNames_; iName++) /// USE STL
            /* Corresponds to: (k denotes factor, i denotes modelled 
                variable -default and recoveries))
                \sum_k a^2_{i,k} a^2_{N+i,k}
            */
        {
            Real cumul = 0.;
            for(Size iB=0; iB<factorWeights[iName].size(); iB++)
                // actually this size is unique
                cumul += factorWeights[iName][iB] * 
                    factorWeights[iName][iB] * 
                    factorWeights[iName + numNames_][iB] * 
                    factorWeights[iName + numNames_][iB];
            crossIdiosyncFctrs_.push_back(cumul);
        }

    }


}

#endif
]]></document_content>
  </document>
  <document index="45">
    <source>spreadedhazardratecurve.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2009 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file spreadedhazardratecurve.hpp
    \brief Default-probability structure with an additive spread on hazard rates
*/

#ifndef quantlib_spreaded_hazard_rate_curve_hpp
#define quantlib_spreaded_hazard_rate_curve_hpp

#include <ql/quote.hpp>
#include <ql/termstructures/credit/hazardratestructure.hpp>
#include <utility>

namespace QuantLib {

    //! Default-probability structure with an additive spread on hazard rates
    /*! \note This term structure will remain linked to the original
              structure, i.e., any changes in the latter will be
              reflected in this structure as well.

        \ingroup termstructures
    */
    class SpreadedHazardRateCurve : public HazardRateStructure {
      public:
        SpreadedHazardRateCurve(Handle<DefaultProbabilityTermStructure> originalCurve,
                                Handle<Quote> spread);
        //! \name DefaultProbabilityTermStructure interface
        //@{
        DayCounter dayCounter() const override;
        Calendar calendar() const override;
        const Date& referenceDate() const override;
        Date maxDate() const override;
        Time maxTime() const override;
        //@}
      protected:
        //! \name DefaultProbabilityTermStructure interface
        //@{
        Real hazardRateImpl(Time t) const override;
        //@}
      private:
        Handle<DefaultProbabilityTermStructure> originalCurve_;
        Handle<Quote> spread_;
    };


    // inline definitions

    inline SpreadedHazardRateCurve::SpreadedHazardRateCurve(
        Handle<DefaultProbabilityTermStructure> h, Handle<Quote> spread)
    : originalCurve_(std::move(h)), spread_(std::move(spread)) {
        registerWith(originalCurve_);
        registerWith(spread_);
    }

    inline DayCounter SpreadedHazardRateCurve::dayCounter() const {
        return originalCurve_->dayCounter();
    }

    inline Calendar SpreadedHazardRateCurve::calendar() const {
        return originalCurve_->calendar();
    }

    inline const Date& SpreadedHazardRateCurve::referenceDate() const {
        return originalCurve_->referenceDate();
    }

    inline Date SpreadedHazardRateCurve::maxDate() const {
        return originalCurve_->maxDate();
    }

    inline Time SpreadedHazardRateCurve::maxTime() const {
        return originalCurve_->maxTime();
    }

    inline Real SpreadedHazardRateCurve::hazardRateImpl(Time t) const {
        return originalCurve_->hazardRate(t, true) + spread_->value();
    }

}

#endif
]]></document_content>
  </document>
  <document index="46">
    <source>syntheticcdo.hpp</source>
    <document_content><![CDATA[/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*
 Copyright (C) 2008 Roland Lichters

 This file is part of QuantLib, a free-software/open-source library
 for financial quantitative analysts and developers - http://quantlib.org/

 QuantLib is free software: you can redistribute it and/or modify it
 under the terms of the QuantLib license.  You should have received a
 copy of the license along with this program; if not, please email
 <quantlib-dev@lists.sf.net>. The license is also available online at
 <http://quantlib.org/license.shtml>.

 This program is distributed in the hope that it will be useful, but WITHOUT
 ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 FOR A PARTICULAR PURPOSE.  See the license for more details.
*/

/*! \file syntheticcdo.hpp
    \brief Synthetic Collateralized Debt Obligation and pricing engines
*/

#ifndef quantlib_synthetic_cdo_hpp
#define quantlib_synthetic_cdo_hpp

#include <ql/qldefines.hpp>

#ifndef QL_PATCH_SOLARIS

#include <ql/instrument.hpp>
#include <ql/default.hpp>
#include <ql/time/schedule.hpp>

#include <ql/experimental/credit/basket.hpp>
#include <ql/cashflows/fixedratecoupon.hpp>

namespace QuantLib {

    class YieldTermStructure;

    //! Synthetic Collateralized Debt Obligation
    /*!
      The instrument prices a mezzanine CDO tranche with loss given default 
      between attachment point \f$ D_1\f$ and detachment point 
      \f$ D_2 > D_1 \f$.

      For purchased protection, the instrument value is given by the difference
      of the protection value \f$ V_1 \f$ and premium value \f$ V_2 \f$,

      \f[ V = V_1 - V_2. \f]

      The protection leg is priced as follows:

      - Build the probability distribution for volume of defaults \f$ L \f$ 
      (before recovery) or Loss Given Default \f$ LGD = (1-r)\,L \f$ at 
      times/dates \f$ t_i, i=1, ..., N\f$ (premium schedule times with
      intermediate steps)

      - Determine the expected value 
      \f$ E_i = E_{t_i}\,\left[Pay(LGD)\right] \f$
      of the protection payoff \f$ Pay(LGD) \f$  at each time \f$ t_i\f$ where
      \f[
      Pay(L) = min (D_1, LGD) - min (D_2, LGD) = \left\{
      \begin{array}{lcl}
      \displaystyle 0 &;& LGD < D_1 \\
      \displaystyle LGD - D_1 &;& D_1 \leq LGD \leq D_2 \\
      \displaystyle D_2 - D_1 &;& LGD > D_2
      \end{array}
      \right.
      \f]

      - The protection value is then calculated as
      \f[ V_1 \:=\: \sum_{i=1}^N (E_i - E_{i-1}) \cdot  d_i \f]
      where \f$ d_i\f$ is the discount factor at time/date \f$ t_i \f$

      The premium is paid on the protected notional amount, initially
      \f$ D_2 - D_1. \f$ This notional amount is reduced by the expected 
      protection
      payments \f$ E_i \f$ at times \f$ t_i, \f$ so that the premium value is
      calculated as

      \f[
      V_2 =m \, \cdot \sum_{i=1}^N \,(D_2 - D_1 - E_i) \cdot \Delta_{i-1,i}\,d_i
      \f]

      where \f$ m \f$ is the premium rate, \f$ \Delta_{i-1, i}\f$ is the day
      count fraction between date/time \f$ t_{i-1}\f$ and \f$ t_i.\f$

      The construction of the portfolio loss distribution \f$ E_i \f$ is
      based on the probability bucketing algorithm described in

      <strong>
      John Hull and Alan White, "Valuation of a CDO and nth to default CDS
      without Monte Carlo simulation", Journal of Derivatives 12, 2, 2004
      </strong>

      The pricing algorithm allows for varying notional amounts and
      default termstructures of the underlyings.

      \ingroup credit

      \todo Investigate and fix cases \f$ E_{i+1} < E_i. \f$
    */
    class SyntheticCDO : public Instrument {
    public:
        class arguments;
        class results;
        class engine;

        // Review: No accrual settlement flag. No separate upfront payment date.
        // Review: Forward start case.
        /*! If the notional exceeds the basket inception tranche
            notional, the cdo is leveraged by that factor.

            \todo: allow for extra payment flags, arbitrary upfront
                   payment date...
        */
        SyntheticCDO (const ext::shared_ptr<Basket>& basket,
                      Protection::Side side,
                      const Schedule& schedule,
                      Rate upfrontRate,
                      Rate runningRate,
                      const DayCounter& dayCounter,
                      BusinessDayConvention paymentConvention,
                      boost::optional<Real> notional = boost::none);

        const ext::shared_ptr<Basket>& basket() const { return basket_; }

        bool isExpired() const override;
        Rate fairPremium() const;
        Rate fairUpfrontPremium() const;
        Rate premiumValue () const;
        Rate protectionValue () const;
        Real premiumLegNPV() const;
        Real protectionLegNPV() const;
        /*!
          Total outstanding tranche notional, not wiped out
        */
        Real remainingNotional() const;
        /*! The number of times the contract contains the portfolio tranched 
                notional.
        */
        Real leverageFactor() const {
            return leverageFactor_;
        }
        //! Last protection date.
        const Date& maturity() const {
            return ext::dynamic_pointer_cast<FixedRateCoupon>(
                normalizedLeg_.back())->accrualEndDate();
        }
        /*! The Gaussian Copula LHP implied correlation that makes the 
            contract zero value. This is for a flat correlation along
            time and portfolio loss level.
        */
        Real implicitCorrelation(const std::vector<Real>& recoveries,
            const Handle<YieldTermStructure>& discountCurve, 
            Real targetNPV = 0.,
            Real accuracy = 1.0e-3) const;

        /*!
          Expected tranche loss for all payment dates
         */
        Disposable<std::vector<Real> > expectedTrancheLoss() const;
        Size error () const;

        void setupArguments(PricingEngine::arguments*) const override;
        void fetchResults(const PricingEngine::results*) const override;

      private:
        void setupExpired() const override;

        ext::shared_ptr<Basket> basket_;
        Protection::Side side_;
        Leg normalizedLeg_;

        Rate upfrontRate_;
        Rate runningRate_;
        const Real leverageFactor_;
        DayCounter dayCounter_;
        BusinessDayConvention paymentConvention_;

        mutable Real premiumValue_;
        mutable Real protectionValue_;
        mutable Real upfrontPremiumValue_;
        mutable Real remainingNotional_;
        mutable Size error_;
        mutable std::vector<Real> expectedTrancheLoss_;
    };

    class SyntheticCDO::arguments : public virtual PricingEngine::arguments {
    public:
        arguments() : side(Protection::Side(-1)),
                      upfrontRate(Null<Real>()),
                      runningRate(Null<Real>()) {}
        void validate() const override;

        ext::shared_ptr<Basket> basket;
        Protection::Side side;
        Leg normalizedLeg;

        Rate upfrontRate;
        Rate runningRate;
        Real leverageFactor;
        DayCounter dayCounter;
        BusinessDayConvention paymentConvention;
    };

    class SyntheticCDO::results : public Instrument::results {
    public:
      void reset() override;
      Real premiumValue;
      Real protectionValue;
      Real upfrontPremiumValue;
      Real remainingNotional;
      Real xMin, xMax;
      Size error;
      /* Expected tranche losses affecting this tranche coupons. Notice this
      number might be below the actual basket losses, since the cdo protection
      might start after basket inception (forward start CDO)*/
      std::vector<Real> expectedTrancheLoss;
    };


    //! CDO base engine
    class SyntheticCDO::engine : 
        public GenericEngine<SyntheticCDO::arguments, 
                             SyntheticCDO::results> { };

}

#endif

#endif
]]></document_content>
  </document>
</documents>